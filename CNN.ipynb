{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TSffer/IA/blob/master/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU0Nw-nX4cbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################################\n",
        "# Usando Tensorflow,\n",
        "# Abajo la version en Keras\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import zipfile\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_data_set(name=\"train\"):\n",
        "    x = None\n",
        "    y = None\n",
        "\n",
        "    maybe_download_and_extract()\n",
        "\n",
        "    folder_name = \"cifar_10\"\n",
        "\n",
        "    f = open('./data_set/'+folder_name+'/batches.meta', 'rb')\n",
        "    f.close()\n",
        "\n",
        "    if name is \"train\":\n",
        "        for i in range(5):\n",
        "            f = open('./data_set/'+folder_name+'/data_batch_' + str(i + 1), 'rb')\n",
        "            datadict = pickle.load(f, encoding='latin1')\n",
        "            f.close()\n",
        "\n",
        "            _X = datadict[\"data\"]\n",
        "            _Y = datadict['labels']\n",
        "\n",
        "            _X = np.array(_X, dtype=float) / 255.0\n",
        "            _X = _X.reshape([-1, 3, 32, 32])\n",
        "            _X = _X.transpose([0, 2, 3, 1])\n",
        "            _X = _X.reshape(-1, 32*32*3)\n",
        "\n",
        "            if x is None:\n",
        "                x = _X\n",
        "                y = _Y\n",
        "            else:\n",
        "                x = np.concatenate((x, _X), axis=0)\n",
        "                y = np.concatenate((y, _Y), axis=0)\n",
        "\n",
        "    elif name is \"test\":\n",
        "        f = open('./data_set/'+folder_name+'/test_batch', 'rb')\n",
        "        datadict = pickle.load(f, encoding='latin1')\n",
        "        f.close()\n",
        "\n",
        "        x = datadict[\"data\"]\n",
        "        y = np.array(datadict['labels'])\n",
        "\n",
        "        x = np.array(x, dtype=float) / 255.0\n",
        "        x = x.reshape([-1, 3, 32, 32])\n",
        "        x = x.transpose([0, 2, 3, 1])\n",
        "        x = x.reshape(-1, 32*32*3)\n",
        "\n",
        "    return x, dense_to_one_hot(y)\n",
        "\n",
        "\n",
        "def dense_to_one_hot(labels_dense, num_classes=10):\n",
        "    num_labels = labels_dense.shape[0]\n",
        "    index_offset = np.arange(num_labels) * num_classes\n",
        "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
        "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
        "\n",
        "    return labels_one_hot\n",
        "\n",
        "\n",
        "def _print_download_progress(count, block_size, total_size):\n",
        "    pct_complete = float(count * block_size) / total_size\n",
        "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
        "    sys.stdout.write(msg)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def maybe_download_and_extract():\n",
        "    main_directory = \"./data_set/\"\n",
        "    cifar_10_directory = main_directory+\"cifar_10/\"\n",
        "    if not os.path.exists(main_directory):\n",
        "        os.makedirs(main_directory)\n",
        "\n",
        "        url = \"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "        filename = url.split('/')[-1]\n",
        "        file_path = os.path.join(main_directory, filename)\n",
        "        zip_cifar_10 = file_path\n",
        "        file_path, _ = urlretrieve(url=url, filename=file_path, reporthook=_print_download_progress)\n",
        "\n",
        "        print()\n",
        "        print(\"Download finished. Extracting files.\")\n",
        "        if file_path.endswith(\".zip\"):\n",
        "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(main_directory)\n",
        "        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
        "            tarfile.open(name=file_path, mode=\"r:gz\").extractall(main_directory)\n",
        "        print(\"Done.\")\n",
        "\n",
        "        os.rename(main_directory+\"./cifar-10-batches-py\", cifar_10_directory)\n",
        "        os.remove(zip_cifar_10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EynMSor84vwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def model():\n",
        "    _IMAGE_SIZE = 32\n",
        "    _IMAGE_CHANNELS = 3\n",
        "    _NUM_CLASSES = 10\n",
        "\n",
        "    with tf.name_scope('main_params'):\n",
        "        x = tf.placeholder(tf.float32, shape=[None, _IMAGE_SIZE * _IMAGE_SIZE * _IMAGE_CHANNELS], name='Input')\n",
        "        y = tf.placeholder(tf.float32, shape=[None, _NUM_CLASSES], name='Output')\n",
        "        x_image = tf.reshape(x, [-1, _IMAGE_SIZE, _IMAGE_SIZE, _IMAGE_CHANNELS], name='images')\n",
        "\n",
        "        global_step = tf.Variable(initial_value=0, trainable=False, name='global_step')\n",
        "        learning_rate = tf.placeholder(tf.float32, shape=[], name='learning_rate')\n",
        "\n",
        "    with tf.variable_scope('conv1') as scope:\n",
        "        conv = tf.layers.conv2d(\n",
        "            inputs=x_image,\n",
        "            filters=32,\n",
        "            kernel_size=[3, 3],\n",
        "            padding='SAME',\n",
        "            activation=tf.nn.relu\n",
        "        )\n",
        "        conv = tf.layers.conv2d(\n",
        "            inputs=conv,\n",
        "            filters=64,\n",
        "            kernel_size=[3, 3],\n",
        "            padding='SAME',\n",
        "            activation=tf.nn.relu\n",
        "        )\n",
        "        pool = tf.layers.max_pooling2d(conv, pool_size=[2, 2], strides=2, padding='SAME')\n",
        "        drop = tf.layers.dropout(pool, rate=0.25, name=scope.name)\n",
        "\n",
        "    with tf.variable_scope('conv2') as scope:\n",
        "        conv = tf.layers.conv2d(\n",
        "            inputs=drop,\n",
        "            filters=128,\n",
        "            kernel_size=[3, 3],\n",
        "            padding='SAME',\n",
        "            activation=tf.nn.relu\n",
        "        )\n",
        "        pool = tf.layers.max_pooling2d(conv, pool_size=[2, 2], strides=2, padding='SAME')\n",
        "        conv = tf.layers.conv2d(\n",
        "            inputs=pool,\n",
        "            filters=128,\n",
        "            kernel_size=[2, 2],\n",
        "            padding='SAME',\n",
        "            activation=tf.nn.relu\n",
        "        )\n",
        "        pool = tf.layers.max_pooling2d(conv, pool_size=[2, 2], strides=2, padding='SAME')\n",
        "        drop = tf.layers.dropout(pool, rate=0.25, name=scope.name)\n",
        "\n",
        "    with tf.variable_scope('fully_connected') as scope:\n",
        "        flat = tf.reshape(drop, [-1, 4 * 4 * 128])\n",
        "\n",
        "        fc = tf.layers.dense(inputs=flat, units=1500, activation=tf.nn.relu)\n",
        "        drop = tf.layers.dropout(fc, rate=0.5)\n",
        "        softmax = tf.layers.dense(inputs=drop, units=_NUM_CLASSES, activation=tf.nn.softmax, name=scope.name)\n",
        "\n",
        "    y_pred_cls = tf.argmax(softmax, axis=1)\n",
        "\n",
        "    return x, y, softmax, y_pred_cls, global_step, learning_rate\n",
        "\n",
        "\n",
        "def lr(epoch):\n",
        "    learning_rate = 1e-3\n",
        "    if epoch > 80:\n",
        "        learning_rate *= 0.5e-3\n",
        "    elif epoch > 60:\n",
        "        learning_rate *= 1e-3\n",
        "    elif epoch > 40:\n",
        "        learning_rate *= 1e-2\n",
        "    elif epoch > 20:\n",
        "        learning_rate *= 1e-1\n",
        "    return learning_rate\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8ob2ioh4zoJ",
        "colab_type": "code",
        "outputId": "f92745da-f2a5-465f-d697-38558a44445a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from time import time\n",
        "import math\n",
        "\n",
        "\n",
        "#from include.data import get_data_set\n",
        "#from include.model import model, lr\n",
        "\n",
        "\n",
        "train_x, train_y = get_data_set(\"train\")\n",
        "test_x, test_y = get_data_set(\"test\")\n",
        "x, y, output, y_pred_cls, global_step, learning_rate = model()\n",
        "global_accuracy = 0\n",
        "\n",
        "\n",
        "# PARAMS\n",
        "_BATCH_SIZE = 128\n",
        "_EPOCH = 60\n",
        "_SAVE_PATH = \"./tensorboard/cifar-10-v1.0.0/\"\n",
        "\n",
        "\n",
        "# LOSS AND OPTIMIZER\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
        "                                   beta1=0.9,\n",
        "                                   beta2=0.999,\n",
        "                                   epsilon=1e-08).minimize(loss, global_step=global_step)\n",
        "\n",
        "\n",
        "# PREDICTION AND ACCURACY CALCULATION\n",
        "correct_prediction = tf.equal(y_pred_cls, tf.argmax(y, axis=1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "\n",
        "# SAVER\n",
        "merged = tf.summary.merge_all()\n",
        "saver = tf.train.Saver()\n",
        "sess = tf.Session()\n",
        "train_writer = tf.summary.FileWriter(_SAVE_PATH, sess.graph)\n",
        "\n",
        "\n",
        "try:\n",
        "    print(\"\\nTrying to restore last checkpoint ...\")\n",
        "    last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=_SAVE_PATH)\n",
        "    saver.restore(sess, save_path=last_chk_path)\n",
        "    print(\"Restored checkpoint from:\", last_chk_path)\n",
        "except ValueError:\n",
        "    print(\"\\nFailed to restore checkpoint. Initializing variables instead.\")\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    batch_size = int(math.ceil(len(train_x) / _BATCH_SIZE))\n",
        "    i_global = 0\n",
        "\n",
        "    for s in range(batch_size):\n",
        "        batch_xs = train_x[s*_BATCH_SIZE: (s+1)*_BATCH_SIZE]\n",
        "        batch_ys = train_y[s*_BATCH_SIZE: (s+1)*_BATCH_SIZE]\n",
        "\n",
        "        start_time = time()\n",
        "        i_global, _, batch_loss, batch_acc = sess.run(\n",
        "            [global_step, optimizer, loss, accuracy],\n",
        "            feed_dict={x: batch_xs, y: batch_ys, learning_rate: lr(epoch)})\n",
        "        duration = time() - start_time\n",
        "\n",
        "        if s % 10 == 0:\n",
        "            percentage = int(round((s/batch_size)*100))\n",
        "\n",
        "            bar_len = 29\n",
        "            filled_len = int((bar_len*int(percentage))/100)\n",
        "            bar = '=' * filled_len + '>' + '-' * (bar_len - filled_len)\n",
        "\n",
        "            msg = \"Global step: {:>5} - [{}] {:>3}% - acc: {:.4f} - loss: {:.4f} - {:.1f} sample/sec\"\n",
        "            print(msg.format(i_global, bar, percentage, batch_acc, batch_loss, _BATCH_SIZE / duration))\n",
        "\n",
        "    test_and_save(i_global, epoch)\n",
        "\n",
        "\n",
        "def test_and_save(_global_step, epoch):\n",
        "    global global_accuracy\n",
        "\n",
        "    i = 0\n",
        "    predicted_class = np.zeros(shape=len(test_x), dtype=np.int)\n",
        "    while i < len(test_x):\n",
        "        j = min(i + _BATCH_SIZE, len(test_x))\n",
        "        batch_xs = test_x[i:j, :]\n",
        "        batch_ys = test_y[i:j, :]\n",
        "        predicted_class[i:j] = sess.run(\n",
        "            y_pred_cls,\n",
        "            feed_dict={x: batch_xs, y: batch_ys, learning_rate: lr(epoch)}\n",
        "        )\n",
        "        i = j\n",
        "\n",
        "    correct = (np.argmax(test_y, axis=1) == predicted_class)\n",
        "    acc = correct.mean()*100\n",
        "    correct_numbers = correct.sum()\n",
        "\n",
        "    mes = \"\\nEpoch {} - accuracy: {:.2f}% ({}/{})\"\n",
        "    print(mes.format((epoch+1), acc, correct_numbers, len(test_x)))\n",
        "\n",
        "    if global_accuracy != 0 and global_accuracy < acc:\n",
        "\n",
        "        summary = tf.Summary(value=[\n",
        "            tf.Summary.Value(tag=\"Accuracy/test\", simple_value=acc),\n",
        "        ])\n",
        "        train_writer.add_summary(summary, _global_step)\n",
        "\n",
        "        saver.save(sess, save_path=_SAVE_PATH, global_step=_global_step)\n",
        "\n",
        "        mes = \"This epoch receive better accuracy: {:.2f} > {:.2f}. Saving session...\"\n",
        "        print(mes.format(acc, global_accuracy))\n",
        "        global_accuracy = acc\n",
        "\n",
        "    elif global_accuracy == 0:\n",
        "        global_accuracy = acc\n",
        "\n",
        "    print(\"###########################################################################################################\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    for i in range(_EPOCH):\n",
        "        print(\"\\nEpoch: {0}/{1}\\n\".format((i+1), _EPOCH))\n",
        "        train(i)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "sess.close()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Download progress: 100.0%\n",
            "Download finished. Extracting files.\n",
            "Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 01:49:00.417875 140239015667584 deprecation.py:323] From <ipython-input-5-1ebe973dafb2>:23: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "W0629 01:49:00.438382 140239015667584 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0629 01:49:00.946916 140239015667584 deprecation.py:323] From <ipython-input-5-1ebe973dafb2>:32: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.MaxPooling2D instead.\n",
            "W0629 01:49:01.230726 140239015667584 deprecation.py:323] From <ipython-input-5-1ebe973dafb2>:33: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "W0629 01:49:01.435683 140239015667584 deprecation.py:323] From <ipython-input-5-1ebe973dafb2>:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Trying to restore last checkpoint ...\n",
            "\n",
            "Failed to restore checkpoint. Initializing variables instead.\n",
            "\n",
            "Epoch: 1/60\n",
            "\n",
            "Global step:     1 - [>-----------------------------]   0% - acc: 0.0938 - loss: 2.3028 - 24.7 sample/sec\n",
            "Global step:    11 - [>-----------------------------]   3% - acc: 0.0859 - loss: 2.2940 - 7796.0 sample/sec\n",
            "Global step:    21 - [=>----------------------------]   5% - acc: 0.2188 - loss: 2.2260 - 7611.1 sample/sec\n",
            "Global step:    31 - [==>---------------------------]   8% - acc: 0.2188 - loss: 2.2037 - 7993.7 sample/sec\n",
            "Global step:    41 - [==>---------------------------]  10% - acc: 0.2266 - loss: 2.2069 - 7808.3 sample/sec\n",
            "Global step:    51 - [===>--------------------------]  13% - acc: 0.3125 - loss: 2.1538 - 7406.2 sample/sec\n",
            "Global step:    61 - [====>-------------------------]  15% - acc: 0.2734 - loss: 2.1687 - 7477.6 sample/sec\n",
            "Global step:    71 - [=====>------------------------]  18% - acc: 0.2812 - loss: 2.1536 - 7548.0 sample/sec\n",
            "Global step:    81 - [=====>------------------------]  20% - acc: 0.4609 - loss: 2.0007 - 8788.6 sample/sec\n",
            "Global step:    91 - [======>-----------------------]  23% - acc: 0.3750 - loss: 2.0686 - 6450.2 sample/sec\n",
            "Global step:   101 - [=======>----------------------]  26% - acc: 0.3828 - loss: 2.0851 - 8531.1 sample/sec\n",
            "Global step:   111 - [========>---------------------]  28% - acc: 0.3203 - loss: 2.1169 - 8765.7 sample/sec\n",
            "Global step:   121 - [========>---------------------]  31% - acc: 0.3359 - loss: 2.0971 - 8266.0 sample/sec\n",
            "Global step:   131 - [=========>--------------------]  33% - acc: 0.3594 - loss: 2.0926 - 9008.2 sample/sec\n",
            "Global step:   141 - [==========>-------------------]  36% - acc: 0.4297 - loss: 2.0349 - 8867.6 sample/sec\n",
            "Global step:   151 - [===========>------------------]  38% - acc: 0.3438 - loss: 2.1308 - 9105.1 sample/sec\n",
            "Global step:   161 - [===========>------------------]  41% - acc: 0.3828 - loss: 2.0798 - 7620.7 sample/sec\n",
            "Global step:   171 - [============>-----------------]  43% - acc: 0.3906 - loss: 2.0640 - 6846.2 sample/sec\n",
            "Global step:   181 - [=============>----------------]  46% - acc: 0.3984 - loss: 2.0603 - 8737.8 sample/sec\n",
            "Global step:   191 - [==============>---------------]  49% - acc: 0.4219 - loss: 2.0281 - 8065.7 sample/sec\n",
            "Global step:   201 - [==============>---------------]  51% - acc: 0.4219 - loss: 2.0387 - 7303.8 sample/sec\n",
            "Global step:   211 - [===============>--------------]  54% - acc: 0.3984 - loss: 2.0660 - 8806.2 sample/sec\n",
            "Global step:   221 - [================>-------------]  56% - acc: 0.4062 - loss: 2.0640 - 8855.9 sample/sec\n",
            "Global step:   231 - [=================>------------]  59% - acc: 0.3281 - loss: 2.1009 - 9045.8 sample/sec\n",
            "Global step:   241 - [=================>------------]  61% - acc: 0.3438 - loss: 2.1013 - 7606.0 sample/sec\n",
            "Global step:   251 - [==================>-----------]  64% - acc: 0.3984 - loss: 2.0467 - 7230.0 sample/sec\n",
            "Global step:   261 - [===================>----------]  66% - acc: 0.4219 - loss: 2.0569 - 8694.8 sample/sec\n",
            "Global step:   271 - [====================>---------]  69% - acc: 0.3984 - loss: 2.0673 - 6493.3 sample/sec\n",
            "Global step:   281 - [====================>---------]  72% - acc: 0.4219 - loss: 2.0238 - 7762.3 sample/sec\n",
            "Global step:   291 - [=====================>--------]  74% - acc: 0.3516 - loss: 2.1139 - 8813.9 sample/sec\n",
            "Global step:   301 - [======================>-------]  77% - acc: 0.3984 - loss: 2.0423 - 8574.6 sample/sec\n",
            "Global step:   311 - [======================>-------]  79% - acc: 0.4141 - loss: 2.0279 - 8948.1 sample/sec\n",
            "Global step:   321 - [=======================>------]  82% - acc: 0.3438 - loss: 2.1048 - 7863.2 sample/sec\n",
            "Global step:   331 - [========================>-----]  84% - acc: 0.4922 - loss: 1.9809 - 8855.3 sample/sec\n",
            "Global step:   341 - [=========================>----]  87% - acc: 0.4609 - loss: 2.0078 - 8262.5 sample/sec\n",
            "Global step:   351 - [==========================>---]  90% - acc: 0.3594 - loss: 2.0841 - 8296.3 sample/sec\n",
            "Global step:   361 - [==========================>---]  92% - acc: 0.5156 - loss: 1.9407 - 7306.9 sample/sec\n",
            "Global step:   371 - [===========================>--]  95% - acc: 0.4219 - loss: 2.0237 - 8003.0 sample/sec\n",
            "Global step:   381 - [============================>-]  97% - acc: 0.4609 - loss: 1.9783 - 8722.7 sample/sec\n",
            "Global step:   391 - [=============================>] 100% - acc: 0.4750 - loss: 2.0050 - 840.0 sample/sec\n",
            "\n",
            "Epoch 1 - accuracy: 46.68% (4668/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 2/60\n",
            "\n",
            "Global step:   392 - [>-----------------------------]   0% - acc: 0.4922 - loss: 1.9696 - 8260.3 sample/sec\n",
            "Global step:   402 - [>-----------------------------]   3% - acc: 0.5156 - loss: 1.9627 - 8513.5 sample/sec\n",
            "Global step:   412 - [=>----------------------------]   5% - acc: 0.5234 - loss: 1.9466 - 8264.1 sample/sec\n",
            "Global step:   422 - [==>---------------------------]   8% - acc: 0.4219 - loss: 2.0310 - 8010.1 sample/sec\n",
            "Global step:   432 - [==>---------------------------]  10% - acc: 0.4219 - loss: 2.0300 - 7872.4 sample/sec\n",
            "Global step:   442 - [===>--------------------------]  13% - acc: 0.4766 - loss: 1.9816 - 7762.8 sample/sec\n",
            "Global step:   452 - [====>-------------------------]  15% - acc: 0.4453 - loss: 2.0170 - 7148.7 sample/sec\n",
            "Global step:   462 - [=====>------------------------]  18% - acc: 0.3984 - loss: 2.0466 - 9219.7 sample/sec\n",
            "Global step:   472 - [=====>------------------------]  20% - acc: 0.4453 - loss: 1.9880 - 8899.5 sample/sec\n",
            "Global step:   482 - [======>-----------------------]  23% - acc: 0.5312 - loss: 1.9191 - 8979.4 sample/sec\n",
            "Global step:   492 - [=======>----------------------]  26% - acc: 0.4531 - loss: 1.9891 - 8457.9 sample/sec\n",
            "Global step:   502 - [========>---------------------]  28% - acc: 0.3828 - loss: 2.0650 - 8957.0 sample/sec\n",
            "Global step:   512 - [========>---------------------]  31% - acc: 0.3984 - loss: 2.0431 - 9260.7 sample/sec\n",
            "Global step:   522 - [=========>--------------------]  33% - acc: 0.5391 - loss: 1.9481 - 9066.3 sample/sec\n",
            "Global step:   532 - [==========>-------------------]  36% - acc: 0.4922 - loss: 1.9772 - 9030.8 sample/sec\n",
            "Global step:   542 - [===========>------------------]  38% - acc: 0.5234 - loss: 1.9592 - 8924.8 sample/sec\n",
            "Global step:   552 - [===========>------------------]  41% - acc: 0.5000 - loss: 1.9704 - 9125.0 sample/sec\n",
            "Global step:   562 - [============>-----------------]  43% - acc: 0.4219 - loss: 2.0183 - 8702.9 sample/sec\n",
            "Global step:   572 - [=============>----------------]  46% - acc: 0.5000 - loss: 1.9562 - 8839.4 sample/sec\n",
            "Global step:   582 - [==============>---------------]  49% - acc: 0.4766 - loss: 1.9763 - 8270.0 sample/sec\n",
            "Global step:   592 - [==============>---------------]  51% - acc: 0.5156 - loss: 1.9500 - 9176.8 sample/sec\n",
            "Global step:   602 - [===============>--------------]  54% - acc: 0.3984 - loss: 2.0445 - 8896.7 sample/sec\n",
            "Global step:   612 - [================>-------------]  56% - acc: 0.5000 - loss: 1.9669 - 8781.6 sample/sec\n",
            "Global step:   622 - [=================>------------]  59% - acc: 0.4609 - loss: 1.9890 - 9165.2 sample/sec\n",
            "Global step:   632 - [=================>------------]  61% - acc: 0.4688 - loss: 1.9842 - 9096.6 sample/sec\n",
            "Global step:   642 - [==================>-----------]  64% - acc: 0.4688 - loss: 1.9792 - 8972.1 sample/sec\n",
            "Global step:   652 - [===================>----------]  66% - acc: 0.4453 - loss: 2.0003 - 8335.2 sample/sec\n",
            "Global step:   662 - [====================>---------]  69% - acc: 0.5156 - loss: 1.9323 - 8871.7 sample/sec\n",
            "Global step:   672 - [====================>---------]  72% - acc: 0.5234 - loss: 1.9260 - 9103.2 sample/sec\n",
            "Global step:   682 - [=====================>--------]  74% - acc: 0.4609 - loss: 2.0027 - 8917.7 sample/sec\n",
            "Global step:   692 - [======================>-------]  77% - acc: 0.5469 - loss: 1.9139 - 8761.9 sample/sec\n",
            "Global step:   702 - [======================>-------]  79% - acc: 0.5078 - loss: 1.9541 - 9144.9 sample/sec\n",
            "Global step:   712 - [=======================>------]  82% - acc: 0.3672 - loss: 2.0597 - 9016.4 sample/sec\n",
            "Global step:   722 - [========================>-----]  84% - acc: 0.5859 - loss: 1.8578 - 7584.5 sample/sec\n",
            "Global step:   732 - [=========================>----]  87% - acc: 0.5078 - loss: 1.9566 - 8973.9 sample/sec\n",
            "Global step:   742 - [==========================>---]  90% - acc: 0.4062 - loss: 2.0340 - 8891.7 sample/sec\n",
            "Global step:   752 - [==========================>---]  92% - acc: 0.6250 - loss: 1.8441 - 8958.0 sample/sec\n",
            "Global step:   762 - [===========================>--]  95% - acc: 0.5547 - loss: 1.9103 - 8985.6 sample/sec\n",
            "Global step:   772 - [============================>-]  97% - acc: 0.5781 - loss: 1.8803 - 9168.0 sample/sec\n",
            "Global step:   782 - [=============================>] 100% - acc: 0.5500 - loss: 1.9085 - 12798.2 sample/sec\n",
            "\n",
            "Epoch 2 - accuracy: 56.36% (5636/10000)\n",
            "This epoch receive better accuracy: 56.36 > 46.68. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 3/60\n",
            "\n",
            "Global step:   783 - [>-----------------------------]   0% - acc: 0.5938 - loss: 1.8657 - 8959.0 sample/sec\n",
            "Global step:   793 - [>-----------------------------]   3% - acc: 0.5156 - loss: 1.9434 - 9128.4 sample/sec\n",
            "Global step:   803 - [=>----------------------------]   5% - acc: 0.6094 - loss: 1.8524 - 8949.8 sample/sec\n",
            "Global step:   813 - [==>---------------------------]   8% - acc: 0.5000 - loss: 1.9490 - 8916.3 sample/sec\n",
            "Global step:   823 - [==>---------------------------]  10% - acc: 0.5547 - loss: 1.9000 - 8970.4 sample/sec\n",
            "Global step:   833 - [===>--------------------------]  13% - acc: 0.5312 - loss: 1.9268 - 8804.8 sample/sec\n",
            "Global step:   843 - [====>-------------------------]  15% - acc: 0.6016 - loss: 1.8576 - 8472.4 sample/sec\n",
            "Global step:   853 - [=====>------------------------]  18% - acc: 0.5312 - loss: 1.9187 - 9148.3 sample/sec\n",
            "Global step:   863 - [=====>------------------------]  20% - acc: 0.4766 - loss: 1.9889 - 9004.4 sample/sec\n",
            "Global step:   873 - [======>-----------------------]  23% - acc: 0.5312 - loss: 1.9144 - 8657.3 sample/sec\n",
            "Global step:   883 - [=======>----------------------]  26% - acc: 0.5547 - loss: 1.8944 - 9514.3 sample/sec\n",
            "Global step:   893 - [========>---------------------]  28% - acc: 0.5078 - loss: 1.9556 - 8999.1 sample/sec\n",
            "Global step:   903 - [========>---------------------]  31% - acc: 0.5859 - loss: 1.9061 - 8902.3 sample/sec\n",
            "Global step:   913 - [=========>--------------------]  33% - acc: 0.5234 - loss: 1.9322 - 8841.0 sample/sec\n",
            "Global step:   923 - [==========>-------------------]  36% - acc: 0.5000 - loss: 1.9362 - 9101.5 sample/sec\n",
            "Global step:   933 - [===========>------------------]  38% - acc: 0.5703 - loss: 1.8955 - 9100.1 sample/sec\n",
            "Global step:   943 - [===========>------------------]  41% - acc: 0.6016 - loss: 1.8699 - 8618.5 sample/sec\n",
            "Global step:   953 - [============>-----------------]  43% - acc: 0.5469 - loss: 1.9026 - 8995.1 sample/sec\n",
            "Global step:   963 - [=============>----------------]  46% - acc: 0.6172 - loss: 1.8226 - 9078.4 sample/sec\n",
            "Global step:   973 - [==============>---------------]  49% - acc: 0.5391 - loss: 1.9071 - 8959.0 sample/sec\n",
            "Global step:   983 - [==============>---------------]  51% - acc: 0.5391 - loss: 1.9142 - 8947.1 sample/sec\n",
            "Global step:   993 - [===============>--------------]  54% - acc: 0.4844 - loss: 1.9718 - 8881.5 sample/sec\n",
            "Global step:  1003 - [================>-------------]  56% - acc: 0.5469 - loss: 1.9040 - 8998.4 sample/sec\n",
            "Global step:  1013 - [=================>------------]  59% - acc: 0.5703 - loss: 1.8808 - 8942.6 sample/sec\n",
            "Global step:  1023 - [=================>------------]  61% - acc: 0.5859 - loss: 1.8896 - 8926.9 sample/sec\n",
            "Global step:  1033 - [==================>-----------]  64% - acc: 0.5469 - loss: 1.8940 - 8894.0 sample/sec\n",
            "Global step:  1043 - [===================>----------]  66% - acc: 0.5547 - loss: 1.9164 - 8936.2 sample/sec\n",
            "Global step:  1053 - [====================>---------]  69% - acc: 0.5078 - loss: 1.9318 - 8994.8 sample/sec\n",
            "Global step:  1063 - [====================>---------]  72% - acc: 0.5625 - loss: 1.8941 - 9000.4 sample/sec\n",
            "Global step:  1073 - [=====================>--------]  74% - acc: 0.5781 - loss: 1.8938 - 8965.8 sample/sec\n",
            "Global step:  1083 - [======================>-------]  77% - acc: 0.6875 - loss: 1.7763 - 8948.6 sample/sec\n",
            "Global step:  1093 - [======================>-------]  79% - acc: 0.5312 - loss: 1.9181 - 9023.2 sample/sec\n",
            "Global step:  1103 - [=======================>------]  82% - acc: 0.4922 - loss: 1.9726 - 8893.5 sample/sec\n",
            "Global step:  1113 - [========================>-----]  84% - acc: 0.6328 - loss: 1.8141 - 6376.5 sample/sec\n",
            "Global step:  1123 - [=========================>----]  87% - acc: 0.5781 - loss: 1.8712 - 8846.7 sample/sec\n",
            "Global step:  1133 - [==========================>---]  90% - acc: 0.5547 - loss: 1.8992 - 9038.4 sample/sec\n",
            "Global step:  1143 - [==========================>---]  92% - acc: 0.6797 - loss: 1.7782 - 8412.8 sample/sec\n",
            "Global step:  1153 - [===========================>--]  95% - acc: 0.5938 - loss: 1.8758 - 9154.9 sample/sec\n",
            "Global step:  1163 - [============================>-]  97% - acc: 0.5625 - loss: 1.8942 - 9086.4 sample/sec\n",
            "Global step:  1173 - [=============================>] 100% - acc: 0.6000 - loss: 1.8629 - 12716.0 sample/sec\n",
            "\n",
            "Epoch 3 - accuracy: 60.99% (6099/10000)\n",
            "This epoch receive better accuracy: 60.99 > 56.36. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 4/60\n",
            "\n",
            "Global step:  1174 - [>-----------------------------]   0% - acc: 0.6484 - loss: 1.8045 - 9112.0 sample/sec\n",
            "Global step:  1184 - [>-----------------------------]   3% - acc: 0.6797 - loss: 1.8020 - 9038.1 sample/sec\n",
            "Global step:  1194 - [=>----------------------------]   5% - acc: 0.6719 - loss: 1.7796 - 9094.0 sample/sec\n",
            "Global step:  1204 - [==>---------------------------]   8% - acc: 0.6562 - loss: 1.8166 - 8971.5 sample/sec\n",
            "Global step:  1214 - [==>---------------------------]  10% - acc: 0.6562 - loss: 1.8136 - 9088.3 sample/sec\n",
            "Global step:  1224 - [===>--------------------------]  13% - acc: 0.6094 - loss: 1.8453 - 8891.5 sample/sec\n",
            "Global step:  1234 - [====>-------------------------]  15% - acc: 0.6562 - loss: 1.8279 - 8568.7 sample/sec\n",
            "Global step:  1244 - [=====>------------------------]  18% - acc: 0.5938 - loss: 1.8618 - 8964.9 sample/sec\n",
            "Global step:  1254 - [=====>------------------------]  20% - acc: 0.6094 - loss: 1.8480 - 8969.8 sample/sec\n",
            "Global step:  1264 - [======>-----------------------]  23% - acc: 0.6250 - loss: 1.8320 - 8940.4 sample/sec\n",
            "Global step:  1274 - [=======>----------------------]  26% - acc: 0.6172 - loss: 1.8341 - 9108.3 sample/sec\n",
            "Global step:  1284 - [========>---------------------]  28% - acc: 0.5625 - loss: 1.8981 - 9088.1 sample/sec\n",
            "Global step:  1294 - [========>---------------------]  31% - acc: 0.6484 - loss: 1.8076 - 9009.4 sample/sec\n",
            "Global step:  1304 - [=========>--------------------]  33% - acc: 0.6016 - loss: 1.8533 - 9003.7 sample/sec\n",
            "Global step:  1314 - [==========>-------------------]  36% - acc: 0.5312 - loss: 1.9197 - 8957.9 sample/sec\n",
            "Global step:  1324 - [===========>------------------]  38% - acc: 0.6172 - loss: 1.8314 - 8954.1 sample/sec\n",
            "Global step:  1334 - [===========>------------------]  41% - acc: 0.6016 - loss: 1.8435 - 8906.6 sample/sec\n",
            "Global step:  1344 - [============>-----------------]  43% - acc: 0.5781 - loss: 1.8673 - 8978.2 sample/sec\n",
            "Global step:  1354 - [=============>----------------]  46% - acc: 0.6562 - loss: 1.8021 - 8985.0 sample/sec\n",
            "Global step:  1364 - [==============>---------------]  49% - acc: 0.6016 - loss: 1.8470 - 9092.9 sample/sec\n",
            "Global step:  1374 - [==============>---------------]  51% - acc: 0.6094 - loss: 1.8555 - 8871.4 sample/sec\n",
            "Global step:  1384 - [===============>--------------]  54% - acc: 0.5781 - loss: 1.8820 - 8910.4 sample/sec\n",
            "Global step:  1394 - [================>-------------]  56% - acc: 0.6406 - loss: 1.8219 - 8973.4 sample/sec\n",
            "Global step:  1404 - [=================>------------]  59% - acc: 0.6328 - loss: 1.8203 - 9023.6 sample/sec\n",
            "Global step:  1414 - [=================>------------]  61% - acc: 0.5781 - loss: 1.8833 - 8905.1 sample/sec\n",
            "Global step:  1424 - [==================>-----------]  64% - acc: 0.5938 - loss: 1.8535 - 9010.3 sample/sec\n",
            "Global step:  1434 - [===================>----------]  66% - acc: 0.6641 - loss: 1.8187 - 4559.8 sample/sec\n",
            "Global step:  1444 - [====================>---------]  69% - acc: 0.6328 - loss: 1.8179 - 8953.8 sample/sec\n",
            "Global step:  1454 - [====================>---------]  72% - acc: 0.6641 - loss: 1.7962 - 8801.2 sample/sec\n",
            "Global step:  1464 - [=====================>--------]  74% - acc: 0.6484 - loss: 1.8180 - 8433.3 sample/sec\n",
            "Global step:  1474 - [======================>-------]  77% - acc: 0.7266 - loss: 1.7312 - 8709.4 sample/sec\n",
            "Global step:  1484 - [======================>-------]  79% - acc: 0.6250 - loss: 1.8371 - 8796.4 sample/sec\n",
            "Global step:  1494 - [=======================>------]  82% - acc: 0.6172 - loss: 1.8444 - 8890.7 sample/sec\n",
            "Global step:  1504 - [========================>-----]  84% - acc: 0.7031 - loss: 1.7542 - 8792.5 sample/sec\n",
            "Global step:  1514 - [=========================>----]  87% - acc: 0.6328 - loss: 1.8220 - 8757.2 sample/sec\n",
            "Global step:  1524 - [==========================>---]  90% - acc: 0.6094 - loss: 1.8465 - 8789.8 sample/sec\n",
            "Global step:  1534 - [==========================>---]  92% - acc: 0.7734 - loss: 1.6946 - 8860.0 sample/sec\n",
            "Global step:  1544 - [===========================>--]  95% - acc: 0.6406 - loss: 1.8067 - 8875.5 sample/sec\n",
            "Global step:  1554 - [============================>-]  97% - acc: 0.6953 - loss: 1.7788 - 8924.6 sample/sec\n",
            "Global step:  1564 - [=============================>] 100% - acc: 0.6750 - loss: 1.7887 - 12913.6 sample/sec\n",
            "\n",
            "Epoch 4 - accuracy: 66.08% (6608/10000)\n",
            "This epoch receive better accuracy: 66.08 > 60.99. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 5/60\n",
            "\n",
            "Global step:  1565 - [>-----------------------------]   0% - acc: 0.6953 - loss: 1.7555 - 9024.4 sample/sec\n",
            "Global step:  1575 - [>-----------------------------]   3% - acc: 0.6484 - loss: 1.8104 - 8914.4 sample/sec\n",
            "Global step:  1585 - [=>----------------------------]   5% - acc: 0.7422 - loss: 1.7077 - 8467.9 sample/sec\n",
            "Global step:  1595 - [==>---------------------------]   8% - acc: 0.7031 - loss: 1.7626 - 8933.6 sample/sec\n",
            "Global step:  1605 - [==>---------------------------]  10% - acc: 0.6016 - loss: 1.8321 - 8744.3 sample/sec\n",
            "Global step:  1615 - [===>--------------------------]  13% - acc: 0.6953 - loss: 1.7801 - 8746.8 sample/sec\n",
            "Global step:  1625 - [====>-------------------------]  15% - acc: 0.6250 - loss: 1.8209 - 8473.5 sample/sec\n",
            "Global step:  1635 - [=====>------------------------]  18% - acc: 0.6562 - loss: 1.8072 - 8751.2 sample/sec\n",
            "Global step:  1645 - [=====>------------------------]  20% - acc: 0.6484 - loss: 1.8140 - 9034.4 sample/sec\n",
            "Global step:  1655 - [======>-----------------------]  23% - acc: 0.6406 - loss: 1.8169 - 8971.0 sample/sec\n",
            "Global step:  1665 - [=======>----------------------]  26% - acc: 0.6953 - loss: 1.7650 - 8926.7 sample/sec\n",
            "Global step:  1675 - [========>---------------------]  28% - acc: 0.6719 - loss: 1.8031 - 9104.4 sample/sec\n",
            "Global step:  1685 - [========>---------------------]  31% - acc: 0.7266 - loss: 1.7380 - 8839.9 sample/sec\n",
            "Global step:  1695 - [=========>--------------------]  33% - acc: 0.6797 - loss: 1.7787 - 8922.3 sample/sec\n",
            "Global step:  1705 - [==========>-------------------]  36% - acc: 0.6484 - loss: 1.8196 - 8815.0 sample/sec\n",
            "Global step:  1715 - [===========>------------------]  38% - acc: 0.7031 - loss: 1.7561 - 8940.0 sample/sec\n",
            "Global step:  1725 - [===========>------------------]  41% - acc: 0.6328 - loss: 1.8160 - 9075.2 sample/sec\n",
            "Global step:  1735 - [============>-----------------]  43% - acc: 0.6328 - loss: 1.8278 - 8759.9 sample/sec\n",
            "Global step:  1745 - [=============>----------------]  46% - acc: 0.7188 - loss: 1.7482 - 8912.8 sample/sec\n",
            "Global step:  1755 - [==============>---------------]  49% - acc: 0.6719 - loss: 1.7907 - 9160.4 sample/sec\n",
            "Global step:  1765 - [==============>---------------]  51% - acc: 0.6328 - loss: 1.8288 - 8807.1 sample/sec\n",
            "Global step:  1775 - [===============>--------------]  54% - acc: 0.6641 - loss: 1.8021 - 8912.9 sample/sec\n",
            "Global step:  1785 - [================>-------------]  56% - acc: 0.7109 - loss: 1.7481 - 8745.7 sample/sec\n",
            "Global step:  1795 - [=================>------------]  59% - acc: 0.6641 - loss: 1.7930 - 8794.4 sample/sec\n",
            "Global step:  1805 - [=================>------------]  61% - acc: 0.6641 - loss: 1.7903 - 8996.1 sample/sec\n",
            "Global step:  1815 - [==================>-----------]  64% - acc: 0.6875 - loss: 1.7762 - 8864.1 sample/sec\n",
            "Global step:  1825 - [===================>----------]  66% - acc: 0.6875 - loss: 1.7696 - 8884.5 sample/sec\n",
            "Global step:  1835 - [====================>---------]  69% - acc: 0.6406 - loss: 1.8174 - 8975.8 sample/sec\n",
            "Global step:  1845 - [====================>---------]  72% - acc: 0.7188 - loss: 1.7532 - 8991.6 sample/sec\n",
            "Global step:  1855 - [=====================>--------]  74% - acc: 0.6406 - loss: 1.8150 - 8612.1 sample/sec\n",
            "Global step:  1865 - [======================>-------]  77% - acc: 0.7578 - loss: 1.7094 - 8710.5 sample/sec\n",
            "Global step:  1875 - [======================>-------]  79% - acc: 0.5859 - loss: 1.8588 - 8735.7 sample/sec\n",
            "Global step:  1885 - [=======================>------]  82% - acc: 0.6094 - loss: 1.8362 - 8676.1 sample/sec\n",
            "Global step:  1895 - [========================>-----]  84% - acc: 0.7109 - loss: 1.7476 - 8994.2 sample/sec\n",
            "Global step:  1905 - [=========================>----]  87% - acc: 0.6875 - loss: 1.7695 - 8836.8 sample/sec\n",
            "Global step:  1915 - [==========================>---]  90% - acc: 0.6641 - loss: 1.7900 - 8689.6 sample/sec\n",
            "Global step:  1925 - [==========================>---]  92% - acc: 0.7422 - loss: 1.7265 - 8678.0 sample/sec\n",
            "Global step:  1935 - [===========================>--]  95% - acc: 0.6562 - loss: 1.8088 - 9037.2 sample/sec\n",
            "Global step:  1945 - [============================>-]  97% - acc: 0.6719 - loss: 1.7919 - 8770.1 sample/sec\n",
            "Global step:  1955 - [=============================>] 100% - acc: 0.6500 - loss: 1.8045 - 12431.9 sample/sec\n",
            "\n",
            "Epoch 5 - accuracy: 65.20% (6520/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 6/60\n",
            "\n",
            "Global step:  1956 - [>-----------------------------]   0% - acc: 0.7109 - loss: 1.7545 - 8913.5 sample/sec\n",
            "Global step:  1966 - [>-----------------------------]   3% - acc: 0.7500 - loss: 1.7290 - 8615.0 sample/sec\n",
            "Global step:  1976 - [=>----------------------------]   5% - acc: 0.7266 - loss: 1.7198 - 9119.9 sample/sec\n",
            "Global step:  1986 - [==>---------------------------]   8% - acc: 0.7031 - loss: 1.7580 - 8734.4 sample/sec\n",
            "Global step:  1996 - [==>---------------------------]  10% - acc: 0.7109 - loss: 1.7462 - 8818.1 sample/sec\n",
            "Global step:  2006 - [===>--------------------------]  13% - acc: 0.6875 - loss: 1.7619 - 8748.7 sample/sec\n",
            "Global step:  2016 - [====>-------------------------]  15% - acc: 0.6953 - loss: 1.7545 - 8987.4 sample/sec\n",
            "Global step:  2026 - [=====>------------------------]  18% - acc: 0.6953 - loss: 1.7695 - 8010.6 sample/sec\n",
            "Global step:  2036 - [=====>------------------------]  20% - acc: 0.7109 - loss: 1.7516 - 7627.8 sample/sec\n",
            "Global step:  2046 - [======>-----------------------]  23% - acc: 0.6953 - loss: 1.7557 - 8984.2 sample/sec\n",
            "Global step:  2056 - [=======>----------------------]  26% - acc: 0.7109 - loss: 1.7328 - 8728.2 sample/sec\n",
            "Global step:  2066 - [========>---------------------]  28% - acc: 0.6406 - loss: 1.8030 - 8764.5 sample/sec\n",
            "Global step:  2076 - [========>---------------------]  31% - acc: 0.7578 - loss: 1.7045 - 8709.5 sample/sec\n",
            "Global step:  2086 - [=========>--------------------]  33% - acc: 0.6406 - loss: 1.8158 - 8933.4 sample/sec\n",
            "Global step:  2096 - [==========>-------------------]  36% - acc: 0.7188 - loss: 1.7384 - 8315.6 sample/sec\n",
            "Global step:  2106 - [===========>------------------]  38% - acc: 0.7031 - loss: 1.7472 - 8680.3 sample/sec\n",
            "Global step:  2116 - [===========>------------------]  41% - acc: 0.7109 - loss: 1.7546 - 9105.2 sample/sec\n",
            "Global step:  2126 - [============>-----------------]  43% - acc: 0.6406 - loss: 1.8143 - 8664.2 sample/sec\n",
            "Global step:  2136 - [=============>----------------]  46% - acc: 0.7344 - loss: 1.7280 - 9090.9 sample/sec\n",
            "Global step:  2146 - [==============>---------------]  49% - acc: 0.6641 - loss: 1.7914 - 8990.3 sample/sec\n",
            "Global step:  2156 - [==============>---------------]  51% - acc: 0.6562 - loss: 1.7970 - 8967.9 sample/sec\n",
            "Global step:  2166 - [===============>--------------]  54% - acc: 0.6719 - loss: 1.7808 - 8960.5 sample/sec\n",
            "Global step:  2176 - [================>-------------]  56% - acc: 0.7188 - loss: 1.7385 - 8910.6 sample/sec\n",
            "Global step:  2186 - [=================>------------]  59% - acc: 0.7031 - loss: 1.7574 - 8954.9 sample/sec\n",
            "Global step:  2196 - [=================>------------]  61% - acc: 0.6641 - loss: 1.7948 - 8659.6 sample/sec\n",
            "Global step:  2206 - [==================>-----------]  64% - acc: 0.7109 - loss: 1.7504 - 8367.8 sample/sec\n",
            "Global step:  2216 - [===================>----------]  66% - acc: 0.6719 - loss: 1.7774 - 8781.3 sample/sec\n",
            "Global step:  2226 - [====================>---------]  69% - acc: 0.6875 - loss: 1.7793 - 9008.1 sample/sec\n",
            "Global step:  2236 - [====================>---------]  72% - acc: 0.7188 - loss: 1.7376 - 9002.8 sample/sec\n",
            "Global step:  2246 - [=====================>--------]  74% - acc: 0.6719 - loss: 1.7936 - 8734.0 sample/sec\n",
            "Global step:  2256 - [======================>-------]  77% - acc: 0.7109 - loss: 1.7410 - 8810.3 sample/sec\n",
            "Global step:  2266 - [======================>-------]  79% - acc: 0.6719 - loss: 1.8001 - 8582.0 sample/sec\n",
            "Global step:  2276 - [=======================>------]  82% - acc: 0.6484 - loss: 1.8281 - 8855.0 sample/sec\n",
            "Global step:  2286 - [========================>-----]  84% - acc: 0.7734 - loss: 1.6830 - 9065.2 sample/sec\n",
            "Global step:  2296 - [=========================>----]  87% - acc: 0.7422 - loss: 1.7222 - 9057.4 sample/sec\n",
            "Global step:  2306 - [==========================>---]  90% - acc: 0.7031 - loss: 1.7583 - 8974.3 sample/sec\n",
            "Global step:  2316 - [==========================>---]  92% - acc: 0.7500 - loss: 1.7075 - 8849.8 sample/sec\n",
            "Global step:  2326 - [===========================>--]  95% - acc: 0.6797 - loss: 1.7783 - 8826.9 sample/sec\n",
            "Global step:  2336 - [============================>-]  97% - acc: 0.7500 - loss: 1.7078 - 8572.2 sample/sec\n",
            "Global step:  2346 - [=============================>] 100% - acc: 0.7250 - loss: 1.7244 - 13140.2 sample/sec\n",
            "\n",
            "Epoch 6 - accuracy: 68.83% (6883/10000)\n",
            "This epoch receive better accuracy: 68.83 > 66.08. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 7/60\n",
            "\n",
            "Global step:  2347 - [>-----------------------------]   0% - acc: 0.7422 - loss: 1.7206 - 9071.5 sample/sec\n",
            "Global step:  2357 - [>-----------------------------]   3% - acc: 0.7422 - loss: 1.7110 - 8888.9 sample/sec\n",
            "Global step:  2367 - [=>----------------------------]   5% - acc: 0.7734 - loss: 1.6813 - 8718.6 sample/sec\n",
            "Global step:  2377 - [==>---------------------------]   8% - acc: 0.6875 - loss: 1.7651 - 8755.2 sample/sec\n",
            "Global step:  2387 - [==>---------------------------]  10% - acc: 0.7500 - loss: 1.7138 - 8165.0 sample/sec\n",
            "Global step:  2397 - [===>--------------------------]  13% - acc: 0.7344 - loss: 1.7293 - 8903.0 sample/sec\n",
            "Global step:  2407 - [====>-------------------------]  15% - acc: 0.7266 - loss: 1.7312 - 8326.5 sample/sec\n",
            "Global step:  2417 - [=====>------------------------]  18% - acc: 0.6719 - loss: 1.7783 - 8961.4 sample/sec\n",
            "Global step:  2427 - [=====>------------------------]  20% - acc: 0.6797 - loss: 1.7678 - 9006.8 sample/sec\n",
            "Global step:  2437 - [======>-----------------------]  23% - acc: 0.6797 - loss: 1.7807 - 8912.6 sample/sec\n",
            "Global step:  2447 - [=======>----------------------]  26% - acc: 0.7969 - loss: 1.6822 - 8768.4 sample/sec\n",
            "Global step:  2457 - [========>---------------------]  28% - acc: 0.7344 - loss: 1.7287 - 8620.3 sample/sec\n",
            "Global step:  2467 - [========>---------------------]  31% - acc: 0.7812 - loss: 1.6808 - 8887.0 sample/sec\n",
            "Global step:  2477 - [=========>--------------------]  33% - acc: 0.7500 - loss: 1.7277 - 8901.3 sample/sec\n",
            "Global step:  2487 - [==========>-------------------]  36% - acc: 0.7734 - loss: 1.7058 - 8930.7 sample/sec\n",
            "Global step:  2497 - [===========>------------------]  38% - acc: 0.7734 - loss: 1.6972 - 8707.4 sample/sec\n",
            "Global step:  2507 - [===========>------------------]  41% - acc: 0.7109 - loss: 1.7419 - 8913.5 sample/sec\n",
            "Global step:  2517 - [============>-----------------]  43% - acc: 0.7422 - loss: 1.7262 - 8945.5 sample/sec\n",
            "Global step:  2527 - [=============>----------------]  46% - acc: 0.7422 - loss: 1.7183 - 8451.1 sample/sec\n",
            "Global step:  2537 - [==============>---------------]  49% - acc: 0.7188 - loss: 1.7304 - 9011.2 sample/sec\n",
            "Global step:  2547 - [==============>---------------]  51% - acc: 0.7109 - loss: 1.7472 - 8632.1 sample/sec\n",
            "Global step:  2557 - [===============>--------------]  54% - acc: 0.7031 - loss: 1.7467 - 8880.7 sample/sec\n",
            "Global step:  2567 - [================>-------------]  56% - acc: 0.7656 - loss: 1.6927 - 8756.7 sample/sec\n",
            "Global step:  2577 - [=================>------------]  59% - acc: 0.7344 - loss: 1.7218 - 8853.9 sample/sec\n",
            "Global step:  2587 - [=================>------------]  61% - acc: 0.6797 - loss: 1.7765 - 8550.9 sample/sec\n",
            "Global step:  2597 - [==================>-----------]  64% - acc: 0.6953 - loss: 1.7558 - 8772.0 sample/sec\n",
            "Global step:  2607 - [===================>----------]  66% - acc: 0.7344 - loss: 1.7396 - 8582.2 sample/sec\n",
            "Global step:  2617 - [====================>---------]  69% - acc: 0.7031 - loss: 1.7468 - 9154.6 sample/sec\n",
            "Global step:  2627 - [====================>---------]  72% - acc: 0.7656 - loss: 1.7010 - 8832.9 sample/sec\n",
            "Global step:  2637 - [=====================>--------]  74% - acc: 0.6953 - loss: 1.7672 - 8939.8 sample/sec\n",
            "Global step:  2647 - [======================>-------]  77% - acc: 0.7500 - loss: 1.7121 - 8661.3 sample/sec\n",
            "Global step:  2657 - [======================>-------]  79% - acc: 0.6016 - loss: 1.8458 - 8967.7 sample/sec\n",
            "Global step:  2667 - [=======================>------]  82% - acc: 0.6641 - loss: 1.7966 - 8914.6 sample/sec\n",
            "Global step:  2677 - [========================>-----]  84% - acc: 0.8203 - loss: 1.6424 - 8783.6 sample/sec\n",
            "Global step:  2687 - [=========================>----]  87% - acc: 0.7656 - loss: 1.6901 - 8855.3 sample/sec\n",
            "Global step:  2697 - [==========================>---]  90% - acc: 0.6719 - loss: 1.7829 - 8209.9 sample/sec\n",
            "Global step:  2707 - [==========================>---]  92% - acc: 0.7891 - loss: 1.6726 - 8575.3 sample/sec\n",
            "Global step:  2717 - [===========================>--]  95% - acc: 0.7344 - loss: 1.7283 - 8894.0 sample/sec\n",
            "Global step:  2727 - [============================>-]  97% - acc: 0.8047 - loss: 1.6607 - 9108.2 sample/sec\n",
            "Global step:  2737 - [=============================>] 100% - acc: 0.7125 - loss: 1.7380 - 12585.2 sample/sec\n",
            "\n",
            "Epoch 7 - accuracy: 67.11% (6711/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 8/60\n",
            "\n",
            "Global step:  2738 - [>-----------------------------]   0% - acc: 0.7578 - loss: 1.7045 - 9114.8 sample/sec\n",
            "Global step:  2748 - [>-----------------------------]   3% - acc: 0.7109 - loss: 1.7532 - 8688.8 sample/sec\n",
            "Global step:  2758 - [=>----------------------------]   5% - acc: 0.8203 - loss: 1.6419 - 8489.3 sample/sec\n",
            "Global step:  2768 - [==>---------------------------]   8% - acc: 0.7188 - loss: 1.7362 - 8779.7 sample/sec\n",
            "Global step:  2778 - [==>---------------------------]  10% - acc: 0.7500 - loss: 1.6990 - 6725.6 sample/sec\n",
            "Global step:  2788 - [===>--------------------------]  13% - acc: 0.7109 - loss: 1.7508 - 8678.1 sample/sec\n",
            "Global step:  2798 - [====>-------------------------]  15% - acc: 0.7500 - loss: 1.7102 - 8911.2 sample/sec\n",
            "Global step:  2808 - [=====>------------------------]  18% - acc: 0.6875 - loss: 1.7754 - 8782.6 sample/sec\n",
            "Global step:  2818 - [=====>------------------------]  20% - acc: 0.7422 - loss: 1.7197 - 8878.3 sample/sec\n",
            "Global step:  2828 - [======>-----------------------]  23% - acc: 0.7422 - loss: 1.7152 - 8800.4 sample/sec\n",
            "Global step:  2838 - [=======>----------------------]  26% - acc: 0.7812 - loss: 1.6734 - 8596.0 sample/sec\n",
            "Global step:  2848 - [========>---------------------]  28% - acc: 0.7578 - loss: 1.6997 - 8417.5 sample/sec\n",
            "Global step:  2858 - [========>---------------------]  31% - acc: 0.7969 - loss: 1.6588 - 8733.7 sample/sec\n",
            "Global step:  2868 - [=========>--------------------]  33% - acc: 0.7969 - loss: 1.6815 - 8773.0 sample/sec\n",
            "Global step:  2878 - [==========>-------------------]  36% - acc: 0.7812 - loss: 1.6952 - 8646.9 sample/sec\n",
            "Global step:  2888 - [===========>------------------]  38% - acc: 0.7969 - loss: 1.6631 - 8621.7 sample/sec\n",
            "Global step:  2898 - [===========>------------------]  41% - acc: 0.7812 - loss: 1.6914 - 8047.4 sample/sec\n",
            "Global step:  2908 - [============>-----------------]  43% - acc: 0.7734 - loss: 1.6929 - 8700.9 sample/sec\n",
            "Global step:  2918 - [=============>----------------]  46% - acc: 0.7578 - loss: 1.6958 - 8345.1 sample/sec\n",
            "Global step:  2928 - [==============>---------------]  49% - acc: 0.7109 - loss: 1.7429 - 8559.0 sample/sec\n",
            "Global step:  2938 - [==============>---------------]  51% - acc: 0.7031 - loss: 1.7610 - 9113.7 sample/sec\n",
            "Global step:  2948 - [===============>--------------]  54% - acc: 0.7109 - loss: 1.7488 - 8597.4 sample/sec\n",
            "Global step:  2958 - [================>-------------]  56% - acc: 0.7969 - loss: 1.6615 - 8876.5 sample/sec\n",
            "Global step:  2968 - [=================>------------]  59% - acc: 0.7734 - loss: 1.7009 - 8591.6 sample/sec\n",
            "Global step:  2978 - [=================>------------]  61% - acc: 0.7109 - loss: 1.7488 - 8587.0 sample/sec\n",
            "Global step:  2988 - [==================>-----------]  64% - acc: 0.7344 - loss: 1.7159 - 8552.8 sample/sec\n",
            "Global step:  2998 - [===================>----------]  66% - acc: 0.7266 - loss: 1.7341 - 8793.5 sample/sec\n",
            "Global step:  3008 - [====================>---------]  69% - acc: 0.7500 - loss: 1.7154 - 8681.3 sample/sec\n",
            "Global step:  3018 - [====================>---------]  72% - acc: 0.7969 - loss: 1.6708 - 8774.1 sample/sec\n",
            "Global step:  3028 - [=====================>--------]  74% - acc: 0.7266 - loss: 1.7436 - 8853.6 sample/sec\n",
            "Global step:  3038 - [======================>-------]  77% - acc: 0.7109 - loss: 1.7464 - 8999.9 sample/sec\n",
            "Global step:  3048 - [======================>-------]  79% - acc: 0.6406 - loss: 1.7878 - 8660.9 sample/sec\n",
            "Global step:  3058 - [=======================>------]  82% - acc: 0.7578 - loss: 1.7128 - 8887.3 sample/sec\n",
            "Global step:  3068 - [========================>-----]  84% - acc: 0.7891 - loss: 1.6605 - 8761.9 sample/sec\n",
            "Global step:  3078 - [=========================>----]  87% - acc: 0.7812 - loss: 1.6698 - 8779.1 sample/sec\n",
            "Global step:  3088 - [==========================>---]  90% - acc: 0.7109 - loss: 1.7440 - 8906.4 sample/sec\n",
            "Global step:  3098 - [==========================>---]  92% - acc: 0.7734 - loss: 1.6882 - 8793.4 sample/sec\n",
            "Global step:  3108 - [===========================>--]  95% - acc: 0.7344 - loss: 1.7356 - 9000.8 sample/sec\n",
            "Global step:  3118 - [============================>-]  97% - acc: 0.7734 - loss: 1.6882 - 8308.1 sample/sec\n",
            "Global step:  3128 - [=============================>] 100% - acc: 0.7500 - loss: 1.7095 - 13061.6 sample/sec\n",
            "\n",
            "Epoch 8 - accuracy: 71.38% (7138/10000)\n",
            "This epoch receive better accuracy: 71.38 > 68.83. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 9/60\n",
            "\n",
            "Global step:  3129 - [>-----------------------------]   0% - acc: 0.7812 - loss: 1.6833 - 9020.8 sample/sec\n",
            "Global step:  3139 - [>-----------------------------]   3% - acc: 0.7578 - loss: 1.7026 - 8929.7 sample/sec\n",
            "Global step:  3149 - [=>----------------------------]   5% - acc: 0.8125 - loss: 1.6399 - 8858.5 sample/sec\n",
            "Global step:  3159 - [==>---------------------------]   8% - acc: 0.7812 - loss: 1.6773 - 8785.0 sample/sec\n",
            "Global step:  3169 - [==>---------------------------]  10% - acc: 0.7812 - loss: 1.6871 - 8930.6 sample/sec\n",
            "Global step:  3179 - [===>--------------------------]  13% - acc: 0.8047 - loss: 1.6485 - 8533.1 sample/sec\n",
            "Global step:  3189 - [====>-------------------------]  15% - acc: 0.7500 - loss: 1.7242 - 8518.1 sample/sec\n",
            "Global step:  3199 - [=====>------------------------]  18% - acc: 0.7500 - loss: 1.7145 - 8696.1 sample/sec\n",
            "Global step:  3209 - [=====>------------------------]  20% - acc: 0.7578 - loss: 1.6984 - 8920.5 sample/sec\n",
            "Global step:  3219 - [======>-----------------------]  23% - acc: 0.7578 - loss: 1.7005 - 8770.0 sample/sec\n",
            "Global step:  3229 - [=======>----------------------]  26% - acc: 0.7578 - loss: 1.6937 - 8813.7 sample/sec\n",
            "Global step:  3239 - [========>---------------------]  28% - acc: 0.7422 - loss: 1.7265 - 9040.4 sample/sec\n",
            "Global step:  3249 - [========>---------------------]  31% - acc: 0.7969 - loss: 1.6658 - 8327.1 sample/sec\n",
            "Global step:  3259 - [=========>--------------------]  33% - acc: 0.7734 - loss: 1.6806 - 8645.4 sample/sec\n",
            "Global step:  3269 - [==========>-------------------]  36% - acc: 0.7266 - loss: 1.7298 - 8702.7 sample/sec\n",
            "Global step:  3279 - [===========>------------------]  38% - acc: 0.7969 - loss: 1.6647 - 8933.1 sample/sec\n",
            "Global step:  3289 - [===========>------------------]  41% - acc: 0.8125 - loss: 1.6510 - 8787.2 sample/sec\n",
            "Global step:  3299 - [============>-----------------]  43% - acc: 0.7578 - loss: 1.7006 - 8754.8 sample/sec\n",
            "Global step:  3309 - [=============>----------------]  46% - acc: 0.7812 - loss: 1.6786 - 8706.0 sample/sec\n",
            "Global step:  3319 - [==============>---------------]  49% - acc: 0.7422 - loss: 1.7153 - 8785.2 sample/sec\n",
            "Global step:  3329 - [==============>---------------]  51% - acc: 0.7500 - loss: 1.7132 - 8676.0 sample/sec\n",
            "Global step:  3339 - [===============>--------------]  54% - acc: 0.7344 - loss: 1.7363 - 8771.4 sample/sec\n",
            "Global step:  3349 - [================>-------------]  56% - acc: 0.8203 - loss: 1.6364 - 8603.4 sample/sec\n",
            "Global step:  3359 - [=================>------------]  59% - acc: 0.7812 - loss: 1.6803 - 8734.6 sample/sec\n",
            "Global step:  3369 - [=================>------------]  61% - acc: 0.7578 - loss: 1.7092 - 8244.3 sample/sec\n",
            "Global step:  3379 - [==================>-----------]  64% - acc: 0.7656 - loss: 1.6979 - 8834.3 sample/sec\n",
            "Global step:  3389 - [===================>----------]  66% - acc: 0.7812 - loss: 1.6811 - 9026.4 sample/sec\n",
            "Global step:  3399 - [====================>---------]  69% - acc: 0.7656 - loss: 1.6923 - 8847.7 sample/sec\n",
            "Global step:  3409 - [====================>---------]  72% - acc: 0.8359 - loss: 1.6246 - 8751.2 sample/sec\n",
            "Global step:  3419 - [=====================>--------]  74% - acc: 0.7656 - loss: 1.7021 - 8686.5 sample/sec\n",
            "Global step:  3429 - [======================>-------]  77% - acc: 0.7812 - loss: 1.6872 - 8825.8 sample/sec\n",
            "Global step:  3439 - [======================>-------]  79% - acc: 0.7109 - loss: 1.7623 - 8620.8 sample/sec\n",
            "Global step:  3449 - [=======================>------]  82% - acc: 0.7266 - loss: 1.7213 - 8932.1 sample/sec\n",
            "Global step:  3459 - [========================>-----]  84% - acc: 0.8594 - loss: 1.6026 - 8370.0 sample/sec\n",
            "Global step:  3469 - [=========================>----]  87% - acc: 0.7734 - loss: 1.6731 - 8651.8 sample/sec\n",
            "Global step:  3479 - [==========================>---]  90% - acc: 0.7266 - loss: 1.7251 - 8557.6 sample/sec\n",
            "Global step:  3489 - [==========================>---]  92% - acc: 0.7969 - loss: 1.6628 - 8448.9 sample/sec\n",
            "Global step:  3499 - [===========================>--]  95% - acc: 0.7422 - loss: 1.7118 - 8631.2 sample/sec\n",
            "Global step:  3509 - [============================>-]  97% - acc: 0.7891 - loss: 1.6738 - 8739.4 sample/sec\n",
            "Global step:  3519 - [=============================>] 100% - acc: 0.7750 - loss: 1.6733 - 12325.1 sample/sec\n",
            "\n",
            "Epoch 9 - accuracy: 71.14% (7114/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 10/60\n",
            "\n",
            "Global step:  3520 - [>-----------------------------]   0% - acc: 0.8125 - loss: 1.6506 - 9033.4 sample/sec\n",
            "Global step:  3530 - [>-----------------------------]   3% - acc: 0.7734 - loss: 1.6898 - 8742.4 sample/sec\n",
            "Global step:  3540 - [=>----------------------------]   5% - acc: 0.8438 - loss: 1.6200 - 8863.4 sample/sec\n",
            "Global step:  3550 - [==>---------------------------]   8% - acc: 0.7656 - loss: 1.6903 - 8807.7 sample/sec\n",
            "Global step:  3560 - [==>---------------------------]  10% - acc: 0.7969 - loss: 1.6691 - 8838.1 sample/sec\n",
            "Global step:  3570 - [===>--------------------------]  13% - acc: 0.8516 - loss: 1.6257 - 8652.0 sample/sec\n",
            "Global step:  3580 - [====>-------------------------]  15% - acc: 0.7656 - loss: 1.6992 - 8765.0 sample/sec\n",
            "Global step:  3590 - [=====>------------------------]  18% - acc: 0.7344 - loss: 1.7300 - 8765.4 sample/sec\n",
            "Global step:  3600 - [=====>------------------------]  20% - acc: 0.8281 - loss: 1.6285 - 8765.4 sample/sec\n",
            "Global step:  3610 - [======>-----------------------]  23% - acc: 0.7500 - loss: 1.6967 - 8815.5 sample/sec\n",
            "Global step:  3620 - [=======>----------------------]  26% - acc: 0.8438 - loss: 1.6162 - 8854.4 sample/sec\n",
            "Global step:  3630 - [========>---------------------]  28% - acc: 0.7891 - loss: 1.6737 - 8822.3 sample/sec\n",
            "Global step:  3640 - [========>---------------------]  31% - acc: 0.8125 - loss: 1.6441 - 8803.8 sample/sec\n",
            "Global step:  3650 - [=========>--------------------]  33% - acc: 0.7969 - loss: 1.6742 - 8323.2 sample/sec\n",
            "Global step:  3660 - [==========>-------------------]  36% - acc: 0.8203 - loss: 1.6365 - 8896.1 sample/sec\n",
            "Global step:  3670 - [===========>------------------]  38% - acc: 0.8047 - loss: 1.6508 - 9030.9 sample/sec\n",
            "Global step:  3680 - [===========>------------------]  41% - acc: 0.8516 - loss: 1.6226 - 8604.3 sample/sec\n",
            "Global step:  3690 - [============>-----------------]  43% - acc: 0.7891 - loss: 1.6790 - 8657.5 sample/sec\n",
            "Global step:  3700 - [=============>----------------]  46% - acc: 0.7734 - loss: 1.6901 - 9111.1 sample/sec\n",
            "Global step:  3710 - [==============>---------------]  49% - acc: 0.7344 - loss: 1.7287 - 8628.6 sample/sec\n",
            "Global step:  3720 - [==============>---------------]  51% - acc: 0.7656 - loss: 1.6921 - 8800.7 sample/sec\n",
            "Global step:  3730 - [===============>--------------]  54% - acc: 0.7891 - loss: 1.6758 - 8770.7 sample/sec\n",
            "Global step:  3740 - [================>-------------]  56% - acc: 0.8203 - loss: 1.6448 - 8614.6 sample/sec\n",
            "Global step:  3750 - [=================>------------]  59% - acc: 0.7891 - loss: 1.6655 - 8695.8 sample/sec\n",
            "Global step:  3760 - [=================>------------]  61% - acc: 0.7188 - loss: 1.7417 - 8705.8 sample/sec\n",
            "Global step:  3770 - [==================>-----------]  64% - acc: 0.8047 - loss: 1.6741 - 8631.2 sample/sec\n",
            "Global step:  3780 - [===================>----------]  66% - acc: 0.7734 - loss: 1.7000 - 8665.5 sample/sec\n",
            "Global step:  3790 - [====================>---------]  69% - acc: 0.8203 - loss: 1.6490 - 8617.0 sample/sec\n",
            "Global step:  3800 - [====================>---------]  72% - acc: 0.8672 - loss: 1.5974 - 8659.6 sample/sec\n",
            "Global step:  3810 - [=====================>--------]  74% - acc: 0.7969 - loss: 1.6662 - 8593.8 sample/sec\n",
            "Global step:  3820 - [======================>-------]  77% - acc: 0.8203 - loss: 1.6527 - 8809.4 sample/sec\n",
            "Global step:  3830 - [======================>-------]  79% - acc: 0.7266 - loss: 1.7306 - 8809.7 sample/sec\n",
            "Global step:  3840 - [=======================>------]  82% - acc: 0.7734 - loss: 1.6978 - 8841.2 sample/sec\n",
            "Global step:  3850 - [========================>-----]  84% - acc: 0.8594 - loss: 1.6036 - 8826.6 sample/sec\n",
            "Global step:  3860 - [=========================>----]  87% - acc: 0.8125 - loss: 1.6556 - 8628.9 sample/sec\n",
            "Global step:  3870 - [==========================>---]  90% - acc: 0.7500 - loss: 1.7099 - 8961.7 sample/sec\n",
            "Global step:  3880 - [==========================>---]  92% - acc: 0.7734 - loss: 1.6770 - 8746.4 sample/sec\n",
            "Global step:  3890 - [===========================>--]  95% - acc: 0.7734 - loss: 1.6841 - 8760.1 sample/sec\n",
            "Global step:  3900 - [============================>-]  97% - acc: 0.8203 - loss: 1.6426 - 7107.9 sample/sec\n",
            "Global step:  3910 - [=============================>] 100% - acc: 0.7875 - loss: 1.6712 - 12748.6 sample/sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0629 01:50:14.282102 140239015667584 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10 - accuracy: 74.19% (7419/10000)\n",
            "This epoch receive better accuracy: 74.19 > 71.38. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 11/60\n",
            "\n",
            "Global step:  3911 - [>-----------------------------]   0% - acc: 0.8203 - loss: 1.6386 - 9105.2 sample/sec\n",
            "Global step:  3921 - [>-----------------------------]   3% - acc: 0.7734 - loss: 1.6745 - 8812.7 sample/sec\n",
            "Global step:  3931 - [=>----------------------------]   5% - acc: 0.8438 - loss: 1.6174 - 8617.8 sample/sec\n",
            "Global step:  3941 - [==>---------------------------]   8% - acc: 0.7969 - loss: 1.6603 - 8526.2 sample/sec\n",
            "Global step:  3951 - [==>---------------------------]  10% - acc: 0.7969 - loss: 1.6593 - 8379.6 sample/sec\n",
            "Global step:  3961 - [===>--------------------------]  13% - acc: 0.8281 - loss: 1.6399 - 8620.1 sample/sec\n",
            "Global step:  3971 - [====>-------------------------]  15% - acc: 0.7734 - loss: 1.6906 - 8433.9 sample/sec\n",
            "Global step:  3981 - [=====>------------------------]  18% - acc: 0.7578 - loss: 1.6952 - 8856.2 sample/sec\n",
            "Global step:  3991 - [=====>------------------------]  20% - acc: 0.8047 - loss: 1.6602 - 8828.5 sample/sec\n",
            "Global step:  4001 - [======>-----------------------]  23% - acc: 0.8125 - loss: 1.6494 - 8719.3 sample/sec\n",
            "Global step:  4011 - [=======>----------------------]  26% - acc: 0.8203 - loss: 1.6415 - 9086.1 sample/sec\n",
            "Global step:  4021 - [========>---------------------]  28% - acc: 0.7969 - loss: 1.6803 - 8747.0 sample/sec\n",
            "Global step:  4031 - [========>---------------------]  31% - acc: 0.7891 - loss: 1.6600 - 8883.4 sample/sec\n",
            "Global step:  4041 - [=========>--------------------]  33% - acc: 0.7812 - loss: 1.6788 - 8563.1 sample/sec\n",
            "Global step:  4051 - [==========>-------------------]  36% - acc: 0.8281 - loss: 1.6433 - 8151.5 sample/sec\n",
            "Global step:  4061 - [===========>------------------]  38% - acc: 0.7969 - loss: 1.6672 - 8574.4 sample/sec\n",
            "Global step:  4071 - [===========>------------------]  41% - acc: 0.8203 - loss: 1.6406 - 8379.1 sample/sec\n",
            "Global step:  4081 - [============>-----------------]  43% - acc: 0.7812 - loss: 1.6613 - 8707.7 sample/sec\n",
            "Global step:  4091 - [=============>----------------]  46% - acc: 0.8281 - loss: 1.6382 - 8860.1 sample/sec\n",
            "Global step:  4101 - [==============>---------------]  49% - acc: 0.7969 - loss: 1.6701 - 8785.0 sample/sec\n",
            "Global step:  4111 - [==============>---------------]  51% - acc: 0.7812 - loss: 1.6772 - 8815.3 sample/sec\n",
            "Global step:  4121 - [===============>--------------]  54% - acc: 0.7891 - loss: 1.6739 - 8588.6 sample/sec\n",
            "Global step:  4131 - [================>-------------]  56% - acc: 0.8516 - loss: 1.6043 - 8664.2 sample/sec\n",
            "Global step:  4141 - [=================>------------]  59% - acc: 0.8125 - loss: 1.6421 - 7888.2 sample/sec\n",
            "Global step:  4151 - [=================>------------]  61% - acc: 0.7500 - loss: 1.7062 - 8801.5 sample/sec\n",
            "Global step:  4161 - [==================>-----------]  64% - acc: 0.7500 - loss: 1.7053 - 8589.5 sample/sec\n",
            "Global step:  4171 - [===================>----------]  66% - acc: 0.8125 - loss: 1.6432 - 8621.7 sample/sec\n",
            "Global step:  4181 - [====================>---------]  69% - acc: 0.7656 - loss: 1.6812 - 8613.1 sample/sec\n",
            "Global step:  4191 - [====================>---------]  72% - acc: 0.8438 - loss: 1.6199 - 8462.4 sample/sec\n",
            "Global step:  4201 - [=====================>--------]  74% - acc: 0.8203 - loss: 1.6432 - 8778.6 sample/sec\n",
            "Global step:  4211 - [======================>-------]  77% - acc: 0.8203 - loss: 1.6331 - 8722.8 sample/sec\n",
            "Global step:  4221 - [======================>-------]  79% - acc: 0.7188 - loss: 1.7350 - 8412.8 sample/sec\n",
            "Global step:  4231 - [=======================>------]  82% - acc: 0.8047 - loss: 1.6654 - 8992.8 sample/sec\n",
            "Global step:  4241 - [========================>-----]  84% - acc: 0.8906 - loss: 1.5787 - 8860.1 sample/sec\n",
            "Global step:  4251 - [=========================>----]  87% - acc: 0.8828 - loss: 1.5809 - 8891.4 sample/sec\n",
            "Global step:  4261 - [==========================>---]  90% - acc: 0.7812 - loss: 1.6735 - 8758.8 sample/sec\n",
            "Global step:  4271 - [==========================>---]  92% - acc: 0.8203 - loss: 1.6447 - 8487.9 sample/sec\n",
            "Global step:  4281 - [===========================>--]  95% - acc: 0.7422 - loss: 1.7131 - 8295.0 sample/sec\n",
            "Global step:  4291 - [============================>-]  97% - acc: 0.8281 - loss: 1.6399 - 8711.8 sample/sec\n",
            "Global step:  4301 - [=============================>] 100% - acc: 0.7375 - loss: 1.7161 - 11251.9 sample/sec\n",
            "\n",
            "Epoch 11 - accuracy: 73.43% (7343/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 12/60\n",
            "\n",
            "Global step:  4302 - [>-----------------------------]   0% - acc: 0.8438 - loss: 1.6050 - 8972.8 sample/sec\n",
            "Global step:  4312 - [>-----------------------------]   3% - acc: 0.7812 - loss: 1.6692 - 8733.4 sample/sec\n",
            "Global step:  4322 - [=>----------------------------]   5% - acc: 0.8359 - loss: 1.6268 - 6366.2 sample/sec\n",
            "Global step:  4332 - [==>---------------------------]   8% - acc: 0.7734 - loss: 1.6843 - 8536.9 sample/sec\n",
            "Global step:  4342 - [==>---------------------------]  10% - acc: 0.8438 - loss: 1.6189 - 8737.3 sample/sec\n",
            "Global step:  4352 - [===>--------------------------]  13% - acc: 0.8359 - loss: 1.6381 - 8582.2 sample/sec\n",
            "Global step:  4362 - [====>-------------------------]  15% - acc: 0.7969 - loss: 1.6668 - 8887.7 sample/sec\n",
            "Global step:  4372 - [=====>------------------------]  18% - acc: 0.7969 - loss: 1.6732 - 8873.5 sample/sec\n",
            "Global step:  4382 - [=====>------------------------]  20% - acc: 0.8125 - loss: 1.6458 - 8409.9 sample/sec\n",
            "Global step:  4392 - [======>-----------------------]  23% - acc: 0.8516 - loss: 1.6159 - 8627.5 sample/sec\n",
            "Global step:  4402 - [=======>----------------------]  26% - acc: 0.8516 - loss: 1.6069 - 8754.5 sample/sec\n",
            "Global step:  4412 - [========>---------------------]  28% - acc: 0.8281 - loss: 1.6364 - 8689.3 sample/sec\n",
            "Global step:  4422 - [========>---------------------]  31% - acc: 0.8359 - loss: 1.6267 - 8671.4 sample/sec\n",
            "Global step:  4432 - [=========>--------------------]  33% - acc: 0.7812 - loss: 1.6688 - 8778.6 sample/sec\n",
            "Global step:  4442 - [==========>-------------------]  36% - acc: 0.8438 - loss: 1.6236 - 8453.3 sample/sec\n",
            "Global step:  4452 - [===========>------------------]  38% - acc: 0.8281 - loss: 1.6330 - 8603.6 sample/sec\n",
            "Global step:  4462 - [===========>------------------]  41% - acc: 0.8359 - loss: 1.6256 - 8504.2 sample/sec\n",
            "Global step:  4472 - [============>-----------------]  43% - acc: 0.8281 - loss: 1.6326 - 8690.2 sample/sec\n",
            "Global step:  4482 - [=============>----------------]  46% - acc: 0.8047 - loss: 1.6485 - 8545.4 sample/sec\n",
            "Global step:  4492 - [==============>---------------]  49% - acc: 0.8125 - loss: 1.6549 - 8545.1 sample/sec\n",
            "Global step:  4502 - [==============>---------------]  51% - acc: 0.8281 - loss: 1.6438 - 8730.3 sample/sec\n",
            "Global step:  4512 - [===============>--------------]  54% - acc: 0.8125 - loss: 1.6593 - 8601.5 sample/sec\n",
            "Global step:  4522 - [================>-------------]  56% - acc: 0.8828 - loss: 1.5759 - 8532.3 sample/sec\n",
            "Global step:  4532 - [=================>------------]  59% - acc: 0.8438 - loss: 1.6255 - 8814.3 sample/sec\n",
            "Global step:  4542 - [=================>------------]  61% - acc: 0.7500 - loss: 1.7145 - 8771.1 sample/sec\n",
            "Global step:  4552 - [==================>-----------]  64% - acc: 0.7891 - loss: 1.6702 - 8738.7 sample/sec\n",
            "Global step:  4562 - [===================>----------]  66% - acc: 0.8047 - loss: 1.6598 - 8517.7 sample/sec\n",
            "Global step:  4572 - [====================>---------]  69% - acc: 0.8047 - loss: 1.6555 - 8546.3 sample/sec\n",
            "Global step:  4582 - [====================>---------]  72% - acc: 0.8594 - loss: 1.6052 - 8598.5 sample/sec\n",
            "Global step:  4592 - [=====================>--------]  74% - acc: 0.7969 - loss: 1.6594 - 8676.6 sample/sec\n",
            "Global step:  4602 - [======================>-------]  77% - acc: 0.8281 - loss: 1.6254 - 8294.5 sample/sec\n",
            "Global step:  4612 - [======================>-------]  79% - acc: 0.7500 - loss: 1.7192 - 8631.6 sample/sec\n",
            "Global step:  4622 - [=======================>------]  82% - acc: 0.7734 - loss: 1.6889 - 8551.8 sample/sec\n",
            "Global step:  4632 - [========================>-----]  84% - acc: 0.8984 - loss: 1.5641 - 8563.1 sample/sec\n",
            "Global step:  4642 - [=========================>----]  87% - acc: 0.8594 - loss: 1.6034 - 8775.6 sample/sec\n",
            "Global step:  4652 - [==========================>---]  90% - acc: 0.7891 - loss: 1.6767 - 8691.6 sample/sec\n",
            "Global step:  4662 - [==========================>---]  92% - acc: 0.8203 - loss: 1.6384 - 8450.9 sample/sec\n",
            "Global step:  4672 - [===========================>--]  95% - acc: 0.7812 - loss: 1.6708 - 8469.9 sample/sec\n",
            "Global step:  4682 - [============================>-]  97% - acc: 0.8594 - loss: 1.6051 - 8557.1 sample/sec\n",
            "Global step:  4692 - [=============================>] 100% - acc: 0.8125 - loss: 1.6444 - 12094.7 sample/sec\n",
            "\n",
            "Epoch 12 - accuracy: 75.31% (7531/10000)\n",
            "This epoch receive better accuracy: 75.31 > 74.19. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 13/60\n",
            "\n",
            "Global step:  4693 - [>-----------------------------]   0% - acc: 0.8672 - loss: 1.5862 - 8986.6 sample/sec\n",
            "Global step:  4703 - [>-----------------------------]   3% - acc: 0.8438 - loss: 1.6156 - 8770.3 sample/sec\n",
            "Global step:  4713 - [=>----------------------------]   5% - acc: 0.8672 - loss: 1.5928 - 8698.9 sample/sec\n",
            "Global step:  4723 - [==>---------------------------]   8% - acc: 0.8281 - loss: 1.6331 - 8534.8 sample/sec\n",
            "Global step:  4733 - [==>---------------------------]  10% - acc: 0.8047 - loss: 1.6596 - 8738.1 sample/sec\n",
            "Global step:  4743 - [===>--------------------------]  13% - acc: 0.8594 - loss: 1.5994 - 8551.9 sample/sec\n",
            "Global step:  4753 - [====>-------------------------]  15% - acc: 0.8203 - loss: 1.6348 - 7960.8 sample/sec\n",
            "Global step:  4763 - [=====>------------------------]  18% - acc: 0.8281 - loss: 1.6407 - 8709.5 sample/sec\n",
            "Global step:  4773 - [=====>------------------------]  20% - acc: 0.8828 - loss: 1.5647 - 8555.3 sample/sec\n",
            "Global step:  4783 - [======>-----------------------]  23% - acc: 0.8438 - loss: 1.6196 - 8453.2 sample/sec\n",
            "Global step:  4793 - [=======>----------------------]  26% - acc: 0.8984 - loss: 1.5654 - 8634.4 sample/sec\n",
            "Global step:  4803 - [========>---------------------]  28% - acc: 0.8047 - loss: 1.6393 - 8791.2 sample/sec\n",
            "Global step:  4813 - [========>---------------------]  31% - acc: 0.8594 - loss: 1.6082 - 8383.5 sample/sec\n",
            "Global step:  4823 - [=========>--------------------]  33% - acc: 0.8281 - loss: 1.6295 - 8512.0 sample/sec\n",
            "Global step:  4833 - [==========>-------------------]  36% - acc: 0.8438 - loss: 1.6209 - 8602.6 sample/sec\n",
            "Global step:  4843 - [===========>------------------]  38% - acc: 0.8516 - loss: 1.6120 - 8581.0 sample/sec\n",
            "Global step:  4853 - [===========>------------------]  41% - acc: 0.8125 - loss: 1.6523 - 8879.5 sample/sec\n",
            "Global step:  4863 - [============>-----------------]  43% - acc: 0.8125 - loss: 1.6479 - 8667.7 sample/sec\n",
            "Global step:  4873 - [=============>----------------]  46% - acc: 0.8438 - loss: 1.6136 - 8794.5 sample/sec\n",
            "Global step:  4883 - [==============>---------------]  49% - acc: 0.8359 - loss: 1.6305 - 8435.1 sample/sec\n",
            "Global step:  4893 - [==============>---------------]  51% - acc: 0.8281 - loss: 1.6329 - 8607.4 sample/sec\n",
            "Global step:  4903 - [===============>--------------]  54% - acc: 0.8281 - loss: 1.6356 - 8663.4 sample/sec\n",
            "Global step:  4913 - [================>-------------]  56% - acc: 0.8750 - loss: 1.5965 - 8485.9 sample/sec\n",
            "Global step:  4923 - [=================>------------]  59% - acc: 0.8125 - loss: 1.6439 - 8736.6 sample/sec\n",
            "Global step:  4933 - [=================>------------]  61% - acc: 0.7266 - loss: 1.7303 - 9076.7 sample/sec\n",
            "Global step:  4943 - [==================>-----------]  64% - acc: 0.7969 - loss: 1.6598 - 8784.0 sample/sec\n",
            "Global step:  4953 - [===================>----------]  66% - acc: 0.8047 - loss: 1.6580 - 8321.0 sample/sec\n",
            "Global step:  4963 - [====================>---------]  69% - acc: 0.8203 - loss: 1.6509 - 8509.5 sample/sec\n",
            "Global step:  4973 - [====================>---------]  72% - acc: 0.8750 - loss: 1.5923 - 8622.9 sample/sec\n",
            "Global step:  4983 - [=====================>--------]  74% - acc: 0.8281 - loss: 1.6400 - 8334.4 sample/sec\n",
            "Global step:  4993 - [======================>-------]  77% - acc: 0.8516 - loss: 1.6113 - 8332.2 sample/sec\n",
            "Global step:  5003 - [======================>-------]  79% - acc: 0.7578 - loss: 1.6974 - 8626.1 sample/sec\n",
            "Global step:  5013 - [=======================>------]  82% - acc: 0.8359 - loss: 1.6244 - 8415.8 sample/sec\n",
            "Global step:  5023 - [========================>-----]  84% - acc: 0.9062 - loss: 1.5575 - 8086.6 sample/sec\n",
            "Global step:  5033 - [=========================>----]  87% - acc: 0.8594 - loss: 1.6031 - 8641.4 sample/sec\n",
            "Global step:  5043 - [==========================>---]  90% - acc: 0.8203 - loss: 1.6415 - 8862.3 sample/sec\n",
            "Global step:  5053 - [==========================>---]  92% - acc: 0.8594 - loss: 1.6040 - 8871.1 sample/sec\n",
            "Global step:  5063 - [===========================>--]  95% - acc: 0.8047 - loss: 1.6527 - 8558.8 sample/sec\n",
            "Global step:  5073 - [============================>-]  97% - acc: 0.8516 - loss: 1.6162 - 8775.0 sample/sec\n",
            "Global step:  5083 - [=============================>] 100% - acc: 0.8250 - loss: 1.6384 - 12270.5 sample/sec\n",
            "\n",
            "Epoch 13 - accuracy: 75.00% (7500/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 14/60\n",
            "\n",
            "Global step:  5084 - [>-----------------------------]   0% - acc: 0.8516 - loss: 1.6001 - 8842.3 sample/sec\n",
            "Global step:  5094 - [>-----------------------------]   3% - acc: 0.8438 - loss: 1.6177 - 8729.8 sample/sec\n",
            "Global step:  5104 - [=>----------------------------]   5% - acc: 0.8281 - loss: 1.6321 - 8458.9 sample/sec\n",
            "Global step:  5114 - [==>---------------------------]   8% - acc: 0.7812 - loss: 1.6785 - 8680.3 sample/sec\n",
            "Global step:  5124 - [==>---------------------------]  10% - acc: 0.8047 - loss: 1.6555 - 8705.3 sample/sec\n",
            "Global step:  5134 - [===>--------------------------]  13% - acc: 0.8281 - loss: 1.6320 - 8467.3 sample/sec\n",
            "Global step:  5144 - [====>-------------------------]  15% - acc: 0.8359 - loss: 1.6295 - 8702.7 sample/sec\n",
            "Global step:  5154 - [=====>------------------------]  18% - acc: 0.8438 - loss: 1.6313 - 8701.6 sample/sec\n",
            "Global step:  5164 - [=====>------------------------]  20% - acc: 0.8750 - loss: 1.5791 - 8552.4 sample/sec\n",
            "Global step:  5174 - [======>-----------------------]  23% - acc: 0.8750 - loss: 1.5868 - 8618.9 sample/sec\n",
            "Global step:  5184 - [=======>----------------------]  26% - acc: 0.8984 - loss: 1.5665 - 8678.1 sample/sec\n",
            "Global step:  5194 - [========>---------------------]  28% - acc: 0.8750 - loss: 1.5930 - 8563.4 sample/sec\n",
            "Global step:  5204 - [========>---------------------]  31% - acc: 0.8594 - loss: 1.5990 - 8723.9 sample/sec\n",
            "Global step:  5214 - [=========>--------------------]  33% - acc: 0.8125 - loss: 1.6453 - 8675.6 sample/sec\n",
            "Global step:  5224 - [==========>-------------------]  36% - acc: 0.8672 - loss: 1.5999 - 8566.2 sample/sec\n",
            "Global step:  5234 - [===========>------------------]  38% - acc: 0.8828 - loss: 1.5795 - 8710.3 sample/sec\n",
            "Global step:  5244 - [===========>------------------]  41% - acc: 0.8594 - loss: 1.6116 - 8664.7 sample/sec\n",
            "Global step:  5254 - [============>-----------------]  43% - acc: 0.8125 - loss: 1.6465 - 8502.3 sample/sec\n",
            "Global step:  5264 - [=============>----------------]  46% - acc: 0.8203 - loss: 1.6379 - 8607.1 sample/sec\n",
            "Global step:  5274 - [==============>---------------]  49% - acc: 0.8281 - loss: 1.6449 - 8545.6 sample/sec\n",
            "Global step:  5284 - [==============>---------------]  51% - acc: 0.8359 - loss: 1.6219 - 8650.1 sample/sec\n",
            "Global step:  5294 - [===============>--------------]  54% - acc: 0.8516 - loss: 1.6146 - 8779.7 sample/sec\n",
            "Global step:  5304 - [================>-------------]  56% - acc: 0.8750 - loss: 1.5898 - 8292.5 sample/sec\n",
            "Global step:  5314 - [=================>------------]  59% - acc: 0.8281 - loss: 1.6307 - 8697.6 sample/sec\n",
            "Global step:  5324 - [=================>------------]  61% - acc: 0.7578 - loss: 1.6947 - 8532.2 sample/sec\n",
            "Global step:  5334 - [==================>-----------]  64% - acc: 0.8203 - loss: 1.6471 - 8357.3 sample/sec\n",
            "Global step:  5344 - [===================>----------]  66% - acc: 0.8125 - loss: 1.6528 - 8738.7 sample/sec\n",
            "Global step:  5354 - [====================>---------]  69% - acc: 0.7969 - loss: 1.6667 - 7613.7 sample/sec\n",
            "Global step:  5364 - [====================>---------]  72% - acc: 0.8672 - loss: 1.5952 - 8623.3 sample/sec\n",
            "Global step:  5374 - [=====================>--------]  74% - acc: 0.8281 - loss: 1.6363 - 8647.9 sample/sec\n",
            "Global step:  5384 - [======================>-------]  77% - acc: 0.8828 - loss: 1.5850 - 8598.7 sample/sec\n",
            "Global step:  5394 - [======================>-------]  79% - acc: 0.7812 - loss: 1.6706 - 8506.1 sample/sec\n",
            "Global step:  5404 - [=======================>------]  82% - acc: 0.8047 - loss: 1.6621 - 8702.0 sample/sec\n",
            "Global step:  5414 - [========================>-----]  84% - acc: 0.8828 - loss: 1.5808 - 8577.3 sample/sec\n",
            "Global step:  5424 - [=========================>----]  87% - acc: 0.8516 - loss: 1.5979 - 8651.0 sample/sec\n",
            "Global step:  5434 - [==========================>---]  90% - acc: 0.8047 - loss: 1.6529 - 8550.7 sample/sec\n",
            "Global step:  5444 - [==========================>---]  92% - acc: 0.8750 - loss: 1.5901 - 8619.4 sample/sec\n",
            "Global step:  5454 - [===========================>--]  95% - acc: 0.8516 - loss: 1.6135 - 8724.1 sample/sec\n",
            "Global step:  5464 - [============================>-]  97% - acc: 0.8438 - loss: 1.6171 - 8460.0 sample/sec\n",
            "Global step:  5474 - [=============================>] 100% - acc: 0.8375 - loss: 1.6190 - 12762.6 sample/sec\n",
            "\n",
            "Epoch 14 - accuracy: 76.09% (7609/10000)\n",
            "This epoch receive better accuracy: 76.09 > 75.31. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 15/60\n",
            "\n",
            "Global step:  5475 - [>-----------------------------]   0% - acc: 0.8984 - loss: 1.5624 - 9398.5 sample/sec\n",
            "Global step:  5485 - [>-----------------------------]   3% - acc: 0.8750 - loss: 1.5841 - 8652.6 sample/sec\n",
            "Global step:  5495 - [=>----------------------------]   5% - acc: 0.9141 - loss: 1.5620 - 8874.2 sample/sec\n",
            "Global step:  5505 - [==>---------------------------]   8% - acc: 0.8438 - loss: 1.6237 - 8533.3 sample/sec\n",
            "Global step:  5515 - [==>---------------------------]  10% - acc: 0.8047 - loss: 1.6411 - 8783.5 sample/sec\n",
            "Global step:  5525 - [===>--------------------------]  13% - acc: 0.8281 - loss: 1.6327 - 8647.2 sample/sec\n",
            "Global step:  5535 - [====>-------------------------]  15% - acc: 0.8203 - loss: 1.6429 - 7948.0 sample/sec\n",
            "Global step:  5545 - [=====>------------------------]  18% - acc: 0.8594 - loss: 1.6080 - 8593.8 sample/sec\n",
            "Global step:  5555 - [=====>------------------------]  20% - acc: 0.8906 - loss: 1.5746 - 8859.3 sample/sec\n",
            "Global step:  5565 - [======>-----------------------]  23% - acc: 0.8281 - loss: 1.6274 - 8822.9 sample/sec\n",
            "Global step:  5575 - [=======>----------------------]  26% - acc: 0.9141 - loss: 1.5474 - 8955.3 sample/sec\n",
            "Global step:  5585 - [========>---------------------]  28% - acc: 0.9062 - loss: 1.5555 - 8841.5 sample/sec\n",
            "Global step:  5595 - [========>---------------------]  31% - acc: 0.8672 - loss: 1.5929 - 8540.9 sample/sec\n",
            "Global step:  5605 - [=========>--------------------]  33% - acc: 0.7969 - loss: 1.6559 - 8559.0 sample/sec\n",
            "Global step:  5615 - [==========>-------------------]  36% - acc: 0.8438 - loss: 1.6225 - 8211.3 sample/sec\n",
            "Global step:  5625 - [===========>------------------]  38% - acc: 0.8438 - loss: 1.6130 - 8732.9 sample/sec\n",
            "Global step:  5635 - [===========>------------------]  41% - acc: 0.8359 - loss: 1.6197 - 8448.1 sample/sec\n",
            "Global step:  5645 - [============>-----------------]  43% - acc: 0.8594 - loss: 1.6060 - 8596.1 sample/sec\n",
            "Global step:  5655 - [=============>----------------]  46% - acc: 0.8594 - loss: 1.5974 - 8267.9 sample/sec\n",
            "Global step:  5665 - [==============>---------------]  49% - acc: 0.8125 - loss: 1.6497 - 8731.7 sample/sec\n",
            "Global step:  5675 - [==============>---------------]  51% - acc: 0.8047 - loss: 1.6513 - 8539.1 sample/sec\n",
            "Global step:  5685 - [===============>--------------]  54% - acc: 0.8516 - loss: 1.6077 - 8809.1 sample/sec\n",
            "Global step:  5695 - [================>-------------]  56% - acc: 0.8672 - loss: 1.5948 - 8494.8 sample/sec\n",
            "Global step:  5705 - [=================>------------]  59% - acc: 0.8750 - loss: 1.5914 - 8829.1 sample/sec\n",
            "Global step:  5715 - [=================>------------]  61% - acc: 0.7734 - loss: 1.6877 - 8693.4 sample/sec\n",
            "Global step:  5725 - [==================>-----------]  64% - acc: 0.8125 - loss: 1.6408 - 8644.4 sample/sec\n",
            "Global step:  5735 - [===================>----------]  66% - acc: 0.8281 - loss: 1.6381 - 8728.0 sample/sec\n",
            "Global step:  5745 - [====================>---------]  69% - acc: 0.8281 - loss: 1.6328 - 8774.6 sample/sec\n",
            "Global step:  5755 - [====================>---------]  72% - acc: 0.8750 - loss: 1.5880 - 8723.2 sample/sec\n",
            "Global step:  5765 - [=====================>--------]  74% - acc: 0.8359 - loss: 1.6203 - 8784.6 sample/sec\n",
            "Global step:  5775 - [======================>-------]  77% - acc: 0.8828 - loss: 1.5846 - 8543.6 sample/sec\n",
            "Global step:  5785 - [======================>-------]  79% - acc: 0.7969 - loss: 1.6631 - 8549.2 sample/sec\n",
            "Global step:  5795 - [=======================>------]  82% - acc: 0.8203 - loss: 1.6436 - 8625.0 sample/sec\n",
            "Global step:  5805 - [========================>-----]  84% - acc: 0.9375 - loss: 1.5223 - 8627.1 sample/sec\n",
            "Global step:  5815 - [=========================>----]  87% - acc: 0.8984 - loss: 1.5637 - 8592.7 sample/sec\n",
            "Global step:  5825 - [==========================>---]  90% - acc: 0.8359 - loss: 1.6306 - 8636.8 sample/sec\n",
            "Global step:  5835 - [==========================>---]  92% - acc: 0.8516 - loss: 1.6119 - 8865.3 sample/sec\n",
            "Global step:  5845 - [===========================>--]  95% - acc: 0.8438 - loss: 1.6222 - 8744.3 sample/sec\n",
            "Global step:  5855 - [============================>-]  97% - acc: 0.8906 - loss: 1.5772 - 8674.3 sample/sec\n",
            "Global step:  5865 - [=============================>] 100% - acc: 0.8375 - loss: 1.6281 - 11986.9 sample/sec\n",
            "\n",
            "Epoch 15 - accuracy: 75.76% (7576/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 16/60\n",
            "\n",
            "Global step:  5866 - [>-----------------------------]   0% - acc: 0.8906 - loss: 1.5616 - 8934.4 sample/sec\n",
            "Global step:  5876 - [>-----------------------------]   3% - acc: 0.8672 - loss: 1.5919 - 8538.4 sample/sec\n",
            "Global step:  5886 - [=>----------------------------]   5% - acc: 0.8984 - loss: 1.5696 - 8784.3 sample/sec\n",
            "Global step:  5896 - [==>---------------------------]   8% - acc: 0.8125 - loss: 1.6422 - 8745.5 sample/sec\n",
            "Global step:  5906 - [==>---------------------------]  10% - acc: 0.8359 - loss: 1.6316 - 8416.1 sample/sec\n",
            "Global step:  5916 - [===>--------------------------]  13% - acc: 0.8516 - loss: 1.6059 - 8901.4 sample/sec\n",
            "Global step:  5926 - [====>-------------------------]  15% - acc: 0.8828 - loss: 1.5756 - 8339.5 sample/sec\n",
            "Global step:  5936 - [=====>------------------------]  18% - acc: 0.8281 - loss: 1.6276 - 8731.2 sample/sec\n",
            "Global step:  5946 - [=====>------------------------]  20% - acc: 0.8906 - loss: 1.5742 - 8428.6 sample/sec\n",
            "Global step:  5956 - [======>-----------------------]  23% - acc: 0.8672 - loss: 1.5957 - 8719.3 sample/sec\n",
            "Global step:  5966 - [=======>----------------------]  26% - acc: 0.9219 - loss: 1.5434 - 8879.0 sample/sec\n",
            "Global step:  5976 - [========>---------------------]  28% - acc: 0.8906 - loss: 1.5726 - 8522.0 sample/sec\n",
            "Global step:  5986 - [========>---------------------]  31% - acc: 0.8750 - loss: 1.5888 - 8675.6 sample/sec\n",
            "Global step:  5996 - [=========>--------------------]  33% - acc: 0.8750 - loss: 1.5899 - 8581.3 sample/sec\n",
            "Global step:  6006 - [==========>-------------------]  36% - acc: 0.8516 - loss: 1.6157 - 8639.4 sample/sec\n",
            "Global step:  6016 - [===========>------------------]  38% - acc: 0.9062 - loss: 1.5625 - 8882.0 sample/sec\n",
            "Global step:  6026 - [===========>------------------]  41% - acc: 0.8359 - loss: 1.6094 - 8748.4 sample/sec\n",
            "Global step:  6036 - [============>-----------------]  43% - acc: 0.8281 - loss: 1.6284 - 8656.0 sample/sec\n",
            "Global step:  6046 - [=============>----------------]  46% - acc: 0.8828 - loss: 1.5777 - 7625.0 sample/sec\n",
            "Global step:  6056 - [==============>---------------]  49% - acc: 0.7969 - loss: 1.6540 - 8801.9 sample/sec\n",
            "Global step:  6066 - [==============>---------------]  51% - acc: 0.8125 - loss: 1.6419 - 8812.1 sample/sec\n",
            "Global step:  6076 - [===============>--------------]  54% - acc: 0.8906 - loss: 1.5773 - 8646.2 sample/sec\n",
            "Global step:  6086 - [================>-------------]  56% - acc: 0.8750 - loss: 1.5809 - 8670.8 sample/sec\n",
            "Global step:  6096 - [=================>------------]  59% - acc: 0.8672 - loss: 1.5901 - 8827.1 sample/sec\n",
            "Global step:  6106 - [=================>------------]  61% - acc: 0.8203 - loss: 1.6449 - 8638.3 sample/sec\n",
            "Global step:  6116 - [==================>-----------]  64% - acc: 0.8047 - loss: 1.6546 - 8681.6 sample/sec\n",
            "Global step:  6126 - [===================>----------]  66% - acc: 0.8203 - loss: 1.6434 - 8710.1 sample/sec\n",
            "Global step:  6136 - [====================>---------]  69% - acc: 0.8750 - loss: 1.5825 - 8731.6 sample/sec\n",
            "Global step:  6146 - [====================>---------]  72% - acc: 0.8906 - loss: 1.5655 - 8506.1 sample/sec\n",
            "Global step:  6156 - [=====================>--------]  74% - acc: 0.8672 - loss: 1.6063 - 8828.7 sample/sec\n",
            "Global step:  6166 - [======================>-------]  77% - acc: 0.8672 - loss: 1.5938 - 8667.3 sample/sec\n",
            "Global step:  6176 - [======================>-------]  79% - acc: 0.8047 - loss: 1.6603 - 8656.1 sample/sec\n",
            "Global step:  6186 - [=======================>------]  82% - acc: 0.8047 - loss: 1.6512 - 8898.8 sample/sec\n",
            "Global step:  6196 - [========================>-----]  84% - acc: 0.9375 - loss: 1.5304 - 8699.5 sample/sec\n",
            "Global step:  6206 - [=========================>----]  87% - acc: 0.8750 - loss: 1.5874 - 8650.6 sample/sec\n",
            "Global step:  6216 - [==========================>---]  90% - acc: 0.8516 - loss: 1.6154 - 8581.6 sample/sec\n",
            "Global step:  6226 - [==========================>---]  92% - acc: 0.8828 - loss: 1.5868 - 9003.2 sample/sec\n",
            "Global step:  6236 - [===========================>--]  95% - acc: 0.8438 - loss: 1.6262 - 8828.8 sample/sec\n",
            "Global step:  6246 - [============================>-]  97% - acc: 0.8594 - loss: 1.5998 - 8584.4 sample/sec\n",
            "Global step:  6256 - [=============================>] 100% - acc: 0.8625 - loss: 1.6012 - 12862.9 sample/sec\n",
            "\n",
            "Epoch 16 - accuracy: 75.08% (7508/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 17/60\n",
            "\n",
            "Global step:  6257 - [>-----------------------------]   0% - acc: 0.8906 - loss: 1.5698 - 8930.3 sample/sec\n",
            "Global step:  6267 - [>-----------------------------]   3% - acc: 0.8906 - loss: 1.5754 - 8767.4 sample/sec\n",
            "Global step:  6277 - [=>----------------------------]   5% - acc: 0.8906 - loss: 1.5758 - 8645.3 sample/sec\n",
            "Global step:  6287 - [==>---------------------------]   8% - acc: 0.8438 - loss: 1.6209 - 8713.7 sample/sec\n",
            "Global step:  6297 - [==>---------------------------]  10% - acc: 0.8125 - loss: 1.6436 - 8594.9 sample/sec\n",
            "Global step:  6307 - [===>--------------------------]  13% - acc: 0.8438 - loss: 1.6089 - 8409.0 sample/sec\n",
            "Global step:  6317 - [====>-------------------------]  15% - acc: 0.8750 - loss: 1.5744 - 8651.1 sample/sec\n",
            "Global step:  6327 - [=====>------------------------]  18% - acc: 0.8359 - loss: 1.6187 - 8642.2 sample/sec\n",
            "Global step:  6337 - [=====>------------------------]  20% - acc: 0.9062 - loss: 1.5586 - 9001.4 sample/sec\n",
            "Global step:  6347 - [======>-----------------------]  23% - acc: 0.9141 - loss: 1.5487 - 8663.7 sample/sec\n",
            "Global step:  6357 - [=======>----------------------]  26% - acc: 0.9062 - loss: 1.5566 - 8634.3 sample/sec\n",
            "Global step:  6367 - [========>---------------------]  28% - acc: 0.8828 - loss: 1.5828 - 8761.9 sample/sec\n",
            "Global step:  6377 - [========>---------------------]  31% - acc: 0.8984 - loss: 1.5658 - 8837.8 sample/sec\n",
            "Global step:  6387 - [=========>--------------------]  33% - acc: 0.8828 - loss: 1.5756 - 8430.4 sample/sec\n",
            "Global step:  6397 - [==========>-------------------]  36% - acc: 0.8672 - loss: 1.5961 - 8831.7 sample/sec\n",
            "Global step:  6407 - [===========>------------------]  38% - acc: 0.8750 - loss: 1.5816 - 8507.9 sample/sec\n",
            "Global step:  6417 - [===========>------------------]  41% - acc: 0.8984 - loss: 1.5666 - 8767.2 sample/sec\n",
            "Global step:  6427 - [============>-----------------]  43% - acc: 0.8594 - loss: 1.6078 - 8574.7 sample/sec\n",
            "Global step:  6437 - [=============>----------------]  46% - acc: 0.8359 - loss: 1.6136 - 8472.1 sample/sec\n",
            "Global step:  6447 - [==============>---------------]  49% - acc: 0.8438 - loss: 1.6145 - 8749.0 sample/sec\n",
            "Global step:  6457 - [==============>---------------]  51% - acc: 0.8516 - loss: 1.6142 - 8738.6 sample/sec\n",
            "Global step:  6467 - [===============>--------------]  54% - acc: 0.8906 - loss: 1.5801 - 8656.0 sample/sec\n",
            "Global step:  6477 - [================>-------------]  56% - acc: 0.8828 - loss: 1.5765 - 8709.2 sample/sec\n",
            "Global step:  6487 - [=================>------------]  59% - acc: 0.8750 - loss: 1.5877 - 8562.9 sample/sec\n",
            "Global step:  6497 - [=================>------------]  61% - acc: 0.7969 - loss: 1.6659 - 8964.4 sample/sec\n",
            "Global step:  6507 - [==================>-----------]  64% - acc: 0.8203 - loss: 1.6388 - 8745.0 sample/sec\n",
            "Global step:  6517 - [===================>----------]  66% - acc: 0.8516 - loss: 1.6098 - 8851.8 sample/sec\n",
            "Global step:  6527 - [====================>---------]  69% - acc: 0.8672 - loss: 1.5970 - 8706.1 sample/sec\n",
            "Global step:  6537 - [====================>---------]  72% - acc: 0.8984 - loss: 1.5645 - 8851.1 sample/sec\n",
            "Global step:  6547 - [=====================>--------]  74% - acc: 0.8672 - loss: 1.5942 - 8347.0 sample/sec\n",
            "Global step:  6557 - [======================>-------]  77% - acc: 0.9062 - loss: 1.5572 - 8748.1 sample/sec\n",
            "Global step:  6567 - [======================>-------]  79% - acc: 0.8047 - loss: 1.6572 - 8737.0 sample/sec\n",
            "Global step:  6577 - [=======================>------]  82% - acc: 0.8438 - loss: 1.6274 - 8751.8 sample/sec\n",
            "Global step:  6587 - [========================>-----]  84% - acc: 0.9141 - loss: 1.5503 - 8729.8 sample/sec\n",
            "Global step:  6597 - [=========================>----]  87% - acc: 0.9062 - loss: 1.5517 - 8907.2 sample/sec\n",
            "Global step:  6607 - [==========================>---]  90% - acc: 0.8672 - loss: 1.6003 - 8978.7 sample/sec\n",
            "Global step:  6617 - [==========================>---]  92% - acc: 0.8828 - loss: 1.5858 - 8542.4 sample/sec\n",
            "Global step:  6627 - [===========================>--]  95% - acc: 0.8594 - loss: 1.6040 - 8543.3 sample/sec\n",
            "Global step:  6637 - [============================>-]  97% - acc: 0.8516 - loss: 1.6040 - 8853.6 sample/sec\n",
            "Global step:  6647 - [=============================>] 100% - acc: 0.8750 - loss: 1.5825 - 12608.8 sample/sec\n",
            "\n",
            "Epoch 17 - accuracy: 75.43% (7543/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 18/60\n",
            "\n",
            "Global step:  6648 - [>-----------------------------]   0% - acc: 0.8516 - loss: 1.5934 - 8798.9 sample/sec\n",
            "Global step:  6658 - [>-----------------------------]   3% - acc: 0.8906 - loss: 1.5670 - 8450.4 sample/sec\n",
            "Global step:  6668 - [=>----------------------------]   5% - acc: 0.9219 - loss: 1.5417 - 8965.2 sample/sec\n",
            "Global step:  6678 - [==>---------------------------]   8% - acc: 0.8828 - loss: 1.5728 - 8644.8 sample/sec\n",
            "Global step:  6688 - [==>---------------------------]  10% - acc: 0.8516 - loss: 1.6048 - 8531.0 sample/sec\n",
            "Global step:  6698 - [===>--------------------------]  13% - acc: 0.8516 - loss: 1.6087 - 8942.9 sample/sec\n",
            "Global step:  6708 - [====>-------------------------]  15% - acc: 0.8984 - loss: 1.5725 - 8675.0 sample/sec\n",
            "Global step:  6718 - [=====>------------------------]  18% - acc: 0.8281 - loss: 1.6391 - 8845.4 sample/sec\n",
            "Global step:  6728 - [=====>------------------------]  20% - acc: 0.8984 - loss: 1.5682 - 8758.2 sample/sec\n",
            "Global step:  6738 - [======>-----------------------]  23% - acc: 0.8984 - loss: 1.5557 - 8819.2 sample/sec\n",
            "Global step:  6748 - [=======>----------------------]  26% - acc: 0.9219 - loss: 1.5407 - 8766.8 sample/sec\n",
            "Global step:  6758 - [========>---------------------]  28% - acc: 0.8984 - loss: 1.5628 - 8953.8 sample/sec\n",
            "Global step:  6768 - [========>---------------------]  31% - acc: 0.9062 - loss: 1.5618 - 8659.3 sample/sec\n",
            "Global step:  6778 - [=========>--------------------]  33% - acc: 0.8906 - loss: 1.5703 - 8937.0 sample/sec\n",
            "Global step:  6788 - [==========>-------------------]  36% - acc: 0.8672 - loss: 1.5882 - 8978.7 sample/sec\n",
            "Global step:  6798 - [===========>------------------]  38% - acc: 0.8750 - loss: 1.5870 - 8845.7 sample/sec\n",
            "Global step:  6808 - [===========>------------------]  41% - acc: 0.8750 - loss: 1.5927 - 8733.0 sample/sec\n",
            "Global step:  6818 - [============>-----------------]  43% - acc: 0.8906 - loss: 1.5684 - 8756.5 sample/sec\n",
            "Global step:  6828 - [=============>----------------]  46% - acc: 0.8594 - loss: 1.6027 - 8706.1 sample/sec\n",
            "Global step:  6838 - [==============>---------------]  49% - acc: 0.8594 - loss: 1.6044 - 8825.5 sample/sec\n",
            "Global step:  6848 - [==============>---------------]  51% - acc: 0.8672 - loss: 1.5972 - 8810.4 sample/sec\n",
            "Global step:  6858 - [===============>--------------]  54% - acc: 0.8828 - loss: 1.5840 - 8634.3 sample/sec\n",
            "Global step:  6868 - [================>-------------]  56% - acc: 0.8984 - loss: 1.5595 - 8531.8 sample/sec\n",
            "Global step:  6878 - [=================>------------]  59% - acc: 0.8828 - loss: 1.5769 - 8751.5 sample/sec\n",
            "Global step:  6888 - [=================>------------]  61% - acc: 0.8047 - loss: 1.6587 - 8823.1 sample/sec\n",
            "Global step:  6898 - [==================>-----------]  64% - acc: 0.8359 - loss: 1.6267 - 8324.4 sample/sec\n",
            "Global step:  6908 - [===================>----------]  66% - acc: 0.8438 - loss: 1.6161 - 8864.2 sample/sec\n",
            "Global step:  6918 - [====================>---------]  69% - acc: 0.8672 - loss: 1.5909 - 8866.7 sample/sec\n",
            "Global step:  6928 - [====================>---------]  72% - acc: 0.8984 - loss: 1.5613 - 8733.6 sample/sec\n",
            "Global step:  6938 - [=====================>--------]  74% - acc: 0.8828 - loss: 1.5771 - 8663.5 sample/sec\n",
            "Global step:  6948 - [======================>-------]  77% - acc: 0.8750 - loss: 1.5817 - 8638.4 sample/sec\n",
            "Global step:  6958 - [======================>-------]  79% - acc: 0.8125 - loss: 1.6477 - 8820.7 sample/sec\n",
            "Global step:  6968 - [=======================>------]  82% - acc: 0.8359 - loss: 1.6233 - 8698.2 sample/sec\n",
            "Global step:  6978 - [========================>-----]  84% - acc: 0.9219 - loss: 1.5348 - 8904.2 sample/sec\n",
            "Global step:  6988 - [=========================>----]  87% - acc: 0.8906 - loss: 1.5681 - 8819.2 sample/sec\n",
            "Global step:  6998 - [==========================>---]  90% - acc: 0.8516 - loss: 1.6079 - 8581.6 sample/sec\n",
            "Global step:  7008 - [==========================>---]  92% - acc: 0.8750 - loss: 1.5930 - 8664.1 sample/sec\n",
            "Global step:  7018 - [===========================>--]  95% - acc: 0.8750 - loss: 1.5864 - 8789.2 sample/sec\n",
            "Global step:  7028 - [============================>-]  97% - acc: 0.8594 - loss: 1.6069 - 8356.5 sample/sec\n",
            "Global step:  7038 - [=============================>] 100% - acc: 0.8375 - loss: 1.6278 - 12786.9 sample/sec\n",
            "\n",
            "Epoch 18 - accuracy: 74.96% (7496/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 19/60\n",
            "\n",
            "Global step:  7039 - [>-----------------------------]   0% - acc: 0.8984 - loss: 1.5623 - 9031.5 sample/sec\n",
            "Global step:  7049 - [>-----------------------------]   3% - acc: 0.8906 - loss: 1.5737 - 8861.0 sample/sec\n",
            "Global step:  7059 - [=>----------------------------]   5% - acc: 0.9062 - loss: 1.5551 - 8949.0 sample/sec\n",
            "Global step:  7069 - [==>---------------------------]   8% - acc: 0.8594 - loss: 1.6042 - 8704.3 sample/sec\n",
            "Global step:  7079 - [==>---------------------------]  10% - acc: 0.8750 - loss: 1.5862 - 8846.0 sample/sec\n",
            "Global step:  7089 - [===>--------------------------]  13% - acc: 0.8828 - loss: 1.5731 - 8735.9 sample/sec\n",
            "Global step:  7099 - [====>-------------------------]  15% - acc: 0.8984 - loss: 1.5557 - 8878.4 sample/sec\n",
            "Global step:  7109 - [=====>------------------------]  18% - acc: 0.8516 - loss: 1.6061 - 8909.7 sample/sec\n",
            "Global step:  7119 - [=====>------------------------]  20% - acc: 0.8906 - loss: 1.5750 - 8758.1 sample/sec\n",
            "Global step:  7129 - [======>-----------------------]  23% - acc: 0.9062 - loss: 1.5440 - 8710.5 sample/sec\n",
            "Global step:  7139 - [=======>----------------------]  26% - acc: 0.9141 - loss: 1.5442 - 8872.1 sample/sec\n",
            "Global step:  7149 - [========>---------------------]  28% - acc: 0.8906 - loss: 1.5735 - 8777.3 sample/sec\n",
            "Global step:  7159 - [========>---------------------]  31% - acc: 0.8672 - loss: 1.5904 - 8749.0 sample/sec\n",
            "Global step:  7169 - [=========>--------------------]  33% - acc: 0.8672 - loss: 1.5926 - 8761.7 sample/sec\n",
            "Global step:  7179 - [==========>-------------------]  36% - acc: 0.8672 - loss: 1.5910 - 8918.6 sample/sec\n",
            "Global step:  7189 - [===========>------------------]  38% - acc: 0.9141 - loss: 1.5503 - 8757.2 sample/sec\n",
            "Global step:  7199 - [===========>------------------]  41% - acc: 0.9062 - loss: 1.5569 - 8813.9 sample/sec\n",
            "Global step:  7209 - [============>-----------------]  43% - acc: 0.8594 - loss: 1.6018 - 8858.5 sample/sec\n",
            "Global step:  7219 - [=============>----------------]  46% - acc: 0.8750 - loss: 1.5920 - 8813.4 sample/sec\n",
            "Global step:  7229 - [==============>---------------]  49% - acc: 0.7969 - loss: 1.6710 - 8976.3 sample/sec\n",
            "Global step:  7239 - [==============>---------------]  51% - acc: 0.8281 - loss: 1.6163 - 9115.9 sample/sec\n",
            "Global step:  7249 - [===============>--------------]  54% - acc: 0.8125 - loss: 1.6421 - 8527.0 sample/sec\n",
            "Global step:  7259 - [================>-------------]  56% - acc: 0.8828 - loss: 1.5823 - 9038.5 sample/sec\n",
            "Global step:  7269 - [=================>------------]  59% - acc: 0.8984 - loss: 1.5638 - 8640.0 sample/sec\n",
            "Global step:  7279 - [=================>------------]  61% - acc: 0.8047 - loss: 1.6587 - 8912.6 sample/sec\n",
            "Global step:  7289 - [==================>-----------]  64% - acc: 0.8047 - loss: 1.6493 - 8631.4 sample/sec\n",
            "Global step:  7299 - [===================>----------]  66% - acc: 0.8594 - loss: 1.5990 - 8648.0 sample/sec\n",
            "Global step:  7309 - [====================>---------]  69% - acc: 0.8672 - loss: 1.5883 - 8769.0 sample/sec\n",
            "Global step:  7319 - [====================>---------]  72% - acc: 0.9062 - loss: 1.5589 - 7487.9 sample/sec\n",
            "Global step:  7329 - [=====================>--------]  74% - acc: 0.8984 - loss: 1.5670 - 8571.4 sample/sec\n",
            "Global step:  7339 - [======================>-------]  77% - acc: 0.8984 - loss: 1.5569 - 8747.4 sample/sec\n",
            "Global step:  7349 - [======================>-------]  79% - acc: 0.8359 - loss: 1.6280 - 8745.3 sample/sec\n",
            "Global step:  7359 - [=======================>------]  82% - acc: 0.8359 - loss: 1.6215 - 8825.9 sample/sec\n",
            "Global step:  7369 - [========================>-----]  84% - acc: 0.9453 - loss: 1.5127 - 8747.4 sample/sec\n",
            "Global step:  7379 - [=========================>----]  87% - acc: 0.8984 - loss: 1.5600 - 8711.8 sample/sec\n",
            "Global step:  7389 - [==========================>---]  90% - acc: 0.8828 - loss: 1.5858 - 8693.1 sample/sec\n",
            "Global step:  7399 - [==========================>---]  92% - acc: 0.8750 - loss: 1.5897 - 8841.7 sample/sec\n",
            "Global step:  7409 - [===========================>--]  95% - acc: 0.8672 - loss: 1.6007 - 8684.3 sample/sec\n",
            "Global step:  7419 - [============================>-]  97% - acc: 0.9219 - loss: 1.5401 - 7518.4 sample/sec\n",
            "Global step:  7429 - [=============================>] 100% - acc: 0.9000 - loss: 1.5668 - 12593.7 sample/sec\n",
            "\n",
            "Epoch 19 - accuracy: 76.01% (7601/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 20/60\n",
            "\n",
            "Global step:  7430 - [>-----------------------------]   0% - acc: 0.9297 - loss: 1.5350 - 9145.7 sample/sec\n",
            "Global step:  7440 - [>-----------------------------]   3% - acc: 0.9219 - loss: 1.5431 - 8927.6 sample/sec\n",
            "Global step:  7450 - [=>----------------------------]   5% - acc: 0.9453 - loss: 1.5246 - 8774.6 sample/sec\n",
            "Global step:  7460 - [==>---------------------------]   8% - acc: 0.8828 - loss: 1.5788 - 8834.2 sample/sec\n",
            "Global step:  7470 - [==>---------------------------]  10% - acc: 0.8984 - loss: 1.5688 - 8906.4 sample/sec\n",
            "Global step:  7480 - [===>--------------------------]  13% - acc: 0.8984 - loss: 1.5725 - 8754.4 sample/sec\n",
            "Global step:  7490 - [====>-------------------------]  15% - acc: 0.8906 - loss: 1.5694 - 8805.6 sample/sec\n",
            "Global step:  7500 - [=====>------------------------]  18% - acc: 0.8438 - loss: 1.6165 - 8755.8 sample/sec\n",
            "Global step:  7510 - [=====>------------------------]  20% - acc: 0.9062 - loss: 1.5566 - 8763.0 sample/sec\n",
            "Global step:  7520 - [======>-----------------------]  23% - acc: 0.9141 - loss: 1.5462 - 8813.6 sample/sec\n",
            "Global step:  7530 - [=======>----------------------]  26% - acc: 0.9219 - loss: 1.5360 - 8871.8 sample/sec\n",
            "Global step:  7540 - [========>---------------------]  28% - acc: 0.8984 - loss: 1.5689 - 8654.2 sample/sec\n",
            "Global step:  7550 - [========>---------------------]  31% - acc: 0.9141 - loss: 1.5513 - 8898.8 sample/sec\n",
            "Global step:  7560 - [=========>--------------------]  33% - acc: 0.8906 - loss: 1.5706 - 8860.3 sample/sec\n",
            "Global step:  7570 - [==========>-------------------]  36% - acc: 0.8906 - loss: 1.5718 - 8685.1 sample/sec\n",
            "Global step:  7580 - [===========>------------------]  38% - acc: 0.9062 - loss: 1.5555 - 8650.1 sample/sec\n",
            "Global step:  7590 - [===========>------------------]  41% - acc: 0.9141 - loss: 1.5487 - 8911.6 sample/sec\n",
            "Global step:  7600 - [============>-----------------]  43% - acc: 0.8828 - loss: 1.5809 - 8923.0 sample/sec\n",
            "Global step:  7610 - [=============>----------------]  46% - acc: 0.8828 - loss: 1.5800 - 8639.4 sample/sec\n",
            "Global step:  7620 - [==============>---------------]  49% - acc: 0.8438 - loss: 1.6064 - 8876.7 sample/sec\n",
            "Global step:  7630 - [==============>---------------]  51% - acc: 0.8594 - loss: 1.6070 - 8605.2 sample/sec\n",
            "Global step:  7640 - [===============>--------------]  54% - acc: 0.8516 - loss: 1.6030 - 8930.9 sample/sec\n",
            "Global step:  7650 - [================>-------------]  56% - acc: 0.8828 - loss: 1.5723 - 9106.0 sample/sec\n",
            "Global step:  7660 - [=================>------------]  59% - acc: 0.9062 - loss: 1.5597 - 8635.1 sample/sec\n",
            "Global step:  7670 - [=================>------------]  61% - acc: 0.8359 - loss: 1.6288 - 8700.3 sample/sec\n",
            "Global step:  7680 - [==================>-----------]  64% - acc: 0.8047 - loss: 1.6527 - 8875.7 sample/sec\n",
            "Global step:  7690 - [===================>----------]  66% - acc: 0.8672 - loss: 1.6022 - 8843.5 sample/sec\n",
            "Global step:  7700 - [====================>---------]  69% - acc: 0.8516 - loss: 1.6058 - 8898.9 sample/sec\n",
            "Global step:  7710 - [====================>---------]  72% - acc: 0.8750 - loss: 1.5912 - 8928.2 sample/sec\n",
            "Global step:  7720 - [=====================>--------]  74% - acc: 0.8750 - loss: 1.5968 - 8779.4 sample/sec\n",
            "Global step:  7730 - [======================>-------]  77% - acc: 0.9297 - loss: 1.5346 - 9068.3 sample/sec\n",
            "Global step:  7740 - [======================>-------]  79% - acc: 0.8438 - loss: 1.6125 - 8974.9 sample/sec\n",
            "Global step:  7750 - [=======================>------]  82% - acc: 0.8672 - loss: 1.5962 - 8638.2 sample/sec\n",
            "Global step:  7760 - [========================>-----]  84% - acc: 0.9609 - loss: 1.5065 - 8841.2 sample/sec\n",
            "Global step:  7770 - [=========================>----]  87% - acc: 0.8828 - loss: 1.5925 - 8658.2 sample/sec\n",
            "Global step:  7780 - [==========================>---]  90% - acc: 0.8828 - loss: 1.5698 - 8931.5 sample/sec\n",
            "Global step:  7790 - [==========================>---]  92% - acc: 0.8672 - loss: 1.5951 - 8664.2 sample/sec\n",
            "Global step:  7800 - [===========================>--]  95% - acc: 0.8750 - loss: 1.5933 - 8577.7 sample/sec\n",
            "Global step:  7810 - [============================>-]  97% - acc: 0.9297 - loss: 1.5356 - 8690.7 sample/sec\n",
            "Global step:  7820 - [=============================>] 100% - acc: 0.8750 - loss: 1.5804 - 12584.0 sample/sec\n",
            "\n",
            "Epoch 20 - accuracy: 77.23% (7723/10000)\n",
            "This epoch receive better accuracy: 77.23 > 76.09. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 21/60\n",
            "\n",
            "Global step:  7821 - [>-----------------------------]   0% - acc: 0.9297 - loss: 1.5396 - 9193.3 sample/sec\n",
            "Global step:  7831 - [>-----------------------------]   3% - acc: 0.9062 - loss: 1.5525 - 8872.3 sample/sec\n",
            "Global step:  7841 - [=>----------------------------]   5% - acc: 0.9141 - loss: 1.5434 - 8845.2 sample/sec\n",
            "Global step:  7851 - [==>---------------------------]   8% - acc: 0.9219 - loss: 1.5385 - 8560.2 sample/sec\n",
            "Global step:  7861 - [==>---------------------------]  10% - acc: 0.8984 - loss: 1.5673 - 8807.9 sample/sec\n",
            "Global step:  7871 - [===>--------------------------]  13% - acc: 0.8906 - loss: 1.5626 - 8788.9 sample/sec\n",
            "Global step:  7881 - [====>-------------------------]  15% - acc: 0.9219 - loss: 1.5456 - 8541.7 sample/sec\n",
            "Global step:  7891 - [=====>------------------------]  18% - acc: 0.8828 - loss: 1.5786 - 8868.6 sample/sec\n",
            "Global step:  7901 - [=====>------------------------]  20% - acc: 0.8984 - loss: 1.5585 - 8912.8 sample/sec\n",
            "Global step:  7911 - [======>-----------------------]  23% - acc: 0.9062 - loss: 1.5497 - 8847.3 sample/sec\n",
            "Global step:  7921 - [=======>----------------------]  26% - acc: 0.8906 - loss: 1.5642 - 8796.7 sample/sec\n",
            "Global step:  7931 - [========>---------------------]  28% - acc: 0.9062 - loss: 1.5570 - 8889.6 sample/sec\n",
            "Global step:  7941 - [========>---------------------]  31% - acc: 0.8750 - loss: 1.5888 - 8858.2 sample/sec\n",
            "Global step:  7951 - [=========>--------------------]  33% - acc: 0.8984 - loss: 1.5613 - 8808.2 sample/sec\n",
            "Global step:  7961 - [==========>-------------------]  36% - acc: 0.8750 - loss: 1.5953 - 8742.3 sample/sec\n",
            "Global step:  7971 - [===========>------------------]  38% - acc: 0.8906 - loss: 1.5705 - 8854.9 sample/sec\n",
            "Global step:  7981 - [===========>------------------]  41% - acc: 0.8750 - loss: 1.5965 - 8748.8 sample/sec\n",
            "Global step:  7991 - [============>-----------------]  43% - acc: 0.8672 - loss: 1.5981 - 8739.3 sample/sec\n",
            "Global step:  8001 - [=============>----------------]  46% - acc: 0.8828 - loss: 1.5764 - 9020.2 sample/sec\n",
            "Global step:  8011 - [==============>---------------]  49% - acc: 0.8516 - loss: 1.6085 - 8865.8 sample/sec\n",
            "Global step:  8021 - [==============>---------------]  51% - acc: 0.8125 - loss: 1.6486 - 8810.7 sample/sec\n",
            "Global step:  8031 - [===============>--------------]  54% - acc: 0.8672 - loss: 1.5992 - 8695.1 sample/sec\n",
            "Global step:  8041 - [================>-------------]  56% - acc: 0.8672 - loss: 1.5924 - 8793.4 sample/sec\n",
            "Global step:  8051 - [=================>------------]  59% - acc: 0.8750 - loss: 1.5898 - 8886.2 sample/sec\n",
            "Global step:  8061 - [=================>------------]  61% - acc: 0.8516 - loss: 1.6088 - 8667.3 sample/sec\n",
            "Global step:  8071 - [==================>-----------]  64% - acc: 0.7969 - loss: 1.6522 - 8991.2 sample/sec\n",
            "Global step:  8081 - [===================>----------]  66% - acc: 0.8906 - loss: 1.5712 - 8575.4 sample/sec\n",
            "Global step:  8091 - [====================>---------]  69% - acc: 0.9141 - loss: 1.5522 - 8654.9 sample/sec\n",
            "Global step:  8101 - [====================>---------]  72% - acc: 0.8594 - loss: 1.6018 - 8643.7 sample/sec\n",
            "Global step:  8111 - [=====================>--------]  74% - acc: 0.8984 - loss: 1.5683 - 8822.1 sample/sec\n",
            "Global step:  8121 - [======================>-------]  77% - acc: 0.9297 - loss: 1.5288 - 8504.5 sample/sec\n",
            "Global step:  8131 - [======================>-------]  79% - acc: 0.8594 - loss: 1.6043 - 8916.3 sample/sec\n",
            "Global step:  8141 - [=======================>------]  82% - acc: 0.8750 - loss: 1.5873 - 8792.2 sample/sec\n",
            "Global step:  8151 - [========================>-----]  84% - acc: 0.9688 - loss: 1.4959 - 8896.1 sample/sec\n",
            "Global step:  8161 - [=========================>----]  87% - acc: 0.9141 - loss: 1.5463 - 8452.5 sample/sec\n",
            "Global step:  8171 - [==========================>---]  90% - acc: 0.8594 - loss: 1.5977 - 8709.5 sample/sec\n",
            "Global step:  8181 - [==========================>---]  92% - acc: 0.8594 - loss: 1.5979 - 8639.7 sample/sec\n",
            "Global step:  8191 - [===========================>--]  95% - acc: 0.8594 - loss: 1.6026 - 8729.8 sample/sec\n",
            "Global step:  8201 - [============================>-]  97% - acc: 0.8828 - loss: 1.5824 - 8770.5 sample/sec\n",
            "Global step:  8211 - [=============================>] 100% - acc: 0.8375 - loss: 1.6196 - 12785.7 sample/sec\n",
            "\n",
            "Epoch 21 - accuracy: 75.09% (7509/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 22/60\n",
            "\n",
            "Global step:  8212 - [>-----------------------------]   0% - acc: 0.9141 - loss: 1.5482 - 8840.7 sample/sec\n",
            "Global step:  8222 - [>-----------------------------]   3% - acc: 0.8906 - loss: 1.5777 - 8799.7 sample/sec\n",
            "Global step:  8232 - [=>----------------------------]   5% - acc: 0.9219 - loss: 1.5421 - 8754.5 sample/sec\n",
            "Global step:  8242 - [==>---------------------------]   8% - acc: 0.9219 - loss: 1.5409 - 8967.1 sample/sec\n",
            "Global step:  8252 - [==>---------------------------]  10% - acc: 0.9062 - loss: 1.5542 - 8688.6 sample/sec\n",
            "Global step:  8262 - [===>--------------------------]  13% - acc: 0.8984 - loss: 1.5640 - 8604.7 sample/sec\n",
            "Global step:  8272 - [====>-------------------------]  15% - acc: 0.9297 - loss: 1.5321 - 8975.8 sample/sec\n",
            "Global step:  8282 - [=====>------------------------]  18% - acc: 0.8750 - loss: 1.5825 - 8818.5 sample/sec\n",
            "Global step:  8292 - [=====>------------------------]  20% - acc: 0.9219 - loss: 1.5376 - 8782.9 sample/sec\n",
            "Global step:  8302 - [======>-----------------------]  23% - acc: 0.9375 - loss: 1.5233 - 8916.0 sample/sec\n",
            "Global step:  8312 - [=======>----------------------]  26% - acc: 0.9219 - loss: 1.5419 - 8604.4 sample/sec\n",
            "Global step:  8322 - [========>---------------------]  28% - acc: 0.9297 - loss: 1.5318 - 8888.0 sample/sec\n",
            "Global step:  8332 - [========>---------------------]  31% - acc: 0.9219 - loss: 1.5378 - 8927.2 sample/sec\n",
            "Global step:  8342 - [=========>--------------------]  33% - acc: 0.9297 - loss: 1.5368 - 8794.0 sample/sec\n",
            "Global step:  8352 - [==========>-------------------]  36% - acc: 0.9141 - loss: 1.5474 - 8629.0 sample/sec\n",
            "Global step:  8362 - [===========>------------------]  38% - acc: 0.9219 - loss: 1.5448 - 8610.9 sample/sec\n",
            "Global step:  8372 - [===========>------------------]  41% - acc: 0.8984 - loss: 1.5627 - 8816.5 sample/sec\n",
            "Global step:  8382 - [============>-----------------]  43% - acc: 0.9062 - loss: 1.5598 - 8745.4 sample/sec\n",
            "Global step:  8392 - [=============>----------------]  46% - acc: 0.8984 - loss: 1.5612 - 8560.1 sample/sec\n",
            "Global step:  8402 - [==============>---------------]  49% - acc: 0.8672 - loss: 1.5901 - 8646.1 sample/sec\n",
            "Global step:  8412 - [==============>---------------]  51% - acc: 0.8594 - loss: 1.6020 - 8904.4 sample/sec\n",
            "Global step:  8422 - [===============>--------------]  54% - acc: 0.9141 - loss: 1.5451 - 8840.9 sample/sec\n",
            "Global step:  8432 - [================>-------------]  56% - acc: 0.9297 - loss: 1.5335 - 8787.6 sample/sec\n",
            "Global step:  8442 - [=================>------------]  59% - acc: 0.9219 - loss: 1.5357 - 8799.3 sample/sec\n",
            "Global step:  8452 - [=================>------------]  61% - acc: 0.8438 - loss: 1.6193 - 8909.2 sample/sec\n",
            "Global step:  8462 - [==================>-----------]  64% - acc: 0.8750 - loss: 1.5882 - 8552.2 sample/sec\n",
            "Global step:  8472 - [===================>----------]  66% - acc: 0.8828 - loss: 1.5742 - 8898.0 sample/sec\n",
            "Global step:  8482 - [====================>---------]  69% - acc: 0.9219 - loss: 1.5386 - 8696.9 sample/sec\n",
            "Global step:  8492 - [====================>---------]  72% - acc: 0.9219 - loss: 1.5387 - 8679.1 sample/sec\n",
            "Global step:  8502 - [=====================>--------]  74% - acc: 0.9219 - loss: 1.5418 - 8963.2 sample/sec\n",
            "Global step:  8512 - [======================>-------]  77% - acc: 0.9531 - loss: 1.5104 - 7133.4 sample/sec\n",
            "Global step:  8522 - [======================>-------]  79% - acc: 0.8594 - loss: 1.5967 - 8843.2 sample/sec\n",
            "Global step:  8532 - [=======================>------]  82% - acc: 0.8906 - loss: 1.5723 - 8842.5 sample/sec\n",
            "Global step:  8542 - [========================>-----]  84% - acc: 0.9688 - loss: 1.4939 - 8952.9 sample/sec\n",
            "Global step:  8552 - [=========================>----]  87% - acc: 0.9531 - loss: 1.5084 - 8608.9 sample/sec\n",
            "Global step:  8562 - [==========================>---]  90% - acc: 0.9062 - loss: 1.5567 - 8869.5 sample/sec\n",
            "Global step:  8572 - [==========================>---]  92% - acc: 0.8828 - loss: 1.5770 - 9043.0 sample/sec\n",
            "Global step:  8582 - [===========================>--]  95% - acc: 0.8906 - loss: 1.5758 - 8918.0 sample/sec\n",
            "Global step:  8592 - [============================>-]  97% - acc: 0.9453 - loss: 1.5211 - 8672.6 sample/sec\n",
            "Global step:  8602 - [=============================>] 100% - acc: 0.8875 - loss: 1.5726 - 12810.1 sample/sec\n",
            "\n",
            "Epoch 22 - accuracy: 78.14% (7814/10000)\n",
            "This epoch receive better accuracy: 78.14 > 77.23. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 23/60\n",
            "\n",
            "Global step:  8603 - [>-----------------------------]   0% - acc: 0.9375 - loss: 1.5253 - 8946.2 sample/sec\n",
            "Global step:  8613 - [>-----------------------------]   3% - acc: 0.9141 - loss: 1.5484 - 8729.2 sample/sec\n",
            "Global step:  8623 - [=>----------------------------]   5% - acc: 0.9297 - loss: 1.5274 - 8832.3 sample/sec\n",
            "Global step:  8633 - [==>---------------------------]   8% - acc: 0.9219 - loss: 1.5437 - 8883.7 sample/sec\n",
            "Global step:  8643 - [==>---------------------------]  10% - acc: 0.9141 - loss: 1.5455 - 8550.3 sample/sec\n",
            "Global step:  8653 - [===>--------------------------]  13% - acc: 0.9062 - loss: 1.5511 - 8920.5 sample/sec\n",
            "Global step:  8663 - [====>-------------------------]  15% - acc: 0.9453 - loss: 1.5208 - 8561.2 sample/sec\n",
            "Global step:  8673 - [=====>------------------------]  18% - acc: 0.8984 - loss: 1.5644 - 8810.3 sample/sec\n",
            "Global step:  8683 - [=====>------------------------]  20% - acc: 0.9375 - loss: 1.5243 - 8706.0 sample/sec\n",
            "Global step:  8693 - [======>-----------------------]  23% - acc: 0.9453 - loss: 1.5160 - 8878.6 sample/sec\n",
            "Global step:  8703 - [=======>----------------------]  26% - acc: 0.9375 - loss: 1.5246 - 8867.6 sample/sec\n",
            "Global step:  8713 - [========>---------------------]  28% - acc: 0.9375 - loss: 1.5222 - 8589.0 sample/sec\n",
            "Global step:  8723 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5267 - 8685.5 sample/sec\n",
            "Global step:  8733 - [=========>--------------------]  33% - acc: 0.9375 - loss: 1.5264 - 8486.9 sample/sec\n",
            "Global step:  8743 - [==========>-------------------]  36% - acc: 0.9141 - loss: 1.5455 - 8902.7 sample/sec\n",
            "Global step:  8753 - [===========>------------------]  38% - acc: 0.9219 - loss: 1.5359 - 8816.9 sample/sec\n",
            "Global step:  8763 - [===========>------------------]  41% - acc: 0.9062 - loss: 1.5507 - 8776.6 sample/sec\n",
            "Global step:  8773 - [============>-----------------]  43% - acc: 0.9062 - loss: 1.5528 - 8652.8 sample/sec\n",
            "Global step:  8783 - [=============>----------------]  46% - acc: 0.9141 - loss: 1.5481 - 8875.7 sample/sec\n",
            "Global step:  8793 - [==============>---------------]  49% - acc: 0.8828 - loss: 1.5783 - 8831.6 sample/sec\n",
            "Global step:  8803 - [==============>---------------]  51% - acc: 0.8750 - loss: 1.5861 - 8775.1 sample/sec\n",
            "Global step:  8813 - [===============>--------------]  54% - acc: 0.9219 - loss: 1.5396 - 8703.9 sample/sec\n",
            "Global step:  8823 - [================>-------------]  56% - acc: 0.9375 - loss: 1.5274 - 8722.5 sample/sec\n",
            "Global step:  8833 - [=================>------------]  59% - acc: 0.9297 - loss: 1.5323 - 8809.7 sample/sec\n",
            "Global step:  8843 - [=================>------------]  61% - acc: 0.8516 - loss: 1.6100 - 8540.9 sample/sec\n",
            "Global step:  8853 - [==================>-----------]  64% - acc: 0.8750 - loss: 1.5851 - 8736.3 sample/sec\n",
            "Global step:  8863 - [===================>----------]  66% - acc: 0.8906 - loss: 1.5671 - 8674.6 sample/sec\n",
            "Global step:  8873 - [====================>---------]  69% - acc: 0.9297 - loss: 1.5313 - 8786.9 sample/sec\n",
            "Global step:  8883 - [====================>---------]  72% - acc: 0.9219 - loss: 1.5371 - 8725.1 sample/sec\n",
            "Global step:  8893 - [=====================>--------]  74% - acc: 0.9297 - loss: 1.5321 - 8796.7 sample/sec\n",
            "Global step:  8903 - [======================>-------]  77% - acc: 0.9531 - loss: 1.5083 - 8922.3 sample/sec\n",
            "Global step:  8913 - [======================>-------]  79% - acc: 0.8672 - loss: 1.5933 - 8711.9 sample/sec\n",
            "Global step:  8923 - [=======================>------]  82% - acc: 0.8906 - loss: 1.5675 - 8931.8 sample/sec\n",
            "Global step:  8933 - [========================>-----]  84% - acc: 0.9688 - loss: 1.4901 - 8634.7 sample/sec\n",
            "Global step:  8943 - [=========================>----]  87% - acc: 0.9531 - loss: 1.5067 - 8589.8 sample/sec\n",
            "Global step:  8953 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5477 - 8590.1 sample/sec\n",
            "Global step:  8963 - [==========================>---]  92% - acc: 0.8906 - loss: 1.5673 - 8754.1 sample/sec\n",
            "Global step:  8973 - [===========================>--]  95% - acc: 0.8906 - loss: 1.5706 - 6680.4 sample/sec\n",
            "Global step:  8983 - [============================>-]  97% - acc: 0.9453 - loss: 1.5176 - 8835.9 sample/sec\n",
            "Global step:  8993 - [=============================>] 100% - acc: 0.8875 - loss: 1.5666 - 12616.2 sample/sec\n",
            "\n",
            "Epoch 23 - accuracy: 78.29% (7829/10000)\n",
            "This epoch receive better accuracy: 78.29 > 78.14. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 24/60\n",
            "\n",
            "Global step:  8994 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5159 - 9259.3 sample/sec\n",
            "Global step:  9004 - [>-----------------------------]   3% - acc: 0.9141 - loss: 1.5463 - 8641.0 sample/sec\n",
            "Global step:  9014 - [=>----------------------------]   5% - acc: 0.9453 - loss: 1.5200 - 8961.6 sample/sec\n",
            "Global step:  9024 - [==>---------------------------]   8% - acc: 0.9375 - loss: 1.5256 - 8847.4 sample/sec\n",
            "Global step:  9034 - [==>---------------------------]  10% - acc: 0.9219 - loss: 1.5393 - 8769.0 sample/sec\n",
            "Global step:  9044 - [===>--------------------------]  13% - acc: 0.9141 - loss: 1.5471 - 8787.6 sample/sec\n",
            "Global step:  9054 - [====>-------------------------]  15% - acc: 0.9453 - loss: 1.5188 - 8190.1 sample/sec\n",
            "Global step:  9064 - [=====>------------------------]  18% - acc: 0.8984 - loss: 1.5635 - 8896.0 sample/sec\n",
            "Global step:  9074 - [=====>------------------------]  20% - acc: 0.9453 - loss: 1.5163 - 8803.2 sample/sec\n",
            "Global step:  9084 - [======>-----------------------]  23% - acc: 0.9453 - loss: 1.5181 - 8707.9 sample/sec\n",
            "Global step:  9094 - [=======>----------------------]  26% - acc: 0.9375 - loss: 1.5227 - 8654.2 sample/sec\n",
            "Global step:  9104 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5158 - 8936.2 sample/sec\n",
            "Global step:  9114 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5244 - 9042.6 sample/sec\n",
            "Global step:  9124 - [=========>--------------------]  33% - acc: 0.9375 - loss: 1.5245 - 8472.5 sample/sec\n",
            "Global step:  9134 - [==========>-------------------]  36% - acc: 0.9219 - loss: 1.5401 - 8712.6 sample/sec\n",
            "Global step:  9144 - [===========>------------------]  38% - acc: 0.9297 - loss: 1.5300 - 8713.9 sample/sec\n",
            "Global step:  9154 - [===========>------------------]  41% - acc: 0.9141 - loss: 1.5433 - 8597.5 sample/sec\n",
            "Global step:  9164 - [============>-----------------]  43% - acc: 0.9141 - loss: 1.5461 - 9020.2 sample/sec\n",
            "Global step:  9174 - [=============>----------------]  46% - acc: 0.9141 - loss: 1.5500 - 8666.5 sample/sec\n",
            "Global step:  9184 - [==============>---------------]  49% - acc: 0.8828 - loss: 1.5771 - 8510.1 sample/sec\n",
            "Global step:  9194 - [==============>---------------]  51% - acc: 0.8828 - loss: 1.5788 - 8767.7 sample/sec\n",
            "Global step:  9204 - [===============>--------------]  54% - acc: 0.9219 - loss: 1.5388 - 8661.2 sample/sec\n",
            "Global step:  9214 - [================>-------------]  56% - acc: 0.9375 - loss: 1.5249 - 8518.5 sample/sec\n",
            "Global step:  9224 - [=================>------------]  59% - acc: 0.9375 - loss: 1.5239 - 8776.7 sample/sec\n",
            "Global step:  9234 - [=================>------------]  61% - acc: 0.8516 - loss: 1.6096 - 8709.8 sample/sec\n",
            "Global step:  9244 - [==================>-----------]  64% - acc: 0.8750 - loss: 1.5855 - 8761.1 sample/sec\n",
            "Global step:  9254 - [===================>----------]  66% - acc: 0.8984 - loss: 1.5628 - 8859.5 sample/sec\n",
            "Global step:  9264 - [====================>---------]  69% - acc: 0.9297 - loss: 1.5321 - 8707.7 sample/sec\n",
            "Global step:  9274 - [====================>---------]  72% - acc: 0.9375 - loss: 1.5230 - 8715.0 sample/sec\n",
            "Global step:  9284 - [=====================>--------]  74% - acc: 0.9297 - loss: 1.5309 - 8700.5 sample/sec\n",
            "Global step:  9294 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5035 - 8727.9 sample/sec\n",
            "Global step:  9304 - [======================>-------]  79% - acc: 0.8672 - loss: 1.5915 - 8526.1 sample/sec\n",
            "Global step:  9314 - [=======================>------]  82% - acc: 0.8984 - loss: 1.5633 - 8707.4 sample/sec\n",
            "Global step:  9324 - [========================>-----]  84% - acc: 0.9688 - loss: 1.4912 - 8755.1 sample/sec\n",
            "Global step:  9334 - [=========================>----]  87% - acc: 0.9609 - loss: 1.4974 - 8790.4 sample/sec\n",
            "Global step:  9344 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5469 - 8368.1 sample/sec\n",
            "Global step:  9354 - [==========================>---]  92% - acc: 0.8984 - loss: 1.5622 - 8372.8 sample/sec\n",
            "Global step:  9364 - [===========================>--]  95% - acc: 0.8984 - loss: 1.5624 - 8681.6 sample/sec\n",
            "Global step:  9374 - [============================>-]  97% - acc: 0.9453 - loss: 1.5157 - 8533.7 sample/sec\n",
            "Global step:  9384 - [=============================>] 100% - acc: 0.9000 - loss: 1.5609 - 12264.6 sample/sec\n",
            "\n",
            "Epoch 24 - accuracy: 78.57% (7857/10000)\n",
            "This epoch receive better accuracy: 78.57 > 78.29. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 25/60\n",
            "\n",
            "Global step:  9385 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5157 - 9248.3 sample/sec\n",
            "Global step:  9395 - [>-----------------------------]   3% - acc: 0.9141 - loss: 1.5454 - 9069.5 sample/sec\n",
            "Global step:  9405 - [=>----------------------------]   5% - acc: 0.9453 - loss: 1.5106 - 8737.7 sample/sec\n",
            "Global step:  9415 - [==>---------------------------]   8% - acc: 0.9453 - loss: 1.5177 - 8735.4 sample/sec\n",
            "Global step:  9425 - [==>---------------------------]  10% - acc: 0.9219 - loss: 1.5358 - 8737.3 sample/sec\n",
            "Global step:  9435 - [===>--------------------------]  13% - acc: 0.9141 - loss: 1.5456 - 8903.3 sample/sec\n",
            "Global step:  9445 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5108 - 8571.1 sample/sec\n",
            "Global step:  9455 - [=====>------------------------]  18% - acc: 0.8984 - loss: 1.5629 - 8495.2 sample/sec\n",
            "Global step:  9465 - [=====>------------------------]  20% - acc: 0.9453 - loss: 1.5155 - 8334.0 sample/sec\n",
            "Global step:  9475 - [======>-----------------------]  23% - acc: 0.9453 - loss: 1.5157 - 8580.1 sample/sec\n",
            "Global step:  9485 - [=======>----------------------]  26% - acc: 0.9531 - loss: 1.5109 - 8753.9 sample/sec\n",
            "Global step:  9495 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5164 - 8553.3 sample/sec\n",
            "Global step:  9505 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5240 - 8680.6 sample/sec\n",
            "Global step:  9515 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5089 - 8520.7 sample/sec\n",
            "Global step:  9525 - [==========>-------------------]  36% - acc: 0.9219 - loss: 1.5393 - 8492.0 sample/sec\n",
            "Global step:  9535 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5268 - 8944.9 sample/sec\n",
            "Global step:  9545 - [===========>------------------]  41% - acc: 0.9297 - loss: 1.5345 - 8714.2 sample/sec\n",
            "Global step:  9555 - [============>-----------------]  43% - acc: 0.9219 - loss: 1.5385 - 8715.3 sample/sec\n",
            "Global step:  9565 - [=============>----------------]  46% - acc: 0.9141 - loss: 1.5495 - 8802.2 sample/sec\n",
            "Global step:  9575 - [==============>---------------]  49% - acc: 0.8828 - loss: 1.5754 - 8671.8 sample/sec\n",
            "Global step:  9585 - [==============>---------------]  51% - acc: 0.8828 - loss: 1.5777 - 8791.7 sample/sec\n",
            "Global step:  9595 - [===============>--------------]  54% - acc: 0.9297 - loss: 1.5313 - 8682.2 sample/sec\n",
            "Global step:  9605 - [================>-------------]  56% - acc: 0.9375 - loss: 1.5276 - 8674.9 sample/sec\n",
            "Global step:  9615 - [=================>------------]  59% - acc: 0.9375 - loss: 1.5238 - 8669.4 sample/sec\n",
            "Global step:  9625 - [=================>------------]  61% - acc: 0.8516 - loss: 1.6092 - 8797.1 sample/sec\n",
            "Global step:  9635 - [==================>-----------]  64% - acc: 0.8750 - loss: 1.5855 - 8742.3 sample/sec\n",
            "Global step:  9645 - [===================>----------]  66% - acc: 0.8984 - loss: 1.5624 - 8574.8 sample/sec\n",
            "Global step:  9655 - [====================>---------]  69% - acc: 0.9375 - loss: 1.5240 - 8760.9 sample/sec\n",
            "Global step:  9665 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5178 - 8381.1 sample/sec\n",
            "Global step:  9675 - [=====================>--------]  74% - acc: 0.9297 - loss: 1.5279 - 8743.5 sample/sec\n",
            "Global step:  9685 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5013 - 8582.9 sample/sec\n",
            "Global step:  9695 - [======================>-------]  79% - acc: 0.8750 - loss: 1.5852 - 8669.1 sample/sec\n",
            "Global step:  9705 - [=======================>------]  82% - acc: 0.8984 - loss: 1.5623 - 8648.6 sample/sec\n",
            "Global step:  9715 - [========================>-----]  84% - acc: 0.9766 - loss: 1.4873 - 8709.2 sample/sec\n",
            "Global step:  9725 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4924 - 8699.2 sample/sec\n",
            "Global step:  9735 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5474 - 8541.1 sample/sec\n",
            "Global step:  9745 - [==========================>---]  92% - acc: 0.8984 - loss: 1.5619 - 8769.1 sample/sec\n",
            "Global step:  9755 - [===========================>--]  95% - acc: 0.8984 - loss: 1.5612 - 8450.8 sample/sec\n",
            "Global step:  9765 - [============================>-]  97% - acc: 0.9453 - loss: 1.5170 - 8715.7 sample/sec\n",
            "Global step:  9775 - [=============================>] 100% - acc: 0.9000 - loss: 1.5609 - 12431.6 sample/sec\n",
            "\n",
            "Epoch 25 - accuracy: 78.52% (7852/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 26/60\n",
            "\n",
            "Global step:  9776 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5156 - 8812.7 sample/sec\n",
            "Global step:  9786 - [>-----------------------------]   3% - acc: 0.9219 - loss: 1.5382 - 8823.4 sample/sec\n",
            "Global step:  9796 - [=>----------------------------]   5% - acc: 0.9609 - loss: 1.5009 - 8577.6 sample/sec\n",
            "Global step:  9806 - [==>---------------------------]   8% - acc: 0.9453 - loss: 1.5172 - 8851.2 sample/sec\n",
            "Global step:  9816 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5320 - 8895.8 sample/sec\n",
            "Global step:  9826 - [===>--------------------------]  13% - acc: 0.9219 - loss: 1.5401 - 8540.5 sample/sec\n",
            "Global step:  9836 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5078 - 8541.8 sample/sec\n",
            "Global step:  9846 - [=====>------------------------]  18% - acc: 0.8984 - loss: 1.5618 - 8785.3 sample/sec\n",
            "Global step:  9856 - [=====>------------------------]  20% - acc: 0.9453 - loss: 1.5160 - 8642.6 sample/sec\n",
            "Global step:  9866 - [======>-----------------------]  23% - acc: 0.9453 - loss: 1.5159 - 8806.8 sample/sec\n",
            "Global step:  9876 - [=======>----------------------]  26% - acc: 0.9609 - loss: 1.5001 - 8906.6 sample/sec\n",
            "Global step:  9886 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5159 - 8617.0 sample/sec\n",
            "Global step:  9896 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5238 - 8434.7 sample/sec\n",
            "Global step:  9906 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5093 - 8806.6 sample/sec\n",
            "Global step:  9916 - [==========>-------------------]  36% - acc: 0.9219 - loss: 1.5399 - 8703.6 sample/sec\n",
            "Global step:  9926 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5238 - 8688.8 sample/sec\n",
            "Global step:  9936 - [===========>------------------]  41% - acc: 0.9297 - loss: 1.5327 - 8601.9 sample/sec\n",
            "Global step:  9946 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5336 - 8688.2 sample/sec\n",
            "Global step:  9956 - [=============>----------------]  46% - acc: 0.9141 - loss: 1.5472 - 8896.8 sample/sec\n",
            "Global step:  9966 - [==============>---------------]  49% - acc: 0.8984 - loss: 1.5655 - 8719.8 sample/sec\n",
            "Global step:  9976 - [==============>---------------]  51% - acc: 0.8828 - loss: 1.5782 - 8853.6 sample/sec\n",
            "Global step:  9986 - [===============>--------------]  54% - acc: 0.9297 - loss: 1.5314 - 8846.3 sample/sec\n",
            "Global step:  9996 - [================>-------------]  56% - acc: 0.9375 - loss: 1.5244 - 8670.7 sample/sec\n",
            "Global step: 10006 - [=================>------------]  59% - acc: 0.9375 - loss: 1.5238 - 8445.6 sample/sec\n",
            "Global step: 10016 - [=================>------------]  61% - acc: 0.8594 - loss: 1.6041 - 8971.3 sample/sec\n",
            "Global step: 10026 - [==================>-----------]  64% - acc: 0.8750 - loss: 1.5850 - 8766.5 sample/sec\n",
            "Global step: 10036 - [===================>----------]  66% - acc: 0.8984 - loss: 1.5622 - 8660.7 sample/sec\n",
            "Global step: 10046 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5214 - 8512.2 sample/sec\n",
            "Global step: 10056 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5162 - 7352.9 sample/sec\n",
            "Global step: 10066 - [=====================>--------]  74% - acc: 0.9453 - loss: 1.5177 - 8943.7 sample/sec\n",
            "Global step: 10076 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5003 - 8685.5 sample/sec\n",
            "Global step: 10086 - [======================>-------]  79% - acc: 0.8750 - loss: 1.5844 - 8868.0 sample/sec\n",
            "Global step: 10096 - [=======================>------]  82% - acc: 0.8984 - loss: 1.5627 - 8689.5 sample/sec\n",
            "Global step: 10106 - [========================>-----]  84% - acc: 0.9766 - loss: 1.4874 - 8807.7 sample/sec\n",
            "Global step: 10116 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4929 - 8772.7 sample/sec\n",
            "Global step: 10126 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5466 - 7383.1 sample/sec\n",
            "Global step: 10136 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5559 - 8810.0 sample/sec\n",
            "Global step: 10146 - [===========================>--]  95% - acc: 0.9062 - loss: 1.5547 - 8866.0 sample/sec\n",
            "Global step: 10156 - [============================>-]  97% - acc: 0.9453 - loss: 1.5169 - 8707.5 sample/sec\n",
            "Global step: 10166 - [=============================>] 100% - acc: 0.9000 - loss: 1.5606 - 12934.8 sample/sec\n",
            "\n",
            "Epoch 26 - accuracy: 78.41% (7841/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 27/60\n",
            "\n",
            "Global step: 10167 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5158 - 8814.7 sample/sec\n",
            "Global step: 10177 - [>-----------------------------]   3% - acc: 0.9297 - loss: 1.5320 - 8568.7 sample/sec\n",
            "Global step: 10187 - [=>----------------------------]   5% - acc: 0.9609 - loss: 1.5012 - 8899.8 sample/sec\n",
            "Global step: 10197 - [==>---------------------------]   8% - acc: 0.9453 - loss: 1.5157 - 8548.2 sample/sec\n",
            "Global step: 10207 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5313 - 8704.6 sample/sec\n",
            "Global step: 10217 - [===>--------------------------]  13% - acc: 0.9219 - loss: 1.5392 - 8803.2 sample/sec\n",
            "Global step: 10227 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5084 - 8730.6 sample/sec\n",
            "Global step: 10237 - [=====>------------------------]  18% - acc: 0.8984 - loss: 1.5603 - 7144.8 sample/sec\n",
            "Global step: 10247 - [=====>------------------------]  20% - acc: 0.9453 - loss: 1.5157 - 8687.1 sample/sec\n",
            "Global step: 10257 - [======>-----------------------]  23% - acc: 0.9453 - loss: 1.5159 - 8725.5 sample/sec\n",
            "Global step: 10267 - [=======>----------------------]  26% - acc: 0.9609 - loss: 1.4988 - 8937.3 sample/sec\n",
            "Global step: 10277 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5158 - 8650.4 sample/sec\n",
            "Global step: 10287 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5240 - 8785.5 sample/sec\n",
            "Global step: 10297 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5095 - 8619.3 sample/sec\n",
            "Global step: 10307 - [==========>-------------------]  36% - acc: 0.9219 - loss: 1.5387 - 7794.9 sample/sec\n",
            "Global step: 10317 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5239 - 6836.7 sample/sec\n",
            "Global step: 10327 - [===========>------------------]  41% - acc: 0.9297 - loss: 1.5316 - 8025.5 sample/sec\n",
            "Global step: 10337 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5312 - 8545.0 sample/sec\n",
            "Global step: 10347 - [=============>----------------]  46% - acc: 0.9141 - loss: 1.5482 - 8846.8 sample/sec\n",
            "Global step: 10357 - [==============>---------------]  49% - acc: 0.9062 - loss: 1.5551 - 8808.2 sample/sec\n",
            "Global step: 10367 - [==============>---------------]  51% - acc: 0.8828 - loss: 1.5784 - 8588.3 sample/sec\n",
            "Global step: 10377 - [===============>--------------]  54% - acc: 0.9297 - loss: 1.5311 - 9038.8 sample/sec\n",
            "Global step: 10387 - [================>-------------]  56% - acc: 0.9375 - loss: 1.5234 - 8816.8 sample/sec\n",
            "Global step: 10397 - [=================>------------]  59% - acc: 0.9375 - loss: 1.5237 - 8650.3 sample/sec\n",
            "Global step: 10407 - [=================>------------]  61% - acc: 0.8594 - loss: 1.6013 - 8640.8 sample/sec\n",
            "Global step: 10417 - [==================>-----------]  64% - acc: 0.8750 - loss: 1.5852 - 8325.6 sample/sec\n",
            "Global step: 10427 - [===================>----------]  66% - acc: 0.8984 - loss: 1.5618 - 8591.6 sample/sec\n",
            "Global step: 10437 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5165 - 8702.9 sample/sec\n",
            "Global step: 10447 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5162 - 9073.7 sample/sec\n",
            "Global step: 10457 - [=====================>--------]  74% - acc: 0.9453 - loss: 1.5163 - 8861.4 sample/sec\n",
            "Global step: 10467 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5003 - 8704.3 sample/sec\n",
            "Global step: 10477 - [======================>-------]  79% - acc: 0.8750 - loss: 1.5844 - 8647.2 sample/sec\n",
            "Global step: 10487 - [=======================>------]  82% - acc: 0.8984 - loss: 1.5629 - 8675.0 sample/sec\n",
            "Global step: 10497 - [========================>-----]  84% - acc: 0.9766 - loss: 1.4851 - 8838.7 sample/sec\n",
            "Global step: 10507 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4925 - 8492.2 sample/sec\n",
            "Global step: 10517 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5465 - 8844.4 sample/sec\n",
            "Global step: 10527 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5545 - 8326.2 sample/sec\n",
            "Global step: 10537 - [===========================>--]  95% - acc: 0.9062 - loss: 1.5547 - 8700.2 sample/sec\n",
            "Global step: 10547 - [============================>-]  97% - acc: 0.9453 - loss: 1.5160 - 8812.6 sample/sec\n",
            "Global step: 10557 - [=============================>] 100% - acc: 0.9000 - loss: 1.5603 - 12455.0 sample/sec\n",
            "\n",
            "Epoch 27 - accuracy: 78.64% (7864/10000)\n",
            "This epoch receive better accuracy: 78.64 > 78.57. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 28/60\n",
            "\n",
            "Global step: 10558 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5161 - 9213.2 sample/sec\n",
            "Global step: 10568 - [>-----------------------------]   3% - acc: 0.9297 - loss: 1.5313 - 8665.1 sample/sec\n",
            "Global step: 10578 - [=>----------------------------]   5% - acc: 0.9609 - loss: 1.5002 - 8686.0 sample/sec\n",
            "Global step: 10588 - [==>---------------------------]   8% - acc: 0.9453 - loss: 1.5152 - 8607.0 sample/sec\n",
            "Global step: 10598 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5311 - 8758.4 sample/sec\n",
            "Global step: 10608 - [===>--------------------------]  13% - acc: 0.9219 - loss: 1.5365 - 8851.7 sample/sec\n",
            "Global step: 10618 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5080 - 7339.1 sample/sec\n",
            "Global step: 10628 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5548 - 8736.4 sample/sec\n",
            "Global step: 10638 - [=====>------------------------]  20% - acc: 0.9453 - loss: 1.5159 - 8697.5 sample/sec\n",
            "Global step: 10648 - [======>-----------------------]  23% - acc: 0.9453 - loss: 1.5164 - 8643.3 sample/sec\n",
            "Global step: 10658 - [=======>----------------------]  26% - acc: 0.9688 - loss: 1.4930 - 8690.9 sample/sec\n",
            "Global step: 10668 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5158 - 8532.3 sample/sec\n",
            "Global step: 10678 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5237 - 8459.3 sample/sec\n",
            "Global step: 10688 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5097 - 8116.6 sample/sec\n",
            "Global step: 10698 - [==========>-------------------]  36% - acc: 0.9219 - loss: 1.5389 - 8658.1 sample/sec\n",
            "Global step: 10708 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5239 - 8661.7 sample/sec\n",
            "Global step: 10718 - [===========>------------------]  41% - acc: 0.9297 - loss: 1.5313 - 8720.1 sample/sec\n",
            "Global step: 10728 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5312 - 8671.7 sample/sec\n",
            "Global step: 10738 - [=============>----------------]  46% - acc: 0.9141 - loss: 1.5471 - 8962.2 sample/sec\n",
            "Global step: 10748 - [==============>---------------]  49% - acc: 0.9062 - loss: 1.5542 - 8743.5 sample/sec\n",
            "Global step: 10758 - [==============>---------------]  51% - acc: 0.8828 - loss: 1.5800 - 8513.7 sample/sec\n",
            "Global step: 10768 - [===============>--------------]  54% - acc: 0.9375 - loss: 1.5239 - 8691.9 sample/sec\n",
            "Global step: 10778 - [================>-------------]  56% - acc: 0.9375 - loss: 1.5209 - 8520.7 sample/sec\n",
            "Global step: 10788 - [=================>------------]  59% - acc: 0.9375 - loss: 1.5231 - 8575.3 sample/sec\n",
            "Global step: 10798 - [=================>------------]  61% - acc: 0.8672 - loss: 1.5940 - 8468.7 sample/sec\n",
            "Global step: 10808 - [==================>-----------]  64% - acc: 0.8750 - loss: 1.5849 - 8530.0 sample/sec\n",
            "Global step: 10818 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5569 - 8490.1 sample/sec\n",
            "Global step: 10828 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5163 - 8691.7 sample/sec\n",
            "Global step: 10838 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5166 - 8536.8 sample/sec\n",
            "Global step: 10848 - [=====================>--------]  74% - acc: 0.9453 - loss: 1.5164 - 8358.4 sample/sec\n",
            "Global step: 10858 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5001 - 8496.8 sample/sec\n",
            "Global step: 10868 - [======================>-------]  79% - acc: 0.8750 - loss: 1.5836 - 8594.7 sample/sec\n",
            "Global step: 10878 - [=======================>------]  82% - acc: 0.8984 - loss: 1.5620 - 7962.1 sample/sec\n",
            "Global step: 10888 - [========================>-----]  84% - acc: 0.9766 - loss: 1.4851 - 8537.6 sample/sec\n",
            "Global step: 10898 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4929 - 8166.6 sample/sec\n",
            "Global step: 10908 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5466 - 8399.4 sample/sec\n",
            "Global step: 10918 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5546 - 8489.6 sample/sec\n",
            "Global step: 10928 - [===========================>--]  95% - acc: 0.9062 - loss: 1.5551 - 8329.5 sample/sec\n",
            "Global step: 10938 - [============================>-]  97% - acc: 0.9453 - loss: 1.5164 - 8569.5 sample/sec\n",
            "Global step: 10948 - [=============================>] 100% - acc: 0.9000 - loss: 1.5601 - 12412.1 sample/sec\n",
            "\n",
            "Epoch 28 - accuracy: 78.70% (7870/10000)\n",
            "This epoch receive better accuracy: 78.70 > 78.64. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 29/60\n",
            "\n",
            "Global step: 10949 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5192 - 8989.8 sample/sec\n",
            "Global step: 10959 - [>-----------------------------]   3% - acc: 0.9297 - loss: 1.5318 - 8749.8 sample/sec\n",
            "Global step: 10969 - [=>----------------------------]   5% - acc: 0.9609 - loss: 1.5005 - 8681.2 sample/sec\n",
            "Global step: 10979 - [==>---------------------------]   8% - acc: 0.9531 - loss: 1.5094 - 8685.8 sample/sec\n",
            "Global step: 10989 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5312 - 8605.6 sample/sec\n",
            "Global step: 10999 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5315 - 8460.5 sample/sec\n",
            "Global step: 11009 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5083 - 8358.0 sample/sec\n",
            "Global step: 11019 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5548 - 8676.0 sample/sec\n",
            "Global step: 11029 - [=====>------------------------]  20% - acc: 0.9453 - loss: 1.5158 - 8445.1 sample/sec\n",
            "Global step: 11039 - [======>-----------------------]  23% - acc: 0.9453 - loss: 1.5163 - 8450.7 sample/sec\n",
            "Global step: 11049 - [=======>----------------------]  26% - acc: 0.9766 - loss: 1.4875 - 8621.7 sample/sec\n",
            "Global step: 11059 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5162 - 8753.9 sample/sec\n",
            "Global step: 11069 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5233 - 8855.6 sample/sec\n",
            "Global step: 11079 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5087 - 8717.1 sample/sec\n",
            "Global step: 11089 - [==========>-------------------]  36% - acc: 0.9219 - loss: 1.5380 - 8665.4 sample/sec\n",
            "Global step: 11099 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5238 - 8454.3 sample/sec\n",
            "Global step: 11109 - [===========>------------------]  41% - acc: 0.9297 - loss: 1.5312 - 8903.8 sample/sec\n",
            "Global step: 11119 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5312 - 8745.3 sample/sec\n",
            "Global step: 11129 - [=============>----------------]  46% - acc: 0.9141 - loss: 1.5469 - 8240.8 sample/sec\n",
            "Global step: 11139 - [==============>---------------]  49% - acc: 0.9062 - loss: 1.5541 - 8884.0 sample/sec\n",
            "Global step: 11149 - [==============>---------------]  51% - acc: 0.8828 - loss: 1.5782 - 8712.3 sample/sec\n",
            "Global step: 11159 - [===============>--------------]  54% - acc: 0.9375 - loss: 1.5268 - 8697.6 sample/sec\n",
            "Global step: 11169 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5103 - 8586.5 sample/sec\n",
            "Global step: 11179 - [=================>------------]  59% - acc: 0.9453 - loss: 1.5161 - 8712.2 sample/sec\n",
            "Global step: 11189 - [=================>------------]  61% - acc: 0.8672 - loss: 1.5942 - 8713.7 sample/sec\n",
            "Global step: 11199 - [==================>-----------]  64% - acc: 0.8750 - loss: 1.5839 - 8846.1 sample/sec\n",
            "Global step: 11209 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5547 - 8768.4 sample/sec\n",
            "Global step: 11219 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5169 - 8646.8 sample/sec\n",
            "Global step: 11229 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5163 - 8687.1 sample/sec\n",
            "Global step: 11239 - [=====================>--------]  74% - acc: 0.9453 - loss: 1.5166 - 8614.1 sample/sec\n",
            "Global step: 11249 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5002 - 8887.0 sample/sec\n",
            "Global step: 11259 - [======================>-------]  79% - acc: 0.8828 - loss: 1.5754 - 8822.1 sample/sec\n",
            "Global step: 11269 - [=======================>------]  82% - acc: 0.9062 - loss: 1.5565 - 8715.6 sample/sec\n",
            "Global step: 11279 - [========================>-----]  84% - acc: 0.9766 - loss: 1.4857 - 8757.5 sample/sec\n",
            "Global step: 11289 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4927 - 8688.6 sample/sec\n",
            "Global step: 11299 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5467 - 8825.0 sample/sec\n",
            "Global step: 11309 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5554 - 8651.3 sample/sec\n",
            "Global step: 11319 - [===========================>--]  95% - acc: 0.9062 - loss: 1.5546 - 8786.0 sample/sec\n",
            "Global step: 11329 - [============================>-]  97% - acc: 0.9453 - loss: 1.5150 - 8657.8 sample/sec\n",
            "Global step: 11339 - [=============================>] 100% - acc: 0.9000 - loss: 1.5605 - 12775.0 sample/sec\n",
            "\n",
            "Epoch 29 - accuracy: 78.56% (7856/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 30/60\n",
            "\n",
            "Global step: 11340 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5156 - 8943.1 sample/sec\n",
            "Global step: 11350 - [>-----------------------------]   3% - acc: 0.9297 - loss: 1.5314 - 8583.5 sample/sec\n",
            "Global step: 11360 - [=>----------------------------]   5% - acc: 0.9609 - loss: 1.5001 - 8597.9 sample/sec\n",
            "Global step: 11370 - [==>---------------------------]   8% - acc: 0.9609 - loss: 1.5031 - 8601.4 sample/sec\n",
            "Global step: 11380 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5316 - 7456.9 sample/sec\n",
            "Global step: 11390 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5320 - 8420.6 sample/sec\n",
            "Global step: 11400 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5079 - 8700.7 sample/sec\n",
            "Global step: 11410 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5546 - 8547.0 sample/sec\n",
            "Global step: 11420 - [=====>------------------------]  20% - acc: 0.9453 - loss: 1.5157 - 8786.3 sample/sec\n",
            "Global step: 11430 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5089 - 8912.1 sample/sec\n",
            "Global step: 11440 - [=======>----------------------]  26% - acc: 0.9766 - loss: 1.4851 - 8769.2 sample/sec\n",
            "Global step: 11450 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5157 - 8747.1 sample/sec\n",
            "Global step: 11460 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5236 - 8749.1 sample/sec\n",
            "Global step: 11470 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5082 - 8605.8 sample/sec\n",
            "Global step: 11480 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5309 - 8821.1 sample/sec\n",
            "Global step: 11490 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5238 - 8353.9 sample/sec\n",
            "Global step: 11500 - [===========>------------------]  41% - acc: 0.9297 - loss: 1.5307 - 8731.0 sample/sec\n",
            "Global step: 11510 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5339 - 8644.0 sample/sec\n",
            "Global step: 11520 - [=============>----------------]  46% - acc: 0.9141 - loss: 1.5469 - 8787.3 sample/sec\n",
            "Global step: 11530 - [==============>---------------]  49% - acc: 0.9062 - loss: 1.5538 - 8659.8 sample/sec\n",
            "Global step: 11540 - [==============>---------------]  51% - acc: 0.8828 - loss: 1.5770 - 8530.8 sample/sec\n",
            "Global step: 11550 - [===============>--------------]  54% - acc: 0.9375 - loss: 1.5242 - 8877.9 sample/sec\n",
            "Global step: 11560 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5092 - 8846.0 sample/sec\n",
            "Global step: 11570 - [=================>------------]  59% - acc: 0.9453 - loss: 1.5157 - 8705.1 sample/sec\n",
            "Global step: 11580 - [=================>------------]  61% - acc: 0.8672 - loss: 1.5937 - 8688.6 sample/sec\n",
            "Global step: 11590 - [==================>-----------]  64% - acc: 0.8828 - loss: 1.5781 - 8796.3 sample/sec\n",
            "Global step: 11600 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5548 - 9029.6 sample/sec\n",
            "Global step: 11610 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5167 - 7625.1 sample/sec\n",
            "Global step: 11620 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8412.1 sample/sec\n",
            "Global step: 11630 - [=====================>--------]  74% - acc: 0.9453 - loss: 1.5164 - 8813.9 sample/sec\n",
            "Global step: 11640 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5001 - 8745.1 sample/sec\n",
            "Global step: 11650 - [======================>-------]  79% - acc: 0.8906 - loss: 1.5698 - 8663.4 sample/sec\n",
            "Global step: 11660 - [=======================>------]  82% - acc: 0.9062 - loss: 1.5548 - 8795.5 sample/sec\n",
            "Global step: 11670 - [========================>-----]  84% - acc: 0.9766 - loss: 1.4849 - 8605.8 sample/sec\n",
            "Global step: 11680 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4924 - 8720.3 sample/sec\n",
            "Global step: 11690 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5466 - 8802.5 sample/sec\n",
            "Global step: 11700 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5544 - 8790.9 sample/sec\n",
            "Global step: 11710 - [===========================>--]  95% - acc: 0.9062 - loss: 1.5545 - 8588.7 sample/sec\n",
            "Global step: 11720 - [============================>-]  97% - acc: 0.9453 - loss: 1.5158 - 8716.7 sample/sec\n",
            "Global step: 11730 - [=============================>] 100% - acc: 0.9000 - loss: 1.5600 - 12831.8 sample/sec\n",
            "\n",
            "Epoch 30 - accuracy: 78.55% (7855/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 31/60\n",
            "\n",
            "Global step: 11731 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5158 - 8863.1 sample/sec\n",
            "Global step: 11741 - [>-----------------------------]   3% - acc: 0.9297 - loss: 1.5313 - 8915.8 sample/sec\n",
            "Global step: 11751 - [=>----------------------------]   5% - acc: 0.9609 - loss: 1.5001 - 8775.6 sample/sec\n",
            "Global step: 11761 - [==>---------------------------]   8% - acc: 0.9609 - loss: 1.5003 - 8724.6 sample/sec\n",
            "Global step: 11771 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5313 - 8729.6 sample/sec\n",
            "Global step: 11781 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5317 - 8630.9 sample/sec\n",
            "Global step: 11791 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5077 - 8696.9 sample/sec\n",
            "Global step: 11801 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5547 - 8769.2 sample/sec\n",
            "Global step: 11811 - [=====>------------------------]  20% - acc: 0.9453 - loss: 1.5155 - 8828.2 sample/sec\n",
            "Global step: 11821 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5081 - 8796.7 sample/sec\n",
            "Global step: 11831 - [=======>----------------------]  26% - acc: 0.9766 - loss: 1.4849 - 8749.1 sample/sec\n",
            "Global step: 11841 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8715.3 sample/sec\n",
            "Global step: 11851 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5233 - 8811.4 sample/sec\n",
            "Global step: 11861 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5081 - 8783.5 sample/sec\n",
            "Global step: 11871 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5308 - 8776.6 sample/sec\n",
            "Global step: 11881 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5238 - 8949.3 sample/sec\n",
            "Global step: 11891 - [===========>------------------]  41% - acc: 0.9297 - loss: 1.5310 - 8876.2 sample/sec\n",
            "Global step: 11901 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5311 - 8603.1 sample/sec\n",
            "Global step: 11911 - [=============>----------------]  46% - acc: 0.9141 - loss: 1.5458 - 8733.3 sample/sec\n",
            "Global step: 11921 - [==============>---------------]  49% - acc: 0.9062 - loss: 1.5540 - 8809.7 sample/sec\n",
            "Global step: 11931 - [==============>---------------]  51% - acc: 0.8906 - loss: 1.5704 - 8745.4 sample/sec\n",
            "Global step: 11941 - [===============>--------------]  54% - acc: 0.9375 - loss: 1.5235 - 8765.8 sample/sec\n",
            "Global step: 11951 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5086 - 8735.4 sample/sec\n",
            "Global step: 11961 - [=================>------------]  59% - acc: 0.9453 - loss: 1.5153 - 8799.7 sample/sec\n",
            "Global step: 11971 - [=================>------------]  61% - acc: 0.8672 - loss: 1.5936 - 6347.3 sample/sec\n",
            "Global step: 11981 - [==================>-----------]  64% - acc: 0.8828 - loss: 1.5775 - 8611.7 sample/sec\n",
            "Global step: 11991 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5547 - 8475.9 sample/sec\n",
            "Global step: 12001 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5165 - 8775.4 sample/sec\n",
            "Global step: 12011 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8696.2 sample/sec\n",
            "Global step: 12021 - [=====================>--------]  74% - acc: 0.9453 - loss: 1.5163 - 8728.6 sample/sec\n",
            "Global step: 12031 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5004 - 8098.7 sample/sec\n",
            "Global step: 12041 - [======================>-------]  79% - acc: 0.8906 - loss: 1.5690 - 8778.3 sample/sec\n",
            "Global step: 12051 - [=======================>------]  82% - acc: 0.9062 - loss: 1.5545 - 8714.3 sample/sec\n",
            "Global step: 12061 - [========================>-----]  84% - acc: 0.9766 - loss: 1.4851 - 8610.7 sample/sec\n",
            "Global step: 12071 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4922 - 8828.7 sample/sec\n",
            "Global step: 12081 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5465 - 8881.5 sample/sec\n",
            "Global step: 12091 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5545 - 8628.3 sample/sec\n",
            "Global step: 12101 - [===========================>--]  95% - acc: 0.9062 - loss: 1.5544 - 8737.3 sample/sec\n",
            "Global step: 12111 - [============================>-]  97% - acc: 0.9453 - loss: 1.5150 - 7789.2 sample/sec\n",
            "Global step: 12121 - [=============================>] 100% - acc: 0.9000 - loss: 1.5602 - 12728.1 sample/sec\n",
            "\n",
            "Epoch 31 - accuracy: 78.73% (7873/10000)\n",
            "This epoch receive better accuracy: 78.73 > 78.70. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 32/60\n",
            "\n",
            "Global step: 12122 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5156 - 9048.4 sample/sec\n",
            "Global step: 12132 - [>-----------------------------]   3% - acc: 0.9297 - loss: 1.5307 - 8617.1 sample/sec\n",
            "Global step: 12142 - [=>----------------------------]   5% - acc: 0.9609 - loss: 1.5000 - 8721.8 sample/sec\n",
            "Global step: 12152 - [==>---------------------------]   8% - acc: 0.9609 - loss: 1.4999 - 8584.6 sample/sec\n",
            "Global step: 12162 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5310 - 8640.1 sample/sec\n",
            "Global step: 12172 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5314 - 8752.0 sample/sec\n",
            "Global step: 12182 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5077 - 8257.3 sample/sec\n",
            "Global step: 12192 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5547 - 8798.1 sample/sec\n",
            "Global step: 12202 - [=====>------------------------]  20% - acc: 0.9453 - loss: 1.5151 - 8754.7 sample/sec\n",
            "Global step: 12212 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5080 - 8844.7 sample/sec\n",
            "Global step: 12222 - [=======>----------------------]  26% - acc: 0.9766 - loss: 1.4845 - 8851.2 sample/sec\n",
            "Global step: 12232 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5157 - 9050.7 sample/sec\n",
            "Global step: 12242 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5235 - 8488.3 sample/sec\n",
            "Global step: 12252 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5081 - 8476.7 sample/sec\n",
            "Global step: 12262 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5315 - 8543.5 sample/sec\n",
            "Global step: 12272 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5238 - 8795.8 sample/sec\n",
            "Global step: 12282 - [===========>------------------]  41% - acc: 0.9297 - loss: 1.5307 - 8920.5 sample/sec\n",
            "Global step: 12292 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5308 - 8708.1 sample/sec\n",
            "Global step: 12302 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5393 - 8851.7 sample/sec\n",
            "Global step: 12312 - [==============>---------------]  49% - acc: 0.9062 - loss: 1.5539 - 8700.9 sample/sec\n",
            "Global step: 12322 - [==============>---------------]  51% - acc: 0.8906 - loss: 1.5700 - 8507.9 sample/sec\n",
            "Global step: 12332 - [===============>--------------]  54% - acc: 0.9375 - loss: 1.5245 - 8711.8 sample/sec\n",
            "Global step: 12342 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5082 - 8797.4 sample/sec\n",
            "Global step: 12352 - [=================>------------]  59% - acc: 0.9453 - loss: 1.5116 - 8679.9 sample/sec\n",
            "Global step: 12362 - [=================>------------]  61% - acc: 0.8672 - loss: 1.5934 - 8717.0 sample/sec\n",
            "Global step: 12372 - [==================>-----------]  64% - acc: 0.8828 - loss: 1.5768 - 8829.4 sample/sec\n",
            "Global step: 12382 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5546 - 8589.2 sample/sec\n",
            "Global step: 12392 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5167 - 8143.5 sample/sec\n",
            "Global step: 12402 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8552.2 sample/sec\n",
            "Global step: 12412 - [=====================>--------]  74% - acc: 0.9453 - loss: 1.5160 - 8276.5 sample/sec\n",
            "Global step: 12422 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5001 - 8548.5 sample/sec\n",
            "Global step: 12432 - [======================>-------]  79% - acc: 0.8906 - loss: 1.5694 - 8660.9 sample/sec\n",
            "Global step: 12442 - [=======================>------]  82% - acc: 0.9062 - loss: 1.5544 - 8639.4 sample/sec\n",
            "Global step: 12452 - [========================>-----]  84% - acc: 0.9766 - loss: 1.4852 - 8725.1 sample/sec\n",
            "Global step: 12462 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4923 - 8623.3 sample/sec\n",
            "Global step: 12472 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5467 - 8749.7 sample/sec\n",
            "Global step: 12482 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5556 - 8889.8 sample/sec\n",
            "Global step: 12492 - [===========================>--]  95% - acc: 0.9062 - loss: 1.5544 - 8579.6 sample/sec\n",
            "Global step: 12502 - [============================>-]  97% - acc: 0.9531 - loss: 1.5081 - 8718.8 sample/sec\n",
            "Global step: 12512 - [=============================>] 100% - acc: 0.9000 - loss: 1.5605 - 12965.1 sample/sec\n",
            "\n",
            "Epoch 32 - accuracy: 78.61% (7861/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 33/60\n",
            "\n",
            "Global step: 12513 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5156 - 8991.5 sample/sec\n",
            "Global step: 12523 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5241 - 8644.8 sample/sec\n",
            "Global step: 12533 - [=>----------------------------]   5% - acc: 0.9609 - loss: 1.5004 - 8789.1 sample/sec\n",
            "Global step: 12543 - [==>---------------------------]   8% - acc: 0.9609 - loss: 1.5000 - 8601.8 sample/sec\n",
            "Global step: 12553 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5310 - 8528.0 sample/sec\n",
            "Global step: 12563 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5313 - 8736.1 sample/sec\n",
            "Global step: 12573 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5076 - 8951.9 sample/sec\n",
            "Global step: 12583 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5546 - 8780.3 sample/sec\n",
            "Global step: 12593 - [=====>------------------------]  20% - acc: 0.9531 - loss: 1.5072 - 8936.7 sample/sec\n",
            "Global step: 12603 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5080 - 8575.4 sample/sec\n",
            "Global step: 12613 - [=======>----------------------]  26% - acc: 0.9766 - loss: 1.4848 - 9032.1 sample/sec\n",
            "Global step: 12623 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5157 - 8844.1 sample/sec\n",
            "Global step: 12633 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5235 - 8633.7 sample/sec\n",
            "Global step: 12643 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5079 - 8800.9 sample/sec\n",
            "Global step: 12653 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5307 - 8659.6 sample/sec\n",
            "Global step: 12663 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5239 - 8988.6 sample/sec\n",
            "Global step: 12673 - [===========>------------------]  41% - acc: 0.9297 - loss: 1.5305 - 8849.9 sample/sec\n",
            "Global step: 12683 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5311 - 8838.8 sample/sec\n",
            "Global step: 12693 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5395 - 8786.0 sample/sec\n",
            "Global step: 12703 - [==============>---------------]  49% - acc: 0.9062 - loss: 1.5539 - 8793.1 sample/sec\n",
            "Global step: 12713 - [==============>---------------]  51% - acc: 0.8906 - loss: 1.5698 - 8672.6 sample/sec\n",
            "Global step: 12723 - [===============>--------------]  54% - acc: 0.9375 - loss: 1.5235 - 8707.5 sample/sec\n",
            "Global step: 12733 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5079 - 8881.2 sample/sec\n",
            "Global step: 12743 - [=================>------------]  59% - acc: 0.9531 - loss: 1.5079 - 8728.0 sample/sec\n",
            "Global step: 12753 - [=================>------------]  61% - acc: 0.8750 - loss: 1.5893 - 8993.3 sample/sec\n",
            "Global step: 12763 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5696 - 8645.1 sample/sec\n",
            "Global step: 12773 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5542 - 8636.2 sample/sec\n",
            "Global step: 12783 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5160 - 8772.3 sample/sec\n",
            "Global step: 12793 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8807.8 sample/sec\n",
            "Global step: 12803 - [=====================>--------]  74% - acc: 0.9453 - loss: 1.5160 - 8723.2 sample/sec\n",
            "Global step: 12813 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5003 - 7665.6 sample/sec\n",
            "Global step: 12823 - [======================>-------]  79% - acc: 0.8906 - loss: 1.5693 - 8650.8 sample/sec\n",
            "Global step: 12833 - [=======================>------]  82% - acc: 0.9062 - loss: 1.5540 - 8833.9 sample/sec\n",
            "Global step: 12843 - [========================>-----]  84% - acc: 0.9766 - loss: 1.4847 - 8741.3 sample/sec\n",
            "Global step: 12853 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4920 - 8548.1 sample/sec\n",
            "Global step: 12863 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5464 - 8542.6 sample/sec\n",
            "Global step: 12873 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5544 - 8749.2 sample/sec\n",
            "Global step: 12883 - [===========================>--]  95% - acc: 0.9062 - loss: 1.5545 - 7426.4 sample/sec\n",
            "Global step: 12893 - [============================>-]  97% - acc: 0.9531 - loss: 1.5076 - 8645.3 sample/sec\n",
            "Global step: 12903 - [=============================>] 100% - acc: 0.9000 - loss: 1.5605 - 12628.1 sample/sec\n",
            "\n",
            "Epoch 33 - accuracy: 78.56% (7856/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 34/60\n",
            "\n",
            "Global step: 12904 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5156 - 9060.2 sample/sec\n",
            "Global step: 12914 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5234 - 8849.9 sample/sec\n",
            "Global step: 12924 - [=>----------------------------]   5% - acc: 0.9609 - loss: 1.5000 - 8238.9 sample/sec\n",
            "Global step: 12934 - [==>---------------------------]   8% - acc: 0.9609 - loss: 1.4998 - 8717.3 sample/sec\n",
            "Global step: 12944 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5311 - 8951.1 sample/sec\n",
            "Global step: 12954 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5315 - 8860.7 sample/sec\n",
            "Global step: 12964 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5079 - 8620.3 sample/sec\n",
            "Global step: 12974 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5547 - 8401.1 sample/sec\n",
            "Global step: 12984 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.5025 - 8460.5 sample/sec\n",
            "Global step: 12994 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5080 - 8087.8 sample/sec\n",
            "Global step: 13004 - [=======>----------------------]  26% - acc: 0.9766 - loss: 1.4852 - 8666.8 sample/sec\n",
            "Global step: 13014 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8671.5 sample/sec\n",
            "Global step: 13024 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5232 - 8512.0 sample/sec\n",
            "Global step: 13034 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5080 - 8722.7 sample/sec\n",
            "Global step: 13044 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5304 - 8619.0 sample/sec\n",
            "Global step: 13054 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5238 - 8785.2 sample/sec\n",
            "Global step: 13064 - [===========>------------------]  41% - acc: 0.9297 - loss: 1.5310 - 8224.6 sample/sec\n",
            "Global step: 13074 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5308 - 8771.4 sample/sec\n",
            "Global step: 13084 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5390 - 8835.8 sample/sec\n",
            "Global step: 13094 - [==============>---------------]  49% - acc: 0.9062 - loss: 1.5531 - 8509.5 sample/sec\n",
            "Global step: 13104 - [==============>---------------]  51% - acc: 0.8906 - loss: 1.5708 - 8765.7 sample/sec\n",
            "Global step: 13114 - [===============>--------------]  54% - acc: 0.9375 - loss: 1.5236 - 8747.1 sample/sec\n",
            "Global step: 13124 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5084 - 8718.3 sample/sec\n",
            "Global step: 13134 - [=================>------------]  59% - acc: 0.9531 - loss: 1.5079 - 8887.1 sample/sec\n",
            "Global step: 13144 - [=================>------------]  61% - acc: 0.8750 - loss: 1.5860 - 8885.4 sample/sec\n",
            "Global step: 13154 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5694 - 8663.4 sample/sec\n",
            "Global step: 13164 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5541 - 8766.1 sample/sec\n",
            "Global step: 13174 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5159 - 8727.1 sample/sec\n",
            "Global step: 13184 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8766.1 sample/sec\n",
            "Global step: 13194 - [=====================>--------]  74% - acc: 0.9453 - loss: 1.5161 - 8777.9 sample/sec\n",
            "Global step: 13204 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5001 - 8790.1 sample/sec\n",
            "Global step: 13214 - [======================>-------]  79% - acc: 0.8906 - loss: 1.5690 - 8767.5 sample/sec\n",
            "Global step: 13224 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5486 - 8565.1 sample/sec\n",
            "Global step: 13234 - [========================>-----]  84% - acc: 0.9766 - loss: 1.4848 - 8865.8 sample/sec\n",
            "Global step: 13244 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4923 - 8451.7 sample/sec\n",
            "Global step: 13254 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5464 - 8713.7 sample/sec\n",
            "Global step: 13264 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5544 - 8744.7 sample/sec\n",
            "Global step: 13274 - [===========================>--]  95% - acc: 0.9062 - loss: 1.5542 - 8891.5 sample/sec\n",
            "Global step: 13284 - [============================>-]  97% - acc: 0.9531 - loss: 1.5077 - 8511.8 sample/sec\n",
            "Global step: 13294 - [=============================>] 100% - acc: 0.9000 - loss: 1.5601 - 12165.9 sample/sec\n",
            "\n",
            "Epoch 34 - accuracy: 78.64% (7864/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 35/60\n",
            "\n",
            "Global step: 13295 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5153 - 8662.7 sample/sec\n",
            "Global step: 13305 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5229 - 8528.8 sample/sec\n",
            "Global step: 13315 - [=>----------------------------]   5% - acc: 0.9609 - loss: 1.5008 - 8744.7 sample/sec\n",
            "Global step: 13325 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4958 - 8848.6 sample/sec\n",
            "Global step: 13335 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5311 - 8824.7 sample/sec\n",
            "Global step: 13345 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5313 - 8959.6 sample/sec\n",
            "Global step: 13355 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5081 - 8890.9 sample/sec\n",
            "Global step: 13365 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5551 - 8733.7 sample/sec\n",
            "Global step: 13375 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.5010 - 8627.9 sample/sec\n",
            "Global step: 13385 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5079 - 8795.8 sample/sec\n",
            "Global step: 13395 - [=======>----------------------]  26% - acc: 0.9766 - loss: 1.4846 - 8650.8 sample/sec\n",
            "Global step: 13405 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5162 - 8565.8 sample/sec\n",
            "Global step: 13415 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5239 - 8624.6 sample/sec\n",
            "Global step: 13425 - [=========>--------------------]  33% - acc: 0.9453 - loss: 1.5140 - 8842.6 sample/sec\n",
            "Global step: 13435 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5308 - 8627.5 sample/sec\n",
            "Global step: 13445 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5237 - 8710.1 sample/sec\n",
            "Global step: 13455 - [===========>------------------]  41% - acc: 0.9297 - loss: 1.5336 - 8647.9 sample/sec\n",
            "Global step: 13465 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5313 - 8821.0 sample/sec\n",
            "Global step: 13475 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5391 - 8595.7 sample/sec\n",
            "Global step: 13485 - [==============>---------------]  49% - acc: 0.9062 - loss: 1.5589 - 8735.7 sample/sec\n",
            "Global step: 13495 - [==============>---------------]  51% - acc: 0.8906 - loss: 1.5699 - 8740.0 sample/sec\n",
            "Global step: 13505 - [===============>--------------]  54% - acc: 0.9375 - loss: 1.5241 - 8646.1 sample/sec\n",
            "Global step: 13515 - [================>-------------]  56% - acc: 0.9453 - loss: 1.5124 - 8547.3 sample/sec\n",
            "Global step: 13525 - [=================>------------]  59% - acc: 0.9531 - loss: 1.5077 - 8874.5 sample/sec\n",
            "Global step: 13535 - [=================>------------]  61% - acc: 0.8750 - loss: 1.5862 - 8825.5 sample/sec\n",
            "Global step: 13545 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5697 - 8599.7 sample/sec\n",
            "Global step: 13555 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5545 - 8569.5 sample/sec\n",
            "Global step: 13565 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5163 - 8634.6 sample/sec\n",
            "Global step: 13575 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5161 - 7646.6 sample/sec\n",
            "Global step: 13585 - [=====================>--------]  74% - acc: 0.9375 - loss: 1.5197 - 8604.0 sample/sec\n",
            "Global step: 13595 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5000 - 8880.5 sample/sec\n",
            "Global step: 13605 - [======================>-------]  79% - acc: 0.8906 - loss: 1.5695 - 8634.7 sample/sec\n",
            "Global step: 13615 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5475 - 8394.9 sample/sec\n",
            "Global step: 13625 - [========================>-----]  84% - acc: 0.9766 - loss: 1.4821 - 8931.8 sample/sec\n",
            "Global step: 13635 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4921 - 8888.4 sample/sec\n",
            "Global step: 13645 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5468 - 8566.8 sample/sec\n",
            "Global step: 13655 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5545 - 8617.5 sample/sec\n",
            "Global step: 13665 - [===========================>--]  95% - acc: 0.9062 - loss: 1.5543 - 8885.8 sample/sec\n",
            "Global step: 13675 - [============================>-]  97% - acc: 0.9531 - loss: 1.5085 - 8801.2 sample/sec\n",
            "Global step: 13685 - [=============================>] 100% - acc: 0.9000 - loss: 1.5595 - 12561.9 sample/sec\n",
            "\n",
            "Epoch 35 - accuracy: 78.73% (7873/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 36/60\n",
            "\n",
            "Global step: 13686 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5154 - 9077.2 sample/sec\n",
            "Global step: 13696 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5233 - 8493.6 sample/sec\n",
            "Global step: 13706 - [=>----------------------------]   5% - acc: 0.9609 - loss: 1.5001 - 8896.4 sample/sec\n",
            "Global step: 13716 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4933 - 8593.4 sample/sec\n",
            "Global step: 13726 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5311 - 8676.4 sample/sec\n",
            "Global step: 13736 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5315 - 8566.4 sample/sec\n",
            "Global step: 13746 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5078 - 9106.6 sample/sec\n",
            "Global step: 13756 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5546 - 8868.6 sample/sec\n",
            "Global step: 13766 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.5004 - 8611.0 sample/sec\n",
            "Global step: 13776 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5078 - 8848.3 sample/sec\n",
            "Global step: 13786 - [=======>----------------------]  26% - acc: 0.9766 - loss: 1.4842 - 8782.4 sample/sec\n",
            "Global step: 13796 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8656.8 sample/sec\n",
            "Global step: 13806 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5241 - 8679.2 sample/sec\n",
            "Global step: 13816 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5083 - 8681.8 sample/sec\n",
            "Global step: 13826 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5310 - 8440.2 sample/sec\n",
            "Global step: 13836 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5237 - 8768.1 sample/sec\n",
            "Global step: 13846 - [===========>------------------]  41% - acc: 0.9297 - loss: 1.5316 - 8777.9 sample/sec\n",
            "Global step: 13856 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5311 - 8834.8 sample/sec\n",
            "Global step: 13866 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5390 - 8789.2 sample/sec\n",
            "Global step: 13876 - [==============>---------------]  49% - acc: 0.9062 - loss: 1.5532 - 8567.3 sample/sec\n",
            "Global step: 13886 - [==============>---------------]  51% - acc: 0.8906 - loss: 1.5707 - 8714.3 sample/sec\n",
            "Global step: 13896 - [===============>--------------]  54% - acc: 0.9375 - loss: 1.5244 - 8719.4 sample/sec\n",
            "Global step: 13906 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5109 - 8885.4 sample/sec\n",
            "Global step: 13916 - [=================>------------]  59% - acc: 0.9531 - loss: 1.5080 - 8743.8 sample/sec\n",
            "Global step: 13926 - [=================>------------]  61% - acc: 0.8750 - loss: 1.5859 - 8811.6 sample/sec\n",
            "Global step: 13936 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5699 - 8571.6 sample/sec\n",
            "Global step: 13946 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5548 - 8698.6 sample/sec\n",
            "Global step: 13956 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5161 - 8699.9 sample/sec\n",
            "Global step: 13966 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8794.2 sample/sec\n",
            "Global step: 13976 - [=====================>--------]  74% - acc: 0.9453 - loss: 1.5159 - 8469.5 sample/sec\n",
            "Global step: 13986 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5003 - 8707.7 sample/sec\n",
            "Global step: 13996 - [======================>-------]  79% - acc: 0.8828 - loss: 1.5767 - 8510.0 sample/sec\n",
            "Global step: 14006 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5492 - 8435.8 sample/sec\n",
            "Global step: 14016 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4769 - 8667.0 sample/sec\n",
            "Global step: 14026 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4922 - 8808.4 sample/sec\n",
            "Global step: 14036 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5467 - 8848.2 sample/sec\n",
            "Global step: 14046 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5545 - 8473.6 sample/sec\n",
            "Global step: 14056 - [===========================>--]  95% - acc: 0.9062 - loss: 1.5545 - 8743.8 sample/sec\n",
            "Global step: 14066 - [============================>-]  97% - acc: 0.9531 - loss: 1.5086 - 8599.6 sample/sec\n",
            "Global step: 14076 - [=============================>] 100% - acc: 0.9125 - loss: 1.5475 - 12678.2 sample/sec\n",
            "\n",
            "Epoch 36 - accuracy: 78.57% (7857/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 37/60\n",
            "\n",
            "Global step: 14077 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5157 - 8757.7 sample/sec\n",
            "Global step: 14087 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5234 - 8876.1 sample/sec\n",
            "Global step: 14097 - [=>----------------------------]   5% - acc: 0.9609 - loss: 1.5001 - 9122.4 sample/sec\n",
            "Global step: 14107 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4926 - 8730.6 sample/sec\n",
            "Global step: 14117 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5309 - 7975.6 sample/sec\n",
            "Global step: 14127 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5319 - 8792.8 sample/sec\n",
            "Global step: 14137 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5084 - 8671.8 sample/sec\n",
            "Global step: 14147 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5546 - 8715.0 sample/sec\n",
            "Global step: 14157 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.5002 - 8742.3 sample/sec\n",
            "Global step: 14167 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5078 - 8795.1 sample/sec\n",
            "Global step: 14177 - [=======>----------------------]  26% - acc: 0.9766 - loss: 1.4835 - 8728.5 sample/sec\n",
            "Global step: 14187 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5157 - 8966.5 sample/sec\n",
            "Global step: 14197 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5231 - 8884.6 sample/sec\n",
            "Global step: 14207 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5085 - 8796.4 sample/sec\n",
            "Global step: 14217 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5309 - 8940.7 sample/sec\n",
            "Global step: 14227 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5236 - 8496.4 sample/sec\n",
            "Global step: 14237 - [===========>------------------]  41% - acc: 0.9297 - loss: 1.5307 - 8803.6 sample/sec\n",
            "Global step: 14247 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5310 - 8759.8 sample/sec\n",
            "Global step: 14257 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5392 - 8621.2 sample/sec\n",
            "Global step: 14267 - [==============>---------------]  49% - acc: 0.9062 - loss: 1.5533 - 9000.2 sample/sec\n",
            "Global step: 14277 - [==============>---------------]  51% - acc: 0.8906 - loss: 1.5696 - 8700.2 sample/sec\n",
            "Global step: 14287 - [===============>--------------]  54% - acc: 0.9375 - loss: 1.5236 - 8842.6 sample/sec\n",
            "Global step: 14297 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5081 - 8626.8 sample/sec\n",
            "Global step: 14307 - [=================>------------]  59% - acc: 0.9531 - loss: 1.5074 - 8553.8 sample/sec\n",
            "Global step: 14317 - [=================>------------]  61% - acc: 0.8750 - loss: 1.5855 - 8482.2 sample/sec\n",
            "Global step: 14327 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5698 - 8823.1 sample/sec\n",
            "Global step: 14337 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5546 - 8822.7 sample/sec\n",
            "Global step: 14347 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5158 - 8834.3 sample/sec\n",
            "Global step: 14357 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5161 - 8568.4 sample/sec\n",
            "Global step: 14367 - [=====================>--------]  74% - acc: 0.9453 - loss: 1.5158 - 7931.4 sample/sec\n",
            "Global step: 14377 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5003 - 8728.5 sample/sec\n",
            "Global step: 14387 - [======================>-------]  79% - acc: 0.8906 - loss: 1.5695 - 8865.5 sample/sec\n",
            "Global step: 14397 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5465 - 8637.8 sample/sec\n",
            "Global step: 14407 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4769 - 8706.4 sample/sec\n",
            "Global step: 14417 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4918 - 8718.0 sample/sec\n",
            "Global step: 14427 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5468 - 8562.4 sample/sec\n",
            "Global step: 14437 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5544 - 8551.1 sample/sec\n",
            "Global step: 14447 - [===========================>--]  95% - acc: 0.9062 - loss: 1.5548 - 8604.0 sample/sec\n",
            "Global step: 14457 - [============================>-]  97% - acc: 0.9531 - loss: 1.5078 - 8976.7 sample/sec\n",
            "Global step: 14467 - [=============================>] 100% - acc: 0.9125 - loss: 1.5473 - 12445.7 sample/sec\n",
            "\n",
            "Epoch 37 - accuracy: 78.69% (7869/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 38/60\n",
            "\n",
            "Global step: 14468 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5154 - 8628.0 sample/sec\n",
            "Global step: 14478 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5228 - 8826.2 sample/sec\n",
            "Global step: 14488 - [=>----------------------------]   5% - acc: 0.9609 - loss: 1.4996 - 8663.0 sample/sec\n",
            "Global step: 14498 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4934 - 8896.0 sample/sec\n",
            "Global step: 14508 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5310 - 8705.0 sample/sec\n",
            "Global step: 14518 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5315 - 8761.5 sample/sec\n",
            "Global step: 14528 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5080 - 8698.9 sample/sec\n",
            "Global step: 14538 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5550 - 8798.4 sample/sec\n",
            "Global step: 14548 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.5001 - 8890.9 sample/sec\n",
            "Global step: 14558 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5078 - 8795.8 sample/sec\n",
            "Global step: 14568 - [=======>----------------------]  26% - acc: 0.9844 - loss: 1.4767 - 8499.5 sample/sec\n",
            "Global step: 14578 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8491.7 sample/sec\n",
            "Global step: 14588 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5236 - 8736.6 sample/sec\n",
            "Global step: 14598 - [=========>--------------------]  33% - acc: 0.9453 - loss: 1.5121 - 8403.3 sample/sec\n",
            "Global step: 14608 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5308 - 8571.3 sample/sec\n",
            "Global step: 14618 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5236 - 8658.0 sample/sec\n",
            "Global step: 14628 - [===========>------------------]  41% - acc: 0.9297 - loss: 1.5271 - 8512.2 sample/sec\n",
            "Global step: 14638 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5309 - 8217.8 sample/sec\n",
            "Global step: 14648 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5391 - 8640.7 sample/sec\n",
            "Global step: 14658 - [==============>---------------]  49% - acc: 0.9062 - loss: 1.5536 - 8667.0 sample/sec\n",
            "Global step: 14668 - [==============>---------------]  51% - acc: 0.8906 - loss: 1.5696 - 8669.7 sample/sec\n",
            "Global step: 14678 - [===============>--------------]  54% - acc: 0.9375 - loss: 1.5235 - 8769.5 sample/sec\n",
            "Global step: 14688 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5081 - 7310.0 sample/sec\n",
            "Global step: 14698 - [=================>------------]  59% - acc: 0.9609 - loss: 1.5004 - 8882.3 sample/sec\n",
            "Global step: 14708 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5791 - 8642.1 sample/sec\n",
            "Global step: 14718 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5713 - 8498.6 sample/sec\n",
            "Global step: 14728 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5543 - 8732.4 sample/sec\n",
            "Global step: 14738 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5159 - 8846.1 sample/sec\n",
            "Global step: 14748 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5162 - 8613.4 sample/sec\n",
            "Global step: 14758 - [=====================>--------]  74% - acc: 0.9453 - loss: 1.5167 - 8498.8 sample/sec\n",
            "Global step: 14768 - [======================>-------]  77% - acc: 0.9609 - loss: 1.4999 - 8859.0 sample/sec\n",
            "Global step: 14778 - [======================>-------]  79% - acc: 0.8906 - loss: 1.5694 - 8665.9 sample/sec\n",
            "Global step: 14788 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5469 - 8699.6 sample/sec\n",
            "Global step: 14798 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4769 - 8540.9 sample/sec\n",
            "Global step: 14808 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4923 - 8327.8 sample/sec\n",
            "Global step: 14818 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5463 - 8687.0 sample/sec\n",
            "Global step: 14828 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5546 - 8675.6 sample/sec\n",
            "Global step: 14838 - [===========================>--]  95% - acc: 0.9062 - loss: 1.5543 - 8849.5 sample/sec\n",
            "Global step: 14848 - [============================>-]  97% - acc: 0.9531 - loss: 1.5075 - 8677.3 sample/sec\n",
            "Global step: 14858 - [=============================>] 100% - acc: 0.9125 - loss: 1.5474 - 12679.4 sample/sec\n",
            "\n",
            "Epoch 38 - accuracy: 78.75% (7875/10000)\n",
            "This epoch receive better accuracy: 78.75 > 78.73. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 39/60\n",
            "\n",
            "Global step: 14859 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5156 - 9169.1 sample/sec\n",
            "Global step: 14869 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5230 - 8929.8 sample/sec\n",
            "Global step: 14879 - [=>----------------------------]   5% - acc: 0.9609 - loss: 1.4999 - 8611.2 sample/sec\n",
            "Global step: 14889 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4923 - 8786.0 sample/sec\n",
            "Global step: 14899 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5309 - 8765.1 sample/sec\n",
            "Global step: 14909 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5313 - 8868.3 sample/sec\n",
            "Global step: 14919 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5086 - 8095.4 sample/sec\n",
            "Global step: 14929 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5545 - 8827.9 sample/sec\n",
            "Global step: 14939 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.5004 - 8730.2 sample/sec\n",
            "Global step: 14949 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5080 - 8770.8 sample/sec\n",
            "Global step: 14959 - [=======>----------------------]  26% - acc: 0.9844 - loss: 1.4770 - 8656.0 sample/sec\n",
            "Global step: 14969 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5159 - 8666.8 sample/sec\n",
            "Global step: 14979 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5235 - 8695.3 sample/sec\n",
            "Global step: 14989 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5084 - 8710.8 sample/sec\n",
            "Global step: 14999 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5319 - 8645.1 sample/sec\n",
            "Global step: 15009 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5238 - 8689.1 sample/sec\n",
            "Global step: 15019 - [===========>------------------]  41% - acc: 0.9375 - loss: 1.5229 - 8638.2 sample/sec\n",
            "Global step: 15029 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5310 - 8874.0 sample/sec\n",
            "Global step: 15039 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5424 - 8864.4 sample/sec\n",
            "Global step: 15049 - [==============>---------------]  49% - acc: 0.9141 - loss: 1.5463 - 8739.8 sample/sec\n",
            "Global step: 15059 - [==============>---------------]  51% - acc: 0.8984 - loss: 1.5630 - 8493.6 sample/sec\n",
            "Global step: 15069 - [===============>--------------]  54% - acc: 0.9375 - loss: 1.5211 - 8665.4 sample/sec\n",
            "Global step: 15079 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5116 - 8846.4 sample/sec\n",
            "Global step: 15089 - [=================>------------]  59% - acc: 0.9609 - loss: 1.5006 - 8644.8 sample/sec\n",
            "Global step: 15099 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5788 - 8719.1 sample/sec\n",
            "Global step: 15109 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5700 - 8941.0 sample/sec\n",
            "Global step: 15119 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5548 - 8753.9 sample/sec\n",
            "Global step: 15129 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5157 - 8728.0 sample/sec\n",
            "Global step: 15139 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5160 - 8349.7 sample/sec\n",
            "Global step: 15149 - [=====================>--------]  74% - acc: 0.9453 - loss: 1.5160 - 8751.7 sample/sec\n",
            "Global step: 15159 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5002 - 8681.5 sample/sec\n",
            "Global step: 15169 - [======================>-------]  79% - acc: 0.8906 - loss: 1.5700 - 8871.3 sample/sec\n",
            "Global step: 15179 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5466 - 8657.8 sample/sec\n",
            "Global step: 15189 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8641.5 sample/sec\n",
            "Global step: 15199 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4923 - 8710.9 sample/sec\n",
            "Global step: 15209 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5467 - 8539.0 sample/sec\n",
            "Global step: 15219 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5551 - 8653.2 sample/sec\n",
            "Global step: 15229 - [===========================>--]  95% - acc: 0.9062 - loss: 1.5527 - 8787.3 sample/sec\n",
            "Global step: 15239 - [============================>-]  97% - acc: 0.9531 - loss: 1.5076 - 8761.1 sample/sec\n",
            "Global step: 15249 - [=============================>] 100% - acc: 0.9125 - loss: 1.5478 - 12666.5 sample/sec\n",
            "\n",
            "Epoch 39 - accuracy: 78.36% (7836/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 40/60\n",
            "\n",
            "Global step: 15250 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5159 - 8045.9 sample/sec\n",
            "Global step: 15260 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5232 - 8187.4 sample/sec\n",
            "Global step: 15270 - [=>----------------------------]   5% - acc: 0.9609 - loss: 1.4999 - 8498.8 sample/sec\n",
            "Global step: 15280 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4925 - 8669.4 sample/sec\n",
            "Global step: 15290 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5321 - 8833.0 sample/sec\n",
            "Global step: 15300 - [===>--------------------------]  13% - acc: 0.9219 - loss: 1.5350 - 8529.9 sample/sec\n",
            "Global step: 15310 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5080 - 8788.1 sample/sec\n",
            "Global step: 15320 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5548 - 8768.8 sample/sec\n",
            "Global step: 15330 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.5001 - 8732.6 sample/sec\n",
            "Global step: 15340 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5079 - 8868.2 sample/sec\n",
            "Global step: 15350 - [=======>----------------------]  26% - acc: 0.9844 - loss: 1.4772 - 8860.4 sample/sec\n",
            "Global step: 15360 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5166 - 8924.2 sample/sec\n",
            "Global step: 15370 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5232 - 8691.6 sample/sec\n",
            "Global step: 15380 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5081 - 8569.2 sample/sec\n",
            "Global step: 15390 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5307 - 9078.7 sample/sec\n",
            "Global step: 15400 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5237 - 8533.7 sample/sec\n",
            "Global step: 15410 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5199 - 8527.4 sample/sec\n",
            "Global step: 15420 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5312 - 8750.5 sample/sec\n",
            "Global step: 15430 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5404 - 8826.0 sample/sec\n",
            "Global step: 15440 - [==============>---------------]  49% - acc: 0.9141 - loss: 1.5460 - 8712.2 sample/sec\n",
            "Global step: 15450 - [==============>---------------]  51% - acc: 0.8984 - loss: 1.5605 - 8708.1 sample/sec\n",
            "Global step: 15460 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5157 - 8978.7 sample/sec\n",
            "Global step: 15470 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5080 - 8687.4 sample/sec\n",
            "Global step: 15480 - [=================>------------]  59% - acc: 0.9609 - loss: 1.5002 - 8726.9 sample/sec\n",
            "Global step: 15490 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5781 - 8691.9 sample/sec\n",
            "Global step: 15500 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5704 - 8883.3 sample/sec\n",
            "Global step: 15510 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5547 - 8280.6 sample/sec\n",
            "Global step: 15520 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5158 - 8806.8 sample/sec\n",
            "Global step: 15530 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5163 - 8801.0 sample/sec\n",
            "Global step: 15540 - [=====================>--------]  74% - acc: 0.9453 - loss: 1.5127 - 8641.2 sample/sec\n",
            "Global step: 15550 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5002 - 8597.6 sample/sec\n",
            "Global step: 15560 - [======================>-------]  79% - acc: 0.8906 - loss: 1.5695 - 8941.1 sample/sec\n",
            "Global step: 15570 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5470 - 8473.5 sample/sec\n",
            "Global step: 15580 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4773 - 8546.6 sample/sec\n",
            "Global step: 15590 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4922 - 8824.0 sample/sec\n",
            "Global step: 15600 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5464 - 8768.4 sample/sec\n",
            "Global step: 15610 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5546 - 8625.0 sample/sec\n",
            "Global step: 15620 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5471 - 8811.0 sample/sec\n",
            "Global step: 15630 - [============================>-]  97% - acc: 0.9531 - loss: 1.5080 - 8771.7 sample/sec\n",
            "Global step: 15640 - [=============================>] 100% - acc: 0.9125 - loss: 1.5478 - 12694.7 sample/sec\n",
            "\n",
            "Epoch 40 - accuracy: 78.56% (7856/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 41/60\n",
            "\n",
            "Global step: 15641 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5154 - 8867.5 sample/sec\n",
            "Global step: 15651 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5240 - 8849.8 sample/sec\n",
            "Global step: 15661 - [=>----------------------------]   5% - acc: 0.9609 - loss: 1.4999 - 8709.6 sample/sec\n",
            "Global step: 15671 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4922 - 8847.0 sample/sec\n",
            "Global step: 15681 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5311 - 8814.2 sample/sec\n",
            "Global step: 15691 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5310 - 8694.8 sample/sec\n",
            "Global step: 15701 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5111 - 8819.7 sample/sec\n",
            "Global step: 15711 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5544 - 8657.4 sample/sec\n",
            "Global step: 15721 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.5001 - 8744.7 sample/sec\n",
            "Global step: 15731 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5082 - 8646.7 sample/sec\n",
            "Global step: 15741 - [=======>----------------------]  26% - acc: 0.9844 - loss: 1.4767 - 8735.9 sample/sec\n",
            "Global step: 15751 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5157 - 8754.2 sample/sec\n",
            "Global step: 15761 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5234 - 8741.5 sample/sec\n",
            "Global step: 15771 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5080 - 8698.9 sample/sec\n",
            "Global step: 15781 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5307 - 8860.3 sample/sec\n",
            "Global step: 15791 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5237 - 8887.4 sample/sec\n",
            "Global step: 15801 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5155 - 8429.3 sample/sec\n",
            "Global step: 15811 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5313 - 8331.3 sample/sec\n",
            "Global step: 15821 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5403 - 8591.3 sample/sec\n",
            "Global step: 15831 - [==============>---------------]  49% - acc: 0.9141 - loss: 1.5458 - 8869.6 sample/sec\n",
            "Global step: 15841 - [==============>---------------]  51% - acc: 0.9062 - loss: 1.5548 - 8790.4 sample/sec\n",
            "Global step: 15851 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5157 - 8576.5 sample/sec\n",
            "Global step: 15861 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5080 - 8722.8 sample/sec\n",
            "Global step: 15871 - [=================>------------]  59% - acc: 0.9609 - loss: 1.5004 - 8039.8 sample/sec\n",
            "Global step: 15881 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5783 - 8531.4 sample/sec\n",
            "Global step: 15891 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5699 - 8715.6 sample/sec\n",
            "Global step: 15901 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5546 - 8573.3 sample/sec\n",
            "Global step: 15911 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5157 - 8383.9 sample/sec\n",
            "Global step: 15921 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8719.0 sample/sec\n",
            "Global step: 15931 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5080 - 8792.7 sample/sec\n",
            "Global step: 15941 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5001 - 8751.7 sample/sec\n",
            "Global step: 15951 - [======================>-------]  79% - acc: 0.8906 - loss: 1.5689 - 8729.5 sample/sec\n",
            "Global step: 15961 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5470 - 8624.6 sample/sec\n",
            "Global step: 15971 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8548.9 sample/sec\n",
            "Global step: 15981 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4924 - 8504.3 sample/sec\n",
            "Global step: 15991 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5465 - 8705.1 sample/sec\n",
            "Global step: 16001 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5546 - 8781.4 sample/sec\n",
            "Global step: 16011 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5472 - 8699.5 sample/sec\n",
            "Global step: 16021 - [============================>-]  97% - acc: 0.9531 - loss: 1.5084 - 8369.6 sample/sec\n",
            "Global step: 16031 - [=============================>] 100% - acc: 0.9125 - loss: 1.5480 - 12614.7 sample/sec\n",
            "\n",
            "Epoch 41 - accuracy: 79.03% (7903/10000)\n",
            "This epoch receive better accuracy: 79.03 > 78.75. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 42/60\n",
            "\n",
            "Global step: 16032 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5159 - 9082.3 sample/sec\n",
            "Global step: 16042 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5231 - 8589.5 sample/sec\n",
            "Global step: 16052 - [=>----------------------------]   5% - acc: 0.9609 - loss: 1.4986 - 8690.2 sample/sec\n",
            "Global step: 16062 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4921 - 8158.4 sample/sec\n",
            "Global step: 16072 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5308 - 8489.0 sample/sec\n",
            "Global step: 16082 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5313 - 8766.0 sample/sec\n",
            "Global step: 16092 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5078 - 8338.3 sample/sec\n",
            "Global step: 16102 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5546 - 8447.1 sample/sec\n",
            "Global step: 16112 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.5000 - 8854.3 sample/sec\n",
            "Global step: 16122 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5079 - 8648.6 sample/sec\n",
            "Global step: 16132 - [=======>----------------------]  26% - acc: 0.9844 - loss: 1.4768 - 8886.8 sample/sec\n",
            "Global step: 16142 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8800.6 sample/sec\n",
            "Global step: 16152 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5232 - 8648.0 sample/sec\n",
            "Global step: 16162 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5080 - 8678.0 sample/sec\n",
            "Global step: 16172 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5305 - 8584.6 sample/sec\n",
            "Global step: 16182 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5236 - 8731.2 sample/sec\n",
            "Global step: 16192 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5153 - 8824.5 sample/sec\n",
            "Global step: 16202 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5311 - 8787.8 sample/sec\n",
            "Global step: 16212 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5390 - 8631.4 sample/sec\n",
            "Global step: 16222 - [==============>---------------]  49% - acc: 0.9141 - loss: 1.5449 - 8583.1 sample/sec\n",
            "Global step: 16232 - [==============>---------------]  51% - acc: 0.9062 - loss: 1.5547 - 7735.1 sample/sec\n",
            "Global step: 16242 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5156 - 8654.0 sample/sec\n",
            "Global step: 16252 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5079 - 8553.0 sample/sec\n",
            "Global step: 16262 - [=================>------------]  59% - acc: 0.9609 - loss: 1.5002 - 8894.0 sample/sec\n",
            "Global step: 16272 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5781 - 8586.8 sample/sec\n",
            "Global step: 16282 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5698 - 8622.8 sample/sec\n",
            "Global step: 16292 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5544 - 8631.8 sample/sec\n",
            "Global step: 16302 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5158 - 8718.6 sample/sec\n",
            "Global step: 16312 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5160 - 8581.7 sample/sec\n",
            "Global step: 16322 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5078 - 8783.6 sample/sec\n",
            "Global step: 16332 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5001 - 8534.1 sample/sec\n",
            "Global step: 16342 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5619 - 8264.0 sample/sec\n",
            "Global step: 16352 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5468 - 8844.4 sample/sec\n",
            "Global step: 16362 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8537.6 sample/sec\n",
            "Global step: 16372 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4921 - 8731.2 sample/sec\n",
            "Global step: 16382 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5454 - 8380.5 sample/sec\n",
            "Global step: 16392 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5546 - 8793.8 sample/sec\n",
            "Global step: 16402 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5469 - 8570.9 sample/sec\n",
            "Global step: 16412 - [============================>-]  97% - acc: 0.9531 - loss: 1.5076 - 8812.9 sample/sec\n",
            "Global step: 16422 - [=============================>] 100% - acc: 0.9125 - loss: 1.5476 - 12504.0 sample/sec\n",
            "\n",
            "Epoch 42 - accuracy: 79.06% (7906/10000)\n",
            "This epoch receive better accuracy: 79.06 > 79.03. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 43/60\n",
            "\n",
            "Global step: 16423 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5159 - 8989.0 sample/sec\n",
            "Global step: 16433 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5230 - 8720.3 sample/sec\n",
            "Global step: 16443 - [=>----------------------------]   5% - acc: 0.9688 - loss: 1.4963 - 8456.4 sample/sec\n",
            "Global step: 16453 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4921 - 8430.1 sample/sec\n",
            "Global step: 16463 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5309 - 8773.7 sample/sec\n",
            "Global step: 16473 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5313 - 8687.5 sample/sec\n",
            "Global step: 16483 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5077 - 8493.0 sample/sec\n",
            "Global step: 16493 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5545 - 8812.7 sample/sec\n",
            "Global step: 16503 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.5000 - 8901.3 sample/sec\n",
            "Global step: 16513 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5078 - 8856.9 sample/sec\n",
            "Global step: 16523 - [=======>----------------------]  26% - acc: 0.9844 - loss: 1.4768 - 8675.7 sample/sec\n",
            "Global step: 16533 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8597.5 sample/sec\n",
            "Global step: 16543 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5231 - 8671.7 sample/sec\n",
            "Global step: 16553 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5080 - 8672.6 sample/sec\n",
            "Global step: 16563 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5304 - 8903.6 sample/sec\n",
            "Global step: 16573 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5236 - 8975.4 sample/sec\n",
            "Global step: 16583 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5152 - 8511.2 sample/sec\n",
            "Global step: 16593 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5310 - 8931.6 sample/sec\n",
            "Global step: 16603 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5389 - 8574.8 sample/sec\n",
            "Global step: 16613 - [==============>---------------]  49% - acc: 0.9141 - loss: 1.5447 - 8515.5 sample/sec\n",
            "Global step: 16623 - [==============>---------------]  51% - acc: 0.9062 - loss: 1.5546 - 8467.5 sample/sec\n",
            "Global step: 16633 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5155 - 8382.5 sample/sec\n",
            "Global step: 16643 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5078 - 8545.2 sample/sec\n",
            "Global step: 16653 - [=================>------------]  59% - acc: 0.9609 - loss: 1.5001 - 8757.2 sample/sec\n",
            "Global step: 16663 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5780 - 8681.6 sample/sec\n",
            "Global step: 16673 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5698 - 8656.1 sample/sec\n",
            "Global step: 16683 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5542 - 8685.8 sample/sec\n",
            "Global step: 16693 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5158 - 8758.2 sample/sec\n",
            "Global step: 16703 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5160 - 8770.5 sample/sec\n",
            "Global step: 16713 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5077 - 8786.0 sample/sec\n",
            "Global step: 16723 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5001 - 8499.1 sample/sec\n",
            "Global step: 16733 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5617 - 8677.7 sample/sec\n",
            "Global step: 16743 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5468 - 8763.2 sample/sec\n",
            "Global step: 16753 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8845.2 sample/sec\n",
            "Global step: 16763 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4921 - 8615.6 sample/sec\n",
            "Global step: 16773 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5448 - 8596.7 sample/sec\n",
            "Global step: 16783 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5546 - 8790.6 sample/sec\n",
            "Global step: 16793 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5469 - 8798.9 sample/sec\n",
            "Global step: 16803 - [============================>-]  97% - acc: 0.9531 - loss: 1.5076 - 8644.4 sample/sec\n",
            "Global step: 16813 - [=============================>] 100% - acc: 0.9125 - loss: 1.5476 - 8758.7 sample/sec\n",
            "\n",
            "Epoch 43 - accuracy: 79.11% (7911/10000)\n",
            "This epoch receive better accuracy: 79.11 > 79.06. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 44/60\n",
            "\n",
            "Global step: 16814 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5158 - 8739.4 sample/sec\n",
            "Global step: 16824 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5230 - 8932.8 sample/sec\n",
            "Global step: 16834 - [=>----------------------------]   5% - acc: 0.9688 - loss: 1.4936 - 8788.9 sample/sec\n",
            "Global step: 16844 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4921 - 8878.7 sample/sec\n",
            "Global step: 16854 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5308 - 8609.4 sample/sec\n",
            "Global step: 16864 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5313 - 8841.3 sample/sec\n",
            "Global step: 16874 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5076 - 8290.3 sample/sec\n",
            "Global step: 16884 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5545 - 8671.4 sample/sec\n",
            "Global step: 16894 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.5000 - 8647.6 sample/sec\n",
            "Global step: 16904 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5077 - 8688.4 sample/sec\n",
            "Global step: 16914 - [=======>----------------------]  26% - acc: 0.9844 - loss: 1.4768 - 8744.0 sample/sec\n",
            "Global step: 16924 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8707.5 sample/sec\n",
            "Global step: 16934 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5230 - 8765.1 sample/sec\n",
            "Global step: 16944 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5080 - 8503.9 sample/sec\n",
            "Global step: 16954 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5304 - 8178.6 sample/sec\n",
            "Global step: 16964 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5236 - 8696.2 sample/sec\n",
            "Global step: 16974 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5152 - 8839.3 sample/sec\n",
            "Global step: 16984 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5311 - 8743.7 sample/sec\n",
            "Global step: 16994 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5390 - 9108.0 sample/sec\n",
            "Global step: 17004 - [==============>---------------]  49% - acc: 0.9141 - loss: 1.5443 - 8716.4 sample/sec\n",
            "Global step: 17014 - [==============>---------------]  51% - acc: 0.9062 - loss: 1.5546 - 8503.1 sample/sec\n",
            "Global step: 17024 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5154 - 8625.1 sample/sec\n",
            "Global step: 17034 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5078 - 8608.1 sample/sec\n",
            "Global step: 17044 - [=================>------------]  59% - acc: 0.9609 - loss: 1.5001 - 8790.6 sample/sec\n",
            "Global step: 17054 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5780 - 6927.9 sample/sec\n",
            "Global step: 17064 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5697 - 8750.7 sample/sec\n",
            "Global step: 17074 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5542 - 8905.7 sample/sec\n",
            "Global step: 17084 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5157 - 8434.7 sample/sec\n",
            "Global step: 17094 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8898.5 sample/sec\n",
            "Global step: 17104 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5077 - 8627.1 sample/sec\n",
            "Global step: 17114 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5001 - 8550.0 sample/sec\n",
            "Global step: 17124 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5616 - 8022.5 sample/sec\n",
            "Global step: 17134 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5468 - 8615.7 sample/sec\n",
            "Global step: 17144 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8827.2 sample/sec\n",
            "Global step: 17154 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4921 - 8579.0 sample/sec\n",
            "Global step: 17164 - [==========================>---]  90% - acc: 0.9141 - loss: 1.5437 - 8508.8 sample/sec\n",
            "Global step: 17174 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5546 - 8539.7 sample/sec\n",
            "Global step: 17184 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5468 - 8751.5 sample/sec\n",
            "Global step: 17194 - [============================>-]  97% - acc: 0.9531 - loss: 1.5076 - 8065.2 sample/sec\n",
            "Global step: 17204 - [=============================>] 100% - acc: 0.9125 - loss: 1.5475 - 12730.8 sample/sec\n",
            "\n",
            "Epoch 44 - accuracy: 79.10% (7910/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 45/60\n",
            "\n",
            "Global step: 17205 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5158 - 8882.4 sample/sec\n",
            "Global step: 17215 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5229 - 8433.5 sample/sec\n",
            "Global step: 17225 - [=>----------------------------]   5% - acc: 0.9688 - loss: 1.4929 - 8837.8 sample/sec\n",
            "Global step: 17235 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4921 - 8330.6 sample/sec\n",
            "Global step: 17245 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5308 - 8407.1 sample/sec\n",
            "Global step: 17255 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5312 - 8805.9 sample/sec\n",
            "Global step: 17265 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5076 - 8475.1 sample/sec\n",
            "Global step: 17275 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5544 - 8645.0 sample/sec\n",
            "Global step: 17285 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.5000 - 8541.3 sample/sec\n",
            "Global step: 17295 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5077 - 8663.3 sample/sec\n",
            "Global step: 17305 - [=======>----------------------]  26% - acc: 0.9844 - loss: 1.4767 - 7299.4 sample/sec\n",
            "Global step: 17315 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8644.8 sample/sec\n",
            "Global step: 17325 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5230 - 8897.3 sample/sec\n",
            "Global step: 17335 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5079 - 9160.5 sample/sec\n",
            "Global step: 17345 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5304 - 8509.1 sample/sec\n",
            "Global step: 17355 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5236 - 8686.4 sample/sec\n",
            "Global step: 17365 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5152 - 8843.6 sample/sec\n",
            "Global step: 17375 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5310 - 8599.6 sample/sec\n",
            "Global step: 17385 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5389 - 8839.7 sample/sec\n",
            "Global step: 17395 - [==============>---------------]  49% - acc: 0.9141 - loss: 1.5436 - 8845.5 sample/sec\n",
            "Global step: 17405 - [==============>---------------]  51% - acc: 0.9062 - loss: 1.5546 - 8612.8 sample/sec\n",
            "Global step: 17415 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5154 - 8392.1 sample/sec\n",
            "Global step: 17425 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5078 - 8492.6 sample/sec\n",
            "Global step: 17435 - [=================>------------]  59% - acc: 0.9609 - loss: 1.5001 - 8652.5 sample/sec\n",
            "Global step: 17445 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5780 - 8874.3 sample/sec\n",
            "Global step: 17455 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5697 - 8563.8 sample/sec\n",
            "Global step: 17465 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5541 - 8507.7 sample/sec\n",
            "Global step: 17475 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5156 - 8526.2 sample/sec\n",
            "Global step: 17485 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8809.4 sample/sec\n",
            "Global step: 17495 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5077 - 8556.3 sample/sec\n",
            "Global step: 17505 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5000 - 8713.2 sample/sec\n",
            "Global step: 17515 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5615 - 8644.6 sample/sec\n",
            "Global step: 17525 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5468 - 8360.8 sample/sec\n",
            "Global step: 17535 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8729.5 sample/sec\n",
            "Global step: 17545 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4920 - 7759.6 sample/sec\n",
            "Global step: 17555 - [==========================>---]  90% - acc: 0.9219 - loss: 1.5414 - 8684.7 sample/sec\n",
            "Global step: 17565 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5546 - 8532.2 sample/sec\n",
            "Global step: 17575 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5468 - 8612.3 sample/sec\n",
            "Global step: 17585 - [============================>-]  97% - acc: 0.9531 - loss: 1.5075 - 8850.6 sample/sec\n",
            "Global step: 17595 - [=============================>] 100% - acc: 0.9125 - loss: 1.5473 - 12486.5 sample/sec\n",
            "\n",
            "Epoch 45 - accuracy: 79.11% (7911/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 46/60\n",
            "\n",
            "Global step: 17596 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5157 - 9005.8 sample/sec\n",
            "Global step: 17606 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5228 - 8883.4 sample/sec\n",
            "Global step: 17616 - [=>----------------------------]   5% - acc: 0.9688 - loss: 1.4927 - 8814.6 sample/sec\n",
            "Global step: 17626 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4921 - 8785.0 sample/sec\n",
            "Global step: 17636 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5307 - 8717.4 sample/sec\n",
            "Global step: 17646 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5312 - 7510.0 sample/sec\n",
            "Global step: 17656 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5075 - 8776.7 sample/sec\n",
            "Global step: 17666 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5544 - 8573.7 sample/sec\n",
            "Global step: 17676 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.5000 - 8703.3 sample/sec\n",
            "Global step: 17686 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5077 - 8797.7 sample/sec\n",
            "Global step: 17696 - [=======>----------------------]  26% - acc: 0.9844 - loss: 1.4767 - 8694.7 sample/sec\n",
            "Global step: 17706 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8643.5 sample/sec\n",
            "Global step: 17716 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5230 - 8819.8 sample/sec\n",
            "Global step: 17726 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5079 - 8800.4 sample/sec\n",
            "Global step: 17736 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5303 - 8687.9 sample/sec\n",
            "Global step: 17746 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5235 - 8605.9 sample/sec\n",
            "Global step: 17756 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5151 - 8564.0 sample/sec\n",
            "Global step: 17766 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5308 - 8760.8 sample/sec\n",
            "Global step: 17776 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5389 - 8428.1 sample/sec\n",
            "Global step: 17786 - [==============>---------------]  49% - acc: 0.9219 - loss: 1.5419 - 8659.8 sample/sec\n",
            "Global step: 17796 - [==============>---------------]  51% - acc: 0.9062 - loss: 1.5546 - 8723.7 sample/sec\n",
            "Global step: 17806 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5154 - 8886.1 sample/sec\n",
            "Global step: 17816 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5078 - 8841.5 sample/sec\n",
            "Global step: 17826 - [=================>------------]  59% - acc: 0.9609 - loss: 1.5000 - 8611.6 sample/sec\n",
            "Global step: 17836 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5781 - 8301.7 sample/sec\n",
            "Global step: 17846 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5697 - 8726.8 sample/sec\n",
            "Global step: 17856 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5541 - 8708.7 sample/sec\n",
            "Global step: 17866 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5156 - 8687.2 sample/sec\n",
            "Global step: 17876 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8687.1 sample/sec\n",
            "Global step: 17886 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5077 - 8567.9 sample/sec\n",
            "Global step: 17896 - [======================>-------]  77% - acc: 0.9609 - loss: 1.5000 - 8719.3 sample/sec\n",
            "Global step: 17906 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5613 - 8501.4 sample/sec\n",
            "Global step: 17916 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5468 - 8909.1 sample/sec\n",
            "Global step: 17926 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8673.3 sample/sec\n",
            "Global step: 17936 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4920 - 8522.4 sample/sec\n",
            "Global step: 17946 - [==========================>---]  90% - acc: 0.9219 - loss: 1.5394 - 8834.6 sample/sec\n",
            "Global step: 17956 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5546 - 8629.8 sample/sec\n",
            "Global step: 17966 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5468 - 8711.9 sample/sec\n",
            "Global step: 17976 - [============================>-]  97% - acc: 0.9531 - loss: 1.5074 - 8600.0 sample/sec\n",
            "Global step: 17986 - [=============================>] 100% - acc: 0.9125 - loss: 1.5472 - 12713.9 sample/sec\n",
            "\n",
            "Epoch 46 - accuracy: 79.15% (7915/10000)\n",
            "This epoch receive better accuracy: 79.15 > 79.11. Saving session...\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 47/60\n",
            "\n",
            "Global step: 17987 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5156 - 9094.0 sample/sec\n",
            "Global step: 17997 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5229 - 8790.4 sample/sec\n",
            "Global step: 18007 - [=>----------------------------]   5% - acc: 0.9688 - loss: 1.4927 - 8678.9 sample/sec\n",
            "Global step: 18017 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4921 - 8612.5 sample/sec\n",
            "Global step: 18027 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5307 - 8696.7 sample/sec\n",
            "Global step: 18037 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5312 - 8787.5 sample/sec\n",
            "Global step: 18047 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5074 - 8564.9 sample/sec\n",
            "Global step: 18057 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5544 - 8818.8 sample/sec\n",
            "Global step: 18067 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.5000 - 8617.0 sample/sec\n",
            "Global step: 18077 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5077 - 8818.7 sample/sec\n",
            "Global step: 18087 - [=======>----------------------]  26% - acc: 0.9844 - loss: 1.4766 - 8685.4 sample/sec\n",
            "Global step: 18097 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8714.2 sample/sec\n",
            "Global step: 18107 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5230 - 8814.6 sample/sec\n",
            "Global step: 18117 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5079 - 8666.1 sample/sec\n",
            "Global step: 18127 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5303 - 8726.9 sample/sec\n",
            "Global step: 18137 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5235 - 8622.8 sample/sec\n",
            "Global step: 18147 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5150 - 9137.9 sample/sec\n",
            "Global step: 18157 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5307 - 8746.8 sample/sec\n",
            "Global step: 18167 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5389 - 8754.8 sample/sec\n",
            "Global step: 18177 - [==============>---------------]  49% - acc: 0.9219 - loss: 1.5399 - 8638.0 sample/sec\n",
            "Global step: 18187 - [==============>---------------]  51% - acc: 0.9062 - loss: 1.5546 - 8741.0 sample/sec\n",
            "Global step: 18197 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5153 - 8574.2 sample/sec\n",
            "Global step: 18207 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5078 - 8340.8 sample/sec\n",
            "Global step: 18217 - [=================>------------]  59% - acc: 0.9609 - loss: 1.5000 - 8768.7 sample/sec\n",
            "Global step: 18227 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5780 - 8841.2 sample/sec\n",
            "Global step: 18237 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5697 - 8874.9 sample/sec\n",
            "Global step: 18247 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5541 - 7371.3 sample/sec\n",
            "Global step: 18257 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5156 - 8568.7 sample/sec\n",
            "Global step: 18267 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8497.2 sample/sec\n",
            "Global step: 18277 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5077 - 8636.8 sample/sec\n",
            "Global step: 18287 - [======================>-------]  77% - acc: 0.9609 - loss: 1.4999 - 8714.6 sample/sec\n",
            "Global step: 18297 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5611 - 8740.4 sample/sec\n",
            "Global step: 18307 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5468 - 8674.0 sample/sec\n",
            "Global step: 18317 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8724.1 sample/sec\n",
            "Global step: 18327 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4920 - 8604.7 sample/sec\n",
            "Global step: 18337 - [==========================>---]  90% - acc: 0.9219 - loss: 1.5387 - 8467.3 sample/sec\n",
            "Global step: 18347 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5546 - 7849.6 sample/sec\n",
            "Global step: 18357 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5468 - 8600.3 sample/sec\n",
            "Global step: 18367 - [============================>-]  97% - acc: 0.9531 - loss: 1.5074 - 8694.4 sample/sec\n",
            "Global step: 18377 - [=============================>] 100% - acc: 0.9125 - loss: 1.5471 - 12733.2 sample/sec\n",
            "\n",
            "Epoch 47 - accuracy: 79.15% (7915/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 48/60\n",
            "\n",
            "Global step: 18378 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5156 - 8859.8 sample/sec\n",
            "Global step: 18388 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5229 - 8658.5 sample/sec\n",
            "Global step: 18398 - [=>----------------------------]   5% - acc: 0.9688 - loss: 1.4926 - 8752.0 sample/sec\n",
            "Global step: 18408 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4921 - 8661.3 sample/sec\n",
            "Global step: 18418 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5305 - 8791.8 sample/sec\n",
            "Global step: 18428 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5312 - 8583.2 sample/sec\n",
            "Global step: 18438 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5075 - 8558.6 sample/sec\n",
            "Global step: 18448 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5543 - 8459.7 sample/sec\n",
            "Global step: 18458 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.5000 - 8024.6 sample/sec\n",
            "Global step: 18468 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5077 - 8657.5 sample/sec\n",
            "Global step: 18478 - [=======>----------------------]  26% - acc: 0.9844 - loss: 1.4765 - 8802.6 sample/sec\n",
            "Global step: 18488 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8496.1 sample/sec\n",
            "Global step: 18498 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5230 - 8904.2 sample/sec\n",
            "Global step: 18508 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5078 - 8877.9 sample/sec\n",
            "Global step: 18518 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5303 - 8638.4 sample/sec\n",
            "Global step: 18528 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5234 - 8947.8 sample/sec\n",
            "Global step: 18538 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5150 - 8703.9 sample/sec\n",
            "Global step: 18548 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5306 - 8583.3 sample/sec\n",
            "Global step: 18558 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5388 - 8855.9 sample/sec\n",
            "Global step: 18568 - [==============>---------------]  49% - acc: 0.9219 - loss: 1.5389 - 8441.4 sample/sec\n",
            "Global step: 18578 - [==============>---------------]  51% - acc: 0.9062 - loss: 1.5546 - 8700.2 sample/sec\n",
            "Global step: 18588 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5153 - 8715.2 sample/sec\n",
            "Global step: 18598 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5078 - 8726.1 sample/sec\n",
            "Global step: 18608 - [=================>------------]  59% - acc: 0.9609 - loss: 1.4999 - 8973.3 sample/sec\n",
            "Global step: 18618 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5780 - 8925.5 sample/sec\n",
            "Global step: 18628 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5696 - 8620.7 sample/sec\n",
            "Global step: 18638 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5541 - 8456.3 sample/sec\n",
            "Global step: 18648 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5155 - 8782.7 sample/sec\n",
            "Global step: 18658 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8696.9 sample/sec\n",
            "Global step: 18668 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5077 - 8803.3 sample/sec\n",
            "Global step: 18678 - [======================>-------]  77% - acc: 0.9609 - loss: 1.4999 - 8873.9 sample/sec\n",
            "Global step: 18688 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5609 - 8687.0 sample/sec\n",
            "Global step: 18698 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5468 - 8704.4 sample/sec\n",
            "Global step: 18708 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8588.0 sample/sec\n",
            "Global step: 18718 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4919 - 8645.5 sample/sec\n",
            "Global step: 18728 - [==========================>---]  90% - acc: 0.9219 - loss: 1.5385 - 8350.2 sample/sec\n",
            "Global step: 18738 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5546 - 8356.2 sample/sec\n",
            "Global step: 18748 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5467 - 8888.6 sample/sec\n",
            "Global step: 18758 - [============================>-]  97% - acc: 0.9531 - loss: 1.5073 - 8362.1 sample/sec\n",
            "Global step: 18768 - [=============================>] 100% - acc: 0.9125 - loss: 1.5470 - 12309.9 sample/sec\n",
            "\n",
            "Epoch 48 - accuracy: 79.09% (7909/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 49/60\n",
            "\n",
            "Global step: 18769 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5155 - 8747.1 sample/sec\n",
            "Global step: 18779 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5228 - 8748.7 sample/sec\n",
            "Global step: 18789 - [=>----------------------------]   5% - acc: 0.9688 - loss: 1.4926 - 8759.8 sample/sec\n",
            "Global step: 18799 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4921 - 8654.5 sample/sec\n",
            "Global step: 18809 - [==>---------------------------]  10% - acc: 0.9297 - loss: 1.5291 - 8892.9 sample/sec\n",
            "Global step: 18819 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5312 - 8788.6 sample/sec\n",
            "Global step: 18829 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5074 - 8697.1 sample/sec\n",
            "Global step: 18839 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5543 - 8656.0 sample/sec\n",
            "Global step: 18849 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.4999 - 8844.8 sample/sec\n",
            "Global step: 18859 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5076 - 8962.8 sample/sec\n",
            "Global step: 18869 - [=======>----------------------]  26% - acc: 0.9844 - loss: 1.4764 - 8414.0 sample/sec\n",
            "Global step: 18879 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8929.7 sample/sec\n",
            "Global step: 18889 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5230 - 8741.4 sample/sec\n",
            "Global step: 18899 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5078 - 8701.3 sample/sec\n",
            "Global step: 18909 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5303 - 8625.8 sample/sec\n",
            "Global step: 18919 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5234 - 8907.0 sample/sec\n",
            "Global step: 18929 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5150 - 8809.0 sample/sec\n",
            "Global step: 18939 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5305 - 8373.6 sample/sec\n",
            "Global step: 18949 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5388 - 8550.5 sample/sec\n",
            "Global step: 18959 - [==============>---------------]  49% - acc: 0.9219 - loss: 1.5384 - 8573.2 sample/sec\n",
            "Global step: 18969 - [==============>---------------]  51% - acc: 0.9062 - loss: 1.5546 - 8521.2 sample/sec\n",
            "Global step: 18979 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5153 - 8728.2 sample/sec\n",
            "Global step: 18989 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5078 - 8517.3 sample/sec\n",
            "Global step: 18999 - [=================>------------]  59% - acc: 0.9609 - loss: 1.4999 - 8591.2 sample/sec\n",
            "Global step: 19009 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5780 - 8477.5 sample/sec\n",
            "Global step: 19019 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5696 - 8736.0 sample/sec\n",
            "Global step: 19029 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5541 - 8698.1 sample/sec\n",
            "Global step: 19039 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5155 - 8704.6 sample/sec\n",
            "Global step: 19049 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8770.3 sample/sec\n",
            "Global step: 19059 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5077 - 8906.3 sample/sec\n",
            "Global step: 19069 - [======================>-------]  77% - acc: 0.9609 - loss: 1.4999 - 8830.5 sample/sec\n",
            "Global step: 19079 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5610 - 8559.4 sample/sec\n",
            "Global step: 19089 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5468 - 8828.5 sample/sec\n",
            "Global step: 19099 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8832.6 sample/sec\n",
            "Global step: 19109 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4919 - 8518.8 sample/sec\n",
            "Global step: 19119 - [==========================>---]  90% - acc: 0.9219 - loss: 1.5384 - 8739.0 sample/sec\n",
            "Global step: 19129 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5546 - 8426.4 sample/sec\n",
            "Global step: 19139 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5468 - 8539.9 sample/sec\n",
            "Global step: 19149 - [============================>-]  97% - acc: 0.9531 - loss: 1.5073 - 8818.5 sample/sec\n",
            "Global step: 19159 - [=============================>] 100% - acc: 0.9125 - loss: 1.5469 - 13166.3 sample/sec\n",
            "\n",
            "Epoch 49 - accuracy: 79.03% (7903/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 50/60\n",
            "\n",
            "Global step: 19160 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5155 - 8786.9 sample/sec\n",
            "Global step: 19170 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5228 - 8798.4 sample/sec\n",
            "Global step: 19180 - [=>----------------------------]   5% - acc: 0.9688 - loss: 1.4925 - 8787.3 sample/sec\n",
            "Global step: 19190 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4921 - 8577.4 sample/sec\n",
            "Global step: 19200 - [==>---------------------------]  10% - acc: 0.9375 - loss: 1.5228 - 8601.2 sample/sec\n",
            "Global step: 19210 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5312 - 8461.3 sample/sec\n",
            "Global step: 19220 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5075 - 8593.9 sample/sec\n",
            "Global step: 19230 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5544 - 8792.7 sample/sec\n",
            "Global step: 19240 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.4999 - 8894.3 sample/sec\n",
            "Global step: 19250 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5076 - 8852.7 sample/sec\n",
            "Global step: 19260 - [=======>----------------------]  26% - acc: 0.9844 - loss: 1.4765 - 8614.1 sample/sec\n",
            "Global step: 19270 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 7996.8 sample/sec\n",
            "Global step: 19280 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5230 - 8628.0 sample/sec\n",
            "Global step: 19290 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5078 - 8408.7 sample/sec\n",
            "Global step: 19300 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5303 - 8805.2 sample/sec\n",
            "Global step: 19310 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5234 - 8907.9 sample/sec\n",
            "Global step: 19320 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5149 - 8651.7 sample/sec\n",
            "Global step: 19330 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5305 - 8628.0 sample/sec\n",
            "Global step: 19340 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5388 - 8695.1 sample/sec\n",
            "Global step: 19350 - [==============>---------------]  49% - acc: 0.9219 - loss: 1.5383 - 8623.6 sample/sec\n",
            "Global step: 19360 - [==============>---------------]  51% - acc: 0.9062 - loss: 1.5545 - 8664.8 sample/sec\n",
            "Global step: 19370 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5153 - 7622.4 sample/sec\n",
            "Global step: 19380 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5079 - 8404.0 sample/sec\n",
            "Global step: 19390 - [=================>------------]  59% - acc: 0.9609 - loss: 1.4999 - 8414.9 sample/sec\n",
            "Global step: 19400 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5780 - 8741.3 sample/sec\n",
            "Global step: 19410 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5696 - 8672.5 sample/sec\n",
            "Global step: 19420 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5540 - 8710.3 sample/sec\n",
            "Global step: 19430 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5155 - 8902.6 sample/sec\n",
            "Global step: 19440 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8709.5 sample/sec\n",
            "Global step: 19450 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5076 - 8807.2 sample/sec\n",
            "Global step: 19460 - [======================>-------]  77% - acc: 0.9609 - loss: 1.4999 - 8809.5 sample/sec\n",
            "Global step: 19470 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5608 - 8864.1 sample/sec\n",
            "Global step: 19480 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5468 - 8512.3 sample/sec\n",
            "Global step: 19490 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8521.9 sample/sec\n",
            "Global step: 19500 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4918 - 8725.9 sample/sec\n",
            "Global step: 19510 - [==========================>---]  90% - acc: 0.9219 - loss: 1.5383 - 8235.6 sample/sec\n",
            "Global step: 19520 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5545 - 8782.9 sample/sec\n",
            "Global step: 19530 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5467 - 8469.3 sample/sec\n",
            "Global step: 19540 - [============================>-]  97% - acc: 0.9531 - loss: 1.5072 - 9005.0 sample/sec\n",
            "Global step: 19550 - [=============================>] 100% - acc: 0.9125 - loss: 1.5469 - 12681.2 sample/sec\n",
            "\n",
            "Epoch 50 - accuracy: 79.06% (7906/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 51/60\n",
            "\n",
            "Global step: 19551 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5155 - 7594.5 sample/sec\n",
            "Global step: 19561 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5228 - 8676.1 sample/sec\n",
            "Global step: 19571 - [=>----------------------------]   5% - acc: 0.9688 - loss: 1.4923 - 8593.4 sample/sec\n",
            "Global step: 19581 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4920 - 8855.0 sample/sec\n",
            "Global step: 19591 - [==>---------------------------]  10% - acc: 0.9375 - loss: 1.5223 - 8551.9 sample/sec\n",
            "Global step: 19601 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5312 - 8105.4 sample/sec\n",
            "Global step: 19611 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5075 - 8785.6 sample/sec\n",
            "Global step: 19621 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5543 - 8750.4 sample/sec\n",
            "Global step: 19631 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.4999 - 8763.4 sample/sec\n",
            "Global step: 19641 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5076 - 8577.9 sample/sec\n",
            "Global step: 19651 - [=======>----------------------]  26% - acc: 0.9844 - loss: 1.4764 - 8773.4 sample/sec\n",
            "Global step: 19661 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8125.2 sample/sec\n",
            "Global step: 19671 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5230 - 8688.9 sample/sec\n",
            "Global step: 19681 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5078 - 8375.7 sample/sec\n",
            "Global step: 19691 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5303 - 8855.3 sample/sec\n",
            "Global step: 19701 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5234 - 8783.2 sample/sec\n",
            "Global step: 19711 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5149 - 8781.2 sample/sec\n",
            "Global step: 19721 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5305 - 8740.6 sample/sec\n",
            "Global step: 19731 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5387 - 8823.9 sample/sec\n",
            "Global step: 19741 - [==============>---------------]  49% - acc: 0.9219 - loss: 1.5381 - 8806.6 sample/sec\n",
            "Global step: 19751 - [==============>---------------]  51% - acc: 0.9062 - loss: 1.5545 - 8735.4 sample/sec\n",
            "Global step: 19761 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5153 - 9039.4 sample/sec\n",
            "Global step: 19771 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5078 - 8802.2 sample/sec\n",
            "Global step: 19781 - [=================>------------]  59% - acc: 0.9609 - loss: 1.4999 - 8759.8 sample/sec\n",
            "Global step: 19791 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5780 - 8442.3 sample/sec\n",
            "Global step: 19801 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5696 - 8805.1 sample/sec\n",
            "Global step: 19811 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5540 - 8758.5 sample/sec\n",
            "Global step: 19821 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5155 - 8703.6 sample/sec\n",
            "Global step: 19831 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8694.0 sample/sec\n",
            "Global step: 19841 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5076 - 8773.5 sample/sec\n",
            "Global step: 19851 - [======================>-------]  77% - acc: 0.9609 - loss: 1.4999 - 8396.2 sample/sec\n",
            "Global step: 19861 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5607 - 8383.9 sample/sec\n",
            "Global step: 19871 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5468 - 8661.9 sample/sec\n",
            "Global step: 19881 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8889.8 sample/sec\n",
            "Global step: 19891 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4918 - 8750.2 sample/sec\n",
            "Global step: 19901 - [==========================>---]  90% - acc: 0.9219 - loss: 1.5382 - 7919.9 sample/sec\n",
            "Global step: 19911 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5545 - 8802.0 sample/sec\n",
            "Global step: 19921 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5466 - 8517.2 sample/sec\n",
            "Global step: 19931 - [============================>-]  97% - acc: 0.9531 - loss: 1.5072 - 8793.5 sample/sec\n",
            "Global step: 19941 - [=============================>] 100% - acc: 0.9125 - loss: 1.5469 - 12893.5 sample/sec\n",
            "\n",
            "Epoch 51 - accuracy: 79.03% (7903/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 52/60\n",
            "\n",
            "Global step: 19942 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5154 - 9063.7 sample/sec\n",
            "Global step: 19952 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5228 - 8554.2 sample/sec\n",
            "Global step: 19962 - [=>----------------------------]   5% - acc: 0.9688 - loss: 1.4922 - 8790.5 sample/sec\n",
            "Global step: 19972 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4919 - 8654.0 sample/sec\n",
            "Global step: 19982 - [==>---------------------------]  10% - acc: 0.9375 - loss: 1.5203 - 8760.4 sample/sec\n",
            "Global step: 19992 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5311 - 8708.4 sample/sec\n",
            "Global step: 20002 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5074 - 8425.1 sample/sec\n",
            "Global step: 20012 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5543 - 9094.9 sample/sec\n",
            "Global step: 20022 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.4999 - 8767.0 sample/sec\n",
            "Global step: 20032 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5075 - 8802.9 sample/sec\n",
            "Global step: 20042 - [=======>----------------------]  26% - acc: 0.9844 - loss: 1.4760 - 8748.7 sample/sec\n",
            "Global step: 20052 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8831.0 sample/sec\n",
            "Global step: 20062 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5230 - 8644.6 sample/sec\n",
            "Global step: 20072 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5077 - 8472.8 sample/sec\n",
            "Global step: 20082 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5303 - 8961.7 sample/sec\n",
            "Global step: 20092 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5234 - 8882.4 sample/sec\n",
            "Global step: 20102 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5149 - 8794.2 sample/sec\n",
            "Global step: 20112 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5305 - 8737.7 sample/sec\n",
            "Global step: 20122 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5387 - 8701.5 sample/sec\n",
            "Global step: 20132 - [==============>---------------]  49% - acc: 0.9219 - loss: 1.5381 - 8658.6 sample/sec\n",
            "Global step: 20142 - [==============>---------------]  51% - acc: 0.9062 - loss: 1.5544 - 8500.4 sample/sec\n",
            "Global step: 20152 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5153 - 8739.6 sample/sec\n",
            "Global step: 20162 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5078 - 8674.7 sample/sec\n",
            "Global step: 20172 - [=================>------------]  59% - acc: 0.9609 - loss: 1.4999 - 8810.7 sample/sec\n",
            "Global step: 20182 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5780 - 8770.1 sample/sec\n",
            "Global step: 20192 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5696 - 8860.1 sample/sec\n",
            "Global step: 20202 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5540 - 8841.2 sample/sec\n",
            "Global step: 20212 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5154 - 8656.6 sample/sec\n",
            "Global step: 20222 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8835.3 sample/sec\n",
            "Global step: 20232 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5076 - 8887.6 sample/sec\n",
            "Global step: 20242 - [======================>-------]  77% - acc: 0.9609 - loss: 1.4999 - 8975.7 sample/sec\n",
            "Global step: 20252 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5607 - 8644.2 sample/sec\n",
            "Global step: 20262 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5467 - 8568.8 sample/sec\n",
            "Global step: 20272 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8550.3 sample/sec\n",
            "Global step: 20282 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4918 - 8912.8 sample/sec\n",
            "Global step: 20292 - [==========================>---]  90% - acc: 0.9219 - loss: 1.5381 - 8910.6 sample/sec\n",
            "Global step: 20302 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5545 - 8781.0 sample/sec\n",
            "Global step: 20312 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5466 - 8932.1 sample/sec\n",
            "Global step: 20322 - [============================>-]  97% - acc: 0.9531 - loss: 1.5072 - 8610.2 sample/sec\n",
            "Global step: 20332 - [=============================>] 100% - acc: 0.9125 - loss: 1.5469 - 12473.8 sample/sec\n",
            "\n",
            "Epoch 52 - accuracy: 79.05% (7905/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 53/60\n",
            "\n",
            "Global step: 20333 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5153 - 8868.6 sample/sec\n",
            "Global step: 20343 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5228 - 8733.7 sample/sec\n",
            "Global step: 20353 - [=>----------------------------]   5% - acc: 0.9688 - loss: 1.4923 - 8629.6 sample/sec\n",
            "Global step: 20363 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4919 - 8863.2 sample/sec\n",
            "Global step: 20373 - [==>---------------------------]  10% - acc: 0.9453 - loss: 1.5160 - 8572.4 sample/sec\n",
            "Global step: 20383 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5311 - 9036.7 sample/sec\n",
            "Global step: 20393 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5074 - 8346.6 sample/sec\n",
            "Global step: 20403 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5543 - 9054.4 sample/sec\n",
            "Global step: 20413 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.4999 - 7909.1 sample/sec\n",
            "Global step: 20423 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5075 - 8674.9 sample/sec\n",
            "Global step: 20433 - [=======>----------------------]  26% - acc: 0.9844 - loss: 1.4746 - 8532.5 sample/sec\n",
            "Global step: 20443 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8830.1 sample/sec\n",
            "Global step: 20453 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5229 - 7749.3 sample/sec\n",
            "Global step: 20463 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5077 - 8536.9 sample/sec\n",
            "Global step: 20473 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5303 - 8535.4 sample/sec\n",
            "Global step: 20483 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5234 - 8857.9 sample/sec\n",
            "Global step: 20493 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5149 - 8691.3 sample/sec\n",
            "Global step: 20503 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5306 - 8708.5 sample/sec\n",
            "Global step: 20513 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5387 - 8661.0 sample/sec\n",
            "Global step: 20523 - [==============>---------------]  49% - acc: 0.9219 - loss: 1.5380 - 8835.2 sample/sec\n",
            "Global step: 20533 - [==============>---------------]  51% - acc: 0.9062 - loss: 1.5544 - 8770.7 sample/sec\n",
            "Global step: 20543 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5153 - 8717.6 sample/sec\n",
            "Global step: 20553 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5078 - 8641.4 sample/sec\n",
            "Global step: 20563 - [=================>------------]  59% - acc: 0.9609 - loss: 1.4999 - 8697.8 sample/sec\n",
            "Global step: 20573 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5780 - 8750.5 sample/sec\n",
            "Global step: 20583 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5696 - 8932.7 sample/sec\n",
            "Global step: 20593 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5540 - 8729.3 sample/sec\n",
            "Global step: 20603 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5153 - 8553.7 sample/sec\n",
            "Global step: 20613 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8961.0 sample/sec\n",
            "Global step: 20623 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5075 - 8755.4 sample/sec\n",
            "Global step: 20633 - [======================>-------]  77% - acc: 0.9609 - loss: 1.4999 - 8756.7 sample/sec\n",
            "Global step: 20643 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5607 - 7796.0 sample/sec\n",
            "Global step: 20653 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5467 - 8737.3 sample/sec\n",
            "Global step: 20663 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8495.3 sample/sec\n",
            "Global step: 20673 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4918 - 8701.6 sample/sec\n",
            "Global step: 20683 - [==========================>---]  90% - acc: 0.9219 - loss: 1.5380 - 8914.7 sample/sec\n",
            "Global step: 20693 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5545 - 8559.5 sample/sec\n",
            "Global step: 20703 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5466 - 8749.2 sample/sec\n",
            "Global step: 20713 - [============================>-]  97% - acc: 0.9531 - loss: 1.5072 - 8816.6 sample/sec\n",
            "Global step: 20723 - [=============================>] 100% - acc: 0.9125 - loss: 1.5469 - 12483.9 sample/sec\n",
            "\n",
            "Epoch 53 - accuracy: 78.96% (7896/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 54/60\n",
            "\n",
            "Global step: 20724 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5153 - 9024.4 sample/sec\n",
            "Global step: 20734 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5228 - 8894.9 sample/sec\n",
            "Global step: 20744 - [=>----------------------------]   5% - acc: 0.9688 - loss: 1.4922 - 8330.6 sample/sec\n",
            "Global step: 20754 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4918 - 8745.1 sample/sec\n",
            "Global step: 20764 - [==>---------------------------]  10% - acc: 0.9453 - loss: 1.5156 - 8653.8 sample/sec\n",
            "Global step: 20774 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5312 - 8514.5 sample/sec\n",
            "Global step: 20784 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5075 - 8922.4 sample/sec\n",
            "Global step: 20794 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5543 - 8935.9 sample/sec\n",
            "Global step: 20804 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.4999 - 8717.4 sample/sec\n",
            "Global step: 20814 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5076 - 8734.3 sample/sec\n",
            "Global step: 20824 - [=======>----------------------]  26% - acc: 0.9922 - loss: 1.4693 - 8819.7 sample/sec\n",
            "Global step: 20834 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8693.6 sample/sec\n",
            "Global step: 20844 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5228 - 8597.9 sample/sec\n",
            "Global step: 20854 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5077 - 8638.3 sample/sec\n",
            "Global step: 20864 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5303 - 8840.0 sample/sec\n",
            "Global step: 20874 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5234 - 8827.1 sample/sec\n",
            "Global step: 20884 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5149 - 8750.0 sample/sec\n",
            "Global step: 20894 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5305 - 8682.2 sample/sec\n",
            "Global step: 20904 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5387 - 8820.7 sample/sec\n",
            "Global step: 20914 - [==============>---------------]  49% - acc: 0.9219 - loss: 1.5379 - 8676.4 sample/sec\n",
            "Global step: 20924 - [==============>---------------]  51% - acc: 0.9062 - loss: 1.5543 - 8665.6 sample/sec\n",
            "Global step: 20934 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5153 - 8831.6 sample/sec\n",
            "Global step: 20944 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5078 - 8747.8 sample/sec\n",
            "Global step: 20954 - [=================>------------]  59% - acc: 0.9609 - loss: 1.4999 - 8786.3 sample/sec\n",
            "Global step: 20964 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5779 - 8783.5 sample/sec\n",
            "Global step: 20974 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5696 - 8756.9 sample/sec\n",
            "Global step: 20984 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5540 - 8880.2 sample/sec\n",
            "Global step: 20994 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5154 - 8678.9 sample/sec\n",
            "Global step: 21004 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8529.3 sample/sec\n",
            "Global step: 21014 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5075 - 8553.8 sample/sec\n",
            "Global step: 21024 - [======================>-------]  77% - acc: 0.9609 - loss: 1.4999 - 8752.8 sample/sec\n",
            "Global step: 21034 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5608 - 8612.0 sample/sec\n",
            "Global step: 21044 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5467 - 8704.4 sample/sec\n",
            "Global step: 21054 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8661.6 sample/sec\n",
            "Global step: 21064 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4918 - 8662.3 sample/sec\n",
            "Global step: 21074 - [==========================>---]  90% - acc: 0.9219 - loss: 1.5380 - 8591.7 sample/sec\n",
            "Global step: 21084 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5544 - 8760.2 sample/sec\n",
            "Global step: 21094 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5465 - 8937.9 sample/sec\n",
            "Global step: 21104 - [============================>-]  97% - acc: 0.9531 - loss: 1.5072 - 8486.2 sample/sec\n",
            "Global step: 21114 - [=============================>] 100% - acc: 0.9125 - loss: 1.5469 - 12708.5 sample/sec\n",
            "\n",
            "Epoch 54 - accuracy: 78.91% (7891/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 55/60\n",
            "\n",
            "Global step: 21115 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5153 - 8683.4 sample/sec\n",
            "Global step: 21125 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5228 - 8611.0 sample/sec\n",
            "Global step: 21135 - [=>----------------------------]   5% - acc: 0.9688 - loss: 1.4922 - 8857.5 sample/sec\n",
            "Global step: 21145 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4918 - 8691.6 sample/sec\n",
            "Global step: 21155 - [==>---------------------------]  10% - acc: 0.9453 - loss: 1.5155 - 8765.0 sample/sec\n",
            "Global step: 21165 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5312 - 8703.7 sample/sec\n",
            "Global step: 21175 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5071 - 7883.0 sample/sec\n",
            "Global step: 21185 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5543 - 8658.2 sample/sec\n",
            "Global step: 21195 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.4999 - 8584.4 sample/sec\n",
            "Global step: 21205 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5075 - 8631.5 sample/sec\n",
            "Global step: 21215 - [=======>----------------------]  26% - acc: 0.9922 - loss: 1.4688 - 8572.1 sample/sec\n",
            "Global step: 21225 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8735.1 sample/sec\n",
            "Global step: 21235 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5228 - 8516.9 sample/sec\n",
            "Global step: 21245 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5078 - 8605.9 sample/sec\n",
            "Global step: 21255 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5303 - 8574.4 sample/sec\n",
            "Global step: 21265 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5234 - 8719.1 sample/sec\n",
            "Global step: 21275 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5149 - 8536.0 sample/sec\n",
            "Global step: 21285 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5305 - 8682.3 sample/sec\n",
            "Global step: 21295 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5387 - 8814.2 sample/sec\n",
            "Global step: 21305 - [==============>---------------]  49% - acc: 0.9219 - loss: 1.5379 - 8577.0 sample/sec\n",
            "Global step: 21315 - [==============>---------------]  51% - acc: 0.9062 - loss: 1.5541 - 8743.1 sample/sec\n",
            "Global step: 21325 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5153 - 8711.3 sample/sec\n",
            "Global step: 21335 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5078 - 8853.9 sample/sec\n",
            "Global step: 21345 - [=================>------------]  59% - acc: 0.9609 - loss: 1.4999 - 8745.0 sample/sec\n",
            "Global step: 21355 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5779 - 8612.4 sample/sec\n",
            "Global step: 21365 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5696 - 8713.9 sample/sec\n",
            "Global step: 21375 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5540 - 8427.9 sample/sec\n",
            "Global step: 21385 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5154 - 8826.5 sample/sec\n",
            "Global step: 21395 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8632.2 sample/sec\n",
            "Global step: 21405 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5076 - 8690.3 sample/sec\n",
            "Global step: 21415 - [======================>-------]  77% - acc: 0.9609 - loss: 1.4999 - 8484.7 sample/sec\n",
            "Global step: 21425 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5608 - 8656.4 sample/sec\n",
            "Global step: 21435 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5468 - 8485.8 sample/sec\n",
            "Global step: 21445 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8700.5 sample/sec\n",
            "Global step: 21455 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4918 - 9187.2 sample/sec\n",
            "Global step: 21465 - [==========================>---]  90% - acc: 0.9219 - loss: 1.5380 - 8535.0 sample/sec\n",
            "Global step: 21475 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5545 - 8488.2 sample/sec\n",
            "Global step: 21485 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5466 - 8403.4 sample/sec\n",
            "Global step: 21495 - [============================>-]  97% - acc: 0.9531 - loss: 1.5073 - 8585.5 sample/sec\n",
            "Global step: 21505 - [=============================>] 100% - acc: 0.9125 - loss: 1.5469 - 12317.2 sample/sec\n",
            "\n",
            "Epoch 55 - accuracy: 78.86% (7886/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 56/60\n",
            "\n",
            "Global step: 21506 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5154 - 8747.5 sample/sec\n",
            "Global step: 21516 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5229 - 8646.4 sample/sec\n",
            "Global step: 21526 - [=>----------------------------]   5% - acc: 0.9688 - loss: 1.4923 - 8717.3 sample/sec\n",
            "Global step: 21536 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4918 - 8687.5 sample/sec\n",
            "Global step: 21546 - [==>---------------------------]  10% - acc: 0.9453 - loss: 1.5155 - 8203.8 sample/sec\n",
            "Global step: 21556 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5312 - 8588.6 sample/sec\n",
            "Global step: 21566 - [====>-------------------------]  15% - acc: 0.9531 - loss: 1.5052 - 8450.0 sample/sec\n",
            "Global step: 21576 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5544 - 8844.2 sample/sec\n",
            "Global step: 21586 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.4999 - 8628.5 sample/sec\n",
            "Global step: 21596 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5075 - 8639.3 sample/sec\n",
            "Global step: 21606 - [=======>----------------------]  26% - acc: 0.9922 - loss: 1.4689 - 8879.3 sample/sec\n",
            "Global step: 21616 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8601.5 sample/sec\n",
            "Global step: 21626 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5227 - 8894.8 sample/sec\n",
            "Global step: 21636 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5077 - 8737.7 sample/sec\n",
            "Global step: 21646 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5303 - 8709.4 sample/sec\n",
            "Global step: 21656 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5234 - 8955.2 sample/sec\n",
            "Global step: 21666 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5149 - 8856.5 sample/sec\n",
            "Global step: 21676 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5306 - 8383.5 sample/sec\n",
            "Global step: 21686 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5388 - 8517.2 sample/sec\n",
            "Global step: 21696 - [==============>---------------]  49% - acc: 0.9219 - loss: 1.5379 - 8618.8 sample/sec\n",
            "Global step: 21706 - [==============>---------------]  51% - acc: 0.9062 - loss: 1.5531 - 8822.4 sample/sec\n",
            "Global step: 21716 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5153 - 8843.9 sample/sec\n",
            "Global step: 21726 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5078 - 8891.4 sample/sec\n",
            "Global step: 21736 - [=================>------------]  59% - acc: 0.9609 - loss: 1.4999 - 8857.8 sample/sec\n",
            "Global step: 21746 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5779 - 8772.3 sample/sec\n",
            "Global step: 21756 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5696 - 8570.9 sample/sec\n",
            "Global step: 21766 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5540 - 8637.1 sample/sec\n",
            "Global step: 21776 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5153 - 8723.5 sample/sec\n",
            "Global step: 21786 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8700.0 sample/sec\n",
            "Global step: 21796 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5075 - 8726.6 sample/sec\n",
            "Global step: 21806 - [======================>-------]  77% - acc: 0.9609 - loss: 1.4999 - 8785.5 sample/sec\n",
            "Global step: 21816 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5607 - 8671.9 sample/sec\n",
            "Global step: 21826 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5467 - 8522.3 sample/sec\n",
            "Global step: 21836 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8777.3 sample/sec\n",
            "Global step: 21846 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4918 - 8689.3 sample/sec\n",
            "Global step: 21856 - [==========================>---]  90% - acc: 0.9219 - loss: 1.5380 - 8517.0 sample/sec\n",
            "Global step: 21866 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5544 - 8937.7 sample/sec\n",
            "Global step: 21876 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5465 - 8572.8 sample/sec\n",
            "Global step: 21886 - [============================>-]  97% - acc: 0.9531 - loss: 1.5072 - 8856.3 sample/sec\n",
            "Global step: 21896 - [=============================>] 100% - acc: 0.9125 - loss: 1.5469 - 12306.5 sample/sec\n",
            "\n",
            "Epoch 56 - accuracy: 78.91% (7891/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 57/60\n",
            "\n",
            "Global step: 21897 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5153 - 8916.3 sample/sec\n",
            "Global step: 21907 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5228 - 8689.8 sample/sec\n",
            "Global step: 21917 - [=>----------------------------]   5% - acc: 0.9688 - loss: 1.4923 - 8898.5 sample/sec\n",
            "Global step: 21927 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4919 - 8918.0 sample/sec\n",
            "Global step: 21937 - [==>---------------------------]  10% - acc: 0.9453 - loss: 1.5154 - 8565.1 sample/sec\n",
            "Global step: 21947 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5311 - 8607.3 sample/sec\n",
            "Global step: 21957 - [====>-------------------------]  15% - acc: 0.9609 - loss: 1.4999 - 8646.8 sample/sec\n",
            "Global step: 21967 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5543 - 8824.3 sample/sec\n",
            "Global step: 21977 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.4999 - 8809.4 sample/sec\n",
            "Global step: 21987 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5075 - 8831.9 sample/sec\n",
            "Global step: 21997 - [=======>----------------------]  26% - acc: 0.9922 - loss: 1.4688 - 8236.6 sample/sec\n",
            "Global step: 22007 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8728.8 sample/sec\n",
            "Global step: 22017 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5227 - 7141.2 sample/sec\n",
            "Global step: 22027 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5077 - 8886.8 sample/sec\n",
            "Global step: 22037 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5303 - 8616.4 sample/sec\n",
            "Global step: 22047 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5235 - 8888.1 sample/sec\n",
            "Global step: 22057 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5148 - 8675.2 sample/sec\n",
            "Global step: 22067 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5305 - 8815.5 sample/sec\n",
            "Global step: 22077 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5387 - 8850.1 sample/sec\n",
            "Global step: 22087 - [==============>---------------]  49% - acc: 0.9219 - loss: 1.5379 - 7485.8 sample/sec\n",
            "Global step: 22097 - [==============>---------------]  51% - acc: 0.9141 - loss: 1.5477 - 8810.1 sample/sec\n",
            "Global step: 22107 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5153 - 8767.5 sample/sec\n",
            "Global step: 22117 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5078 - 8798.6 sample/sec\n",
            "Global step: 22127 - [=================>------------]  59% - acc: 0.9609 - loss: 1.4999 - 8804.8 sample/sec\n",
            "Global step: 22137 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5779 - 8745.3 sample/sec\n",
            "Global step: 22147 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5696 - 8696.2 sample/sec\n",
            "Global step: 22157 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5539 - 8855.5 sample/sec\n",
            "Global step: 22167 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5154 - 8697.6 sample/sec\n",
            "Global step: 22177 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8565.5 sample/sec\n",
            "Global step: 22187 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5075 - 8962.6 sample/sec\n",
            "Global step: 22197 - [======================>-------]  77% - acc: 0.9609 - loss: 1.4999 - 8548.5 sample/sec\n",
            "Global step: 22207 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5607 - 7959.5 sample/sec\n",
            "Global step: 22217 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5467 - 8690.0 sample/sec\n",
            "Global step: 22227 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8336.2 sample/sec\n",
            "Global step: 22237 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4918 - 8550.5 sample/sec\n",
            "Global step: 22247 - [==========================>---]  90% - acc: 0.9219 - loss: 1.5380 - 8153.6 sample/sec\n",
            "Global step: 22257 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5544 - 8865.7 sample/sec\n",
            "Global step: 22267 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5464 - 8672.4 sample/sec\n",
            "Global step: 22277 - [============================>-]  97% - acc: 0.9531 - loss: 1.5072 - 8626.4 sample/sec\n",
            "Global step: 22287 - [=============================>] 100% - acc: 0.9125 - loss: 1.5469 - 12572.5 sample/sec\n",
            "\n",
            "Epoch 57 - accuracy: 78.87% (7887/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 58/60\n",
            "\n",
            "Global step: 22288 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5153 - 8219.2 sample/sec\n",
            "Global step: 22298 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5228 - 8762.4 sample/sec\n",
            "Global step: 22308 - [=>----------------------------]   5% - acc: 0.9688 - loss: 1.4923 - 8466.0 sample/sec\n",
            "Global step: 22318 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4919 - 8871.0 sample/sec\n",
            "Global step: 22328 - [==>---------------------------]  10% - acc: 0.9453 - loss: 1.5154 - 8729.8 sample/sec\n",
            "Global step: 22338 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5311 - 8775.6 sample/sec\n",
            "Global step: 22348 - [====>-------------------------]  15% - acc: 0.9609 - loss: 1.4998 - 8620.3 sample/sec\n",
            "Global step: 22358 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5543 - 8784.2 sample/sec\n",
            "Global step: 22368 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.4999 - 8671.7 sample/sec\n",
            "Global step: 22378 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5075 - 8563.8 sample/sec\n",
            "Global step: 22388 - [=======>----------------------]  26% - acc: 0.9922 - loss: 1.4688 - 8598.5 sample/sec\n",
            "Global step: 22398 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8677.8 sample/sec\n",
            "Global step: 22408 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5227 - 8864.5 sample/sec\n",
            "Global step: 22418 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5077 - 8752.4 sample/sec\n",
            "Global step: 22428 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5303 - 8604.5 sample/sec\n",
            "Global step: 22438 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5234 - 8963.2 sample/sec\n",
            "Global step: 22448 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5148 - 8346.4 sample/sec\n",
            "Global step: 22458 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5305 - 8927.2 sample/sec\n",
            "Global step: 22468 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5387 - 8787.0 sample/sec\n",
            "Global step: 22478 - [==============>---------------]  49% - acc: 0.9219 - loss: 1.5379 - 8542.8 sample/sec\n",
            "Global step: 22488 - [==============>---------------]  51% - acc: 0.9141 - loss: 1.5469 - 8654.9 sample/sec\n",
            "Global step: 22498 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5152 - 8273.0 sample/sec\n",
            "Global step: 22508 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5077 - 8818.4 sample/sec\n",
            "Global step: 22518 - [=================>------------]  59% - acc: 0.9609 - loss: 1.4998 - 8660.7 sample/sec\n",
            "Global step: 22528 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5779 - 8662.8 sample/sec\n",
            "Global step: 22538 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5696 - 8856.5 sample/sec\n",
            "Global step: 22548 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5539 - 8745.4 sample/sec\n",
            "Global step: 22558 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5153 - 8509.7 sample/sec\n",
            "Global step: 22568 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8870.4 sample/sec\n",
            "Global step: 22578 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5075 - 8831.1 sample/sec\n",
            "Global step: 22588 - [======================>-------]  77% - acc: 0.9609 - loss: 1.4999 - 9036.4 sample/sec\n",
            "Global step: 22598 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5607 - 8461.7 sample/sec\n",
            "Global step: 22608 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5467 - 8744.5 sample/sec\n",
            "Global step: 22618 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8894.8 sample/sec\n",
            "Global step: 22628 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4918 - 8832.6 sample/sec\n",
            "Global step: 22638 - [==========================>---]  90% - acc: 0.9219 - loss: 1.5380 - 8678.4 sample/sec\n",
            "Global step: 22648 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5544 - 8771.3 sample/sec\n",
            "Global step: 22658 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5463 - 8833.0 sample/sec\n",
            "Global step: 22668 - [============================>-]  97% - acc: 0.9531 - loss: 1.5072 - 8577.2 sample/sec\n",
            "Global step: 22678 - [=============================>] 100% - acc: 0.9125 - loss: 1.5469 - 12154.7 sample/sec\n",
            "\n",
            "Epoch 58 - accuracy: 78.94% (7894/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 59/60\n",
            "\n",
            "Global step: 22679 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5153 - 8866.3 sample/sec\n",
            "Global step: 22689 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5228 - 8829.5 sample/sec\n",
            "Global step: 22699 - [=>----------------------------]   5% - acc: 0.9688 - loss: 1.4922 - 8713.2 sample/sec\n",
            "Global step: 22709 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4919 - 8828.2 sample/sec\n",
            "Global step: 22719 - [==>---------------------------]  10% - acc: 0.9453 - loss: 1.5154 - 8524.9 sample/sec\n",
            "Global step: 22729 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5311 - 8722.1 sample/sec\n",
            "Global step: 22739 - [====>-------------------------]  15% - acc: 0.9609 - loss: 1.4998 - 8754.9 sample/sec\n",
            "Global step: 22749 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5543 - 8743.4 sample/sec\n",
            "Global step: 22759 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.4999 - 8668.0 sample/sec\n",
            "Global step: 22769 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5075 - 8692.3 sample/sec\n",
            "Global step: 22779 - [=======>----------------------]  26% - acc: 0.9922 - loss: 1.4688 - 8581.1 sample/sec\n",
            "Global step: 22789 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8730.5 sample/sec\n",
            "Global step: 22799 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5227 - 8871.7 sample/sec\n",
            "Global step: 22809 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5077 - 8818.2 sample/sec\n",
            "Global step: 22819 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5303 - 8685.5 sample/sec\n",
            "Global step: 22829 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5234 - 8757.2 sample/sec\n",
            "Global step: 22839 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5148 - 8643.0 sample/sec\n",
            "Global step: 22849 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5305 - 8859.4 sample/sec\n",
            "Global step: 22859 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5387 - 8625.0 sample/sec\n",
            "Global step: 22869 - [==============>---------------]  49% - acc: 0.9219 - loss: 1.5378 - 8197.8 sample/sec\n",
            "Global step: 22879 - [==============>---------------]  51% - acc: 0.9141 - loss: 1.5469 - 8984.5 sample/sec\n",
            "Global step: 22889 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5152 - 8660.5 sample/sec\n",
            "Global step: 22899 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5077 - 8453.9 sample/sec\n",
            "Global step: 22909 - [=================>------------]  59% - acc: 0.9609 - loss: 1.4998 - 8680.9 sample/sec\n",
            "Global step: 22919 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5779 - 8731.5 sample/sec\n",
            "Global step: 22929 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5695 - 8716.7 sample/sec\n",
            "Global step: 22939 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5539 - 8918.0 sample/sec\n",
            "Global step: 22949 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5153 - 8593.5 sample/sec\n",
            "Global step: 22959 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8781.3 sample/sec\n",
            "Global step: 22969 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5075 - 8507.6 sample/sec\n",
            "Global step: 22979 - [======================>-------]  77% - acc: 0.9609 - loss: 1.4999 - 8708.7 sample/sec\n",
            "Global step: 22989 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5607 - 8716.3 sample/sec\n",
            "Global step: 22999 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5467 - 8749.1 sample/sec\n",
            "Global step: 23009 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8774.0 sample/sec\n",
            "Global step: 23019 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4918 - 8647.9 sample/sec\n",
            "Global step: 23029 - [==========================>---]  90% - acc: 0.9219 - loss: 1.5379 - 8601.1 sample/sec\n",
            "Global step: 23039 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5544 - 8803.3 sample/sec\n",
            "Global step: 23049 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5462 - 8597.6 sample/sec\n",
            "Global step: 23059 - [============================>-]  97% - acc: 0.9531 - loss: 1.5072 - 8635.5 sample/sec\n",
            "Global step: 23069 - [=============================>] 100% - acc: 0.9125 - loss: 1.5469 - 12416.1 sample/sec\n",
            "\n",
            "Epoch 59 - accuracy: 78.99% (7899/10000)\n",
            "###########################################################################################################\n",
            "\n",
            "Epoch: 60/60\n",
            "\n",
            "Global step: 23070 - [>-----------------------------]   0% - acc: 0.9453 - loss: 1.5153 - 8907.3 sample/sec\n",
            "Global step: 23080 - [>-----------------------------]   3% - acc: 0.9375 - loss: 1.5228 - 8840.0 sample/sec\n",
            "Global step: 23090 - [=>----------------------------]   5% - acc: 0.9688 - loss: 1.4923 - 8729.5 sample/sec\n",
            "Global step: 23100 - [==>---------------------------]   8% - acc: 0.9688 - loss: 1.4919 - 8644.8 sample/sec\n",
            "Global step: 23110 - [==>---------------------------]  10% - acc: 0.9453 - loss: 1.5154 - 8537.2 sample/sec\n",
            "Global step: 23120 - [===>--------------------------]  13% - acc: 0.9297 - loss: 1.5309 - 8925.8 sample/sec\n",
            "Global step: 23130 - [====>-------------------------]  15% - acc: 0.9609 - loss: 1.4997 - 8817.8 sample/sec\n",
            "Global step: 23140 - [=====>------------------------]  18% - acc: 0.9062 - loss: 1.5543 - 8731.6 sample/sec\n",
            "Global step: 23150 - [=====>------------------------]  20% - acc: 0.9609 - loss: 1.4999 - 8304.3 sample/sec\n",
            "Global step: 23160 - [======>-----------------------]  23% - acc: 0.9531 - loss: 1.5075 - 8756.4 sample/sec\n",
            "Global step: 23170 - [=======>----------------------]  26% - acc: 0.9922 - loss: 1.4688 - 8805.5 sample/sec\n",
            "Global step: 23180 - [========>---------------------]  28% - acc: 0.9453 - loss: 1.5156 - 8693.0 sample/sec\n",
            "Global step: 23190 - [========>---------------------]  31% - acc: 0.9375 - loss: 1.5227 - 8833.7 sample/sec\n",
            "Global step: 23200 - [=========>--------------------]  33% - acc: 0.9531 - loss: 1.5077 - 8788.1 sample/sec\n",
            "Global step: 23210 - [==========>-------------------]  36% - acc: 0.9297 - loss: 1.5303 - 8730.2 sample/sec\n",
            "Global step: 23220 - [===========>------------------]  38% - acc: 0.9375 - loss: 1.5234 - 8790.9 sample/sec\n",
            "Global step: 23230 - [===========>------------------]  41% - acc: 0.9453 - loss: 1.5148 - 8706.7 sample/sec\n",
            "Global step: 23240 - [============>-----------------]  43% - acc: 0.9297 - loss: 1.5305 - 8649.3 sample/sec\n",
            "Global step: 23250 - [=============>----------------]  46% - acc: 0.9219 - loss: 1.5387 - 8721.0 sample/sec\n",
            "Global step: 23260 - [==============>---------------]  49% - acc: 0.9219 - loss: 1.5378 - 8803.6 sample/sec\n",
            "Global step: 23270 - [==============>---------------]  51% - acc: 0.9141 - loss: 1.5469 - 8808.8 sample/sec\n",
            "Global step: 23280 - [===============>--------------]  54% - acc: 0.9453 - loss: 1.5152 - 8709.4 sample/sec\n",
            "Global step: 23290 - [================>-------------]  56% - acc: 0.9531 - loss: 1.5077 - 8836.9 sample/sec\n",
            "Global step: 23300 - [=================>------------]  59% - acc: 0.9609 - loss: 1.4997 - 8772.4 sample/sec\n",
            "Global step: 23310 - [=================>------------]  61% - acc: 0.8828 - loss: 1.5778 - 8530.8 sample/sec\n",
            "Global step: 23320 - [==================>-----------]  64% - acc: 0.8906 - loss: 1.5695 - 8595.4 sample/sec\n",
            "Global step: 23330 - [===================>----------]  66% - acc: 0.9062 - loss: 1.5539 - 8641.2 sample/sec\n",
            "Global step: 23340 - [====================>---------]  69% - acc: 0.9453 - loss: 1.5154 - 8743.4 sample/sec\n",
            "Global step: 23350 - [====================>---------]  72% - acc: 0.9453 - loss: 1.5159 - 8850.5 sample/sec\n",
            "Global step: 23360 - [=====================>--------]  74% - acc: 0.9531 - loss: 1.5075 - 8829.8 sample/sec\n",
            "Global step: 23370 - [======================>-------]  77% - acc: 0.9609 - loss: 1.4999 - 8824.2 sample/sec\n",
            "Global step: 23380 - [======================>-------]  79% - acc: 0.8984 - loss: 1.5606 - 8762.8 sample/sec\n",
            "Global step: 23390 - [=======================>------]  82% - acc: 0.9141 - loss: 1.5466 - 8690.3 sample/sec\n",
            "Global step: 23400 - [========================>-----]  84% - acc: 0.9844 - loss: 1.4768 - 8812.1 sample/sec\n",
            "Global step: 23410 - [=========================>----]  87% - acc: 0.9688 - loss: 1.4918 - 8915.8 sample/sec\n",
            "Global step: 23420 - [==========================>---]  90% - acc: 0.9219 - loss: 1.5379 - 8791.8 sample/sec\n",
            "Global step: 23430 - [==========================>---]  92% - acc: 0.9062 - loss: 1.5544 - 8642.2 sample/sec\n",
            "Global step: 23440 - [===========================>--]  95% - acc: 0.9141 - loss: 1.5459 - 8642.8 sample/sec\n",
            "Global step: 23450 - [============================>-]  97% - acc: 0.9531 - loss: 1.5072 - 8642.9 sample/sec\n",
            "Global step: 23460 - [=============================>] 100% - acc: 0.9125 - loss: 1.5468 - 10066.2 sample/sec\n",
            "\n",
            "Epoch 60 - accuracy: 79.00% (7900/10000)\n",
            "###########################################################################################################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExYeRmfG46nW",
        "colab_type": "code",
        "outputId": "17a4d097-c505-4c11-a97b-3ac82324bd9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "#import numpy as np\n",
        "#import tensorflow as tf\n",
        "\n",
        "##from include.data import get_data_set\n",
        "##from include.model import model\n",
        "\n",
        "\n",
        "test_x, test_y = get_data_set(\"test\")\n",
        "#x, y, output, y_pred_cls, global_step, learning_rate = model()\n",
        "#tf.reset_default_graph()\n",
        "\n",
        "_BATCH_SIZE = 128\n",
        "_CLASS_SIZE = 10\n",
        "_SAVE_PATH = \"./tensorboard/cifar-10-v1.0.0/\"\n",
        "\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "sess = tf.Session()\n",
        "\n",
        "\n",
        "try:\n",
        "    print(\"\\nTrying to restore last checkpoint ...\")\n",
        "    last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=_SAVE_PATH)\n",
        "    saver.restore(sess, save_path=last_chk_path)\n",
        "    print(\"Restored checkpoint from:\", last_chk_path)\n",
        "except ValueError:\n",
        "    print(\"\\nFailed to restore checkpoint. Initializing variables instead.\")\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "def main():\n",
        "    i = 0\n",
        "    predicted_class = np.zeros(shape=len(test_x), dtype=np.int)\n",
        "    while i < len(test_x):\n",
        "        j = min(i + _BATCH_SIZE, len(test_x))\n",
        "        batch_xs = test_x[i:j, :]\n",
        "        batch_ys = test_y[i:j, :]\n",
        "        predicted_class[i:j] = sess.run(y_pred_cls, feed_dict={x: batch_xs, y: batch_ys})\n",
        "        i = j\n",
        "\n",
        "    correct = (np.argmax(test_y, axis=1) == predicted_class)\n",
        "    acc = correct.mean() * 100\n",
        "    correct_numbers = correct.sum()\n",
        "    print()\n",
        "    print(\"Accuracy on Test-Set: {0:.2f}% ({1} / {2})\".format(acc, correct_numbers, len(test_x)))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0629 01:55:25.858767 140239015667584 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Trying to restore last checkpoint ...\n",
            "Restored checkpoint from: ./tensorboard/cifar-10-v1.0.0/-17986\n",
            "\n",
            "Accuracy on Test-Set: 79.15% (7915 / 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnB9WtrKc-BR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmGbI3GEhBHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################\n",
        "# Usando Keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zobVWe4linSg",
        "colab_type": "code",
        "outputId": "b950e3f0-caa9-4797-f8d4-c8d4e3bb7900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Invalid requirement: 'keras=1.2.2'\n",
            "= is not a valid operator. Did you mean == ?\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21tXpH1Gim9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.regularizers import l2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyt94ZC4jEcn",
        "colab_type": "code",
        "outputId": "2c322f1c-2608-4aaf-bdf8-e3e5ded6556b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "print('Training data shape : ', train_images.shape, train_labels.shape)\n",
        "print('Testing data shape : '), test_images.shape, test_labels.shape\n",
        "\n",
        "classes = np.unique(train_labels)\n",
        "nClasses = len(classes)\n",
        "print('Total number of outputs : ', nClasses)\n",
        "print('Output classes  : ', classes)\n",
        "\n",
        "plt.figure(figsize = [4,2])\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.imshow(train_images[0,:,:],cmap='gray')\n",
        "plt.title(\"Ground Truth : {}\".format(train_labels[0]))\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.imshow(test_images[0,:,:],cmap='gray')\n",
        "plt.title(\"Ground Truth : {}\".format(test_labels[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data shape :  (50000, 32, 32, 3) (50000, 1)\n",
            "Testing data shape : \n",
            "Total number of outputs :  10\n",
            "Output classes  :  [0 1 2 3 4 5 6 7 8 9]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Ground Truth : [3]')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAACUCAYAAACa2zzDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWFyDY8fgWgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 288x144 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oZx1NNdlLBa",
        "colab_type": "code",
        "outputId": "46c4c6b7-7a7b-4141-b2cb-7ef0932e3414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "nRows, nCols, nDims = train_images.shape[1:]\n",
        "train_data = train_images.reshape(train_images.shape[0],nRows, nCols, nDims)\n",
        "test_data = test_images.reshape(test_images.shape[0],nRows,nCols,nDims)\n",
        "input_shape = (nRows, nCols, nDims)\n",
        "\n",
        "train_data = train_data.astype('float32')\n",
        "test_data = test_data.astype('float32')\n",
        "\n",
        "train_data /= 255\n",
        "test_data /= 255\n",
        "\n",
        "train_labels_one_hot = to_categorical(train_labels)\n",
        "test_labels_one_hot = to_categorical(test_labels)\n",
        "\n",
        "print(nRows)\n",
        "print(nCols)\n",
        "print('Orginal label 0 : ', train_labels[0])\n",
        "print('After convertion to categorical : ', train_labels_one_hot[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n",
            "32\n",
            "Orginal label 0 :  [6]\n",
            "After convertion to categorical :  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIB5Nav0lLxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createModel():\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Conv2D(32,(3,3),padding='same', activation='relu',input_shape=input_shape))   # El paper indica 16\n",
        "  model.add(Conv2D(32,(3,3),activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  \n",
        "  model.add(Conv2D(64,(3,3),padding='same', activation='relu'))  # El paper indica el doble del anterior\n",
        "  model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  \n",
        "  model.add(Conv2D(64,(3,3),padding='same',activation='relu'))\n",
        "  model.add(Conv2D(64,(3,3),activation='relu',W_regularizer=l2(0.01)))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  \n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(256,activation='relu',W_regularizer=l2(0.01)))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(nClasses,activation='softmax'))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5nboRljjE-d",
        "colab_type": "code",
        "outputId": "4f40ddcd-591f-4b3c-bbd5-f555dd6eb64b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model1 = createModel()\n",
        "batch_size = 256\n",
        "epochs = 50\n",
        "model1.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model1.summary()\n",
        "\n",
        "history = model1.fit(train_data,train_labels_one_hot,batch_size=batch_size, epochs = epochs,verbose=1,validation_data=(test_data, test_labels_one_hot))\n",
        "model1.evaluate(test_data, test_labels_one_hot)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, activation=\"relu\", kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_42 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 6, 6, 64)          36928     \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 2, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 2, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 207,786\n",
            "Trainable params: 207,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "50000/50000 [==============================] - 6s 122us/step - loss: 2.5891 - acc: 0.1940 - val_loss: 1.9759 - val_acc: 0.2672\n",
            "Epoch 2/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.9316 - acc: 0.3008 - val_loss: 1.9475 - val_acc: 0.3224\n",
            "Epoch 3/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.7586 - acc: 0.3758 - val_loss: 1.6555 - val_acc: 0.4194\n",
            "Epoch 4/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.6420 - acc: 0.4208 - val_loss: 1.4851 - val_acc: 0.4776\n",
            "Epoch 5/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.5595 - acc: 0.4563 - val_loss: 1.4547 - val_acc: 0.4970\n",
            "Epoch 6/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.4888 - acc: 0.4865 - val_loss: 1.4836 - val_acc: 0.4799\n",
            "Epoch 7/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.4309 - acc: 0.5119 - val_loss: 1.5781 - val_acc: 0.4807\n",
            "Epoch 8/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 1.3829 - acc: 0.5320 - val_loss: 1.5997 - val_acc: 0.4401\n",
            "Epoch 9/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.3440 - acc: 0.5478 - val_loss: 1.4261 - val_acc: 0.5450\n",
            "Epoch 10/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.3046 - acc: 0.5652 - val_loss: 1.3428 - val_acc: 0.5765\n",
            "Epoch 11/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.2675 - acc: 0.5801 - val_loss: 1.1724 - val_acc: 0.6019\n",
            "Epoch 12/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.2287 - acc: 0.5975 - val_loss: 1.2341 - val_acc: 0.5867\n",
            "Epoch 13/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.2003 - acc: 0.6054 - val_loss: 1.1886 - val_acc: 0.6173\n",
            "Epoch 14/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.1663 - acc: 0.6203 - val_loss: 1.0912 - val_acc: 0.6427\n",
            "Epoch 15/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.1351 - acc: 0.6315 - val_loss: 1.0803 - val_acc: 0.6507\n",
            "Epoch 16/100\n",
            "50000/50000 [==============================] - 5s 94us/step - loss: 1.1069 - acc: 0.6418 - val_loss: 1.0455 - val_acc: 0.6647\n",
            "Epoch 17/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.0812 - acc: 0.6568 - val_loss: 1.0243 - val_acc: 0.6725\n",
            "Epoch 18/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.0550 - acc: 0.6631 - val_loss: 1.1201 - val_acc: 0.6561\n",
            "Epoch 19/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.0354 - acc: 0.6757 - val_loss: 1.0172 - val_acc: 0.6765\n",
            "Epoch 20/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 1.0100 - acc: 0.6840 - val_loss: 0.9743 - val_acc: 0.6910\n",
            "Epoch 21/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.9802 - acc: 0.6953 - val_loss: 1.0492 - val_acc: 0.6738\n",
            "Epoch 22/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.9672 - acc: 0.6998 - val_loss: 1.0073 - val_acc: 0.6954\n",
            "Epoch 23/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.9460 - acc: 0.7098 - val_loss: 1.0399 - val_acc: 0.6781\n",
            "Epoch 24/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.9209 - acc: 0.7163 - val_loss: 0.9394 - val_acc: 0.7107\n",
            "Epoch 25/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.9119 - acc: 0.7202 - val_loss: 0.9376 - val_acc: 0.7186\n",
            "Epoch 26/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.8883 - acc: 0.7296 - val_loss: 0.8760 - val_acc: 0.7353\n",
            "Epoch 27/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.8857 - acc: 0.7296 - val_loss: 0.8060 - val_acc: 0.7607\n",
            "Epoch 28/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.8620 - acc: 0.7399 - val_loss: 0.8442 - val_acc: 0.7475\n",
            "Epoch 29/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.8515 - acc: 0.7442 - val_loss: 0.8986 - val_acc: 0.7241\n",
            "Epoch 30/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.8412 - acc: 0.7505 - val_loss: 0.9193 - val_acc: 0.7146\n",
            "Epoch 31/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.8265 - acc: 0.7517 - val_loss: 0.8279 - val_acc: 0.7550\n",
            "Epoch 32/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.8104 - acc: 0.7581 - val_loss: 0.7771 - val_acc: 0.7634\n",
            "Epoch 33/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.8029 - acc: 0.7617 - val_loss: 0.9129 - val_acc: 0.7335\n",
            "Epoch 34/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.7905 - acc: 0.7652 - val_loss: 0.8738 - val_acc: 0.7368\n",
            "Epoch 35/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.7793 - acc: 0.7700 - val_loss: 0.8790 - val_acc: 0.7445\n",
            "Epoch 36/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.7714 - acc: 0.7702 - val_loss: 0.8663 - val_acc: 0.7480\n",
            "Epoch 37/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.7560 - acc: 0.7753 - val_loss: 0.8138 - val_acc: 0.7601\n",
            "Epoch 38/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.7570 - acc: 0.7781 - val_loss: 0.9155 - val_acc: 0.7350\n",
            "Epoch 39/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.7460 - acc: 0.7813 - val_loss: 0.8820 - val_acc: 0.7352\n",
            "Epoch 40/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.7344 - acc: 0.7837 - val_loss: 0.8694 - val_acc: 0.7488\n",
            "Epoch 41/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.7334 - acc: 0.7845 - val_loss: 0.7790 - val_acc: 0.7719\n",
            "Epoch 42/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.7198 - acc: 0.7900 - val_loss: 0.9064 - val_acc: 0.7303\n",
            "Epoch 43/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.7217 - acc: 0.7883 - val_loss: 0.7133 - val_acc: 0.7873\n",
            "Epoch 44/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.7084 - acc: 0.7928 - val_loss: 0.8394 - val_acc: 0.7582\n",
            "Epoch 45/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.7036 - acc: 0.7950 - val_loss: 0.7189 - val_acc: 0.7877\n",
            "Epoch 46/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.6976 - acc: 0.7979 - val_loss: 0.6742 - val_acc: 0.8061\n",
            "Epoch 47/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.6851 - acc: 0.8009 - val_loss: 0.7686 - val_acc: 0.7783\n",
            "Epoch 48/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.6836 - acc: 0.8027 - val_loss: 0.8211 - val_acc: 0.7670\n",
            "Epoch 49/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.6737 - acc: 0.8066 - val_loss: 0.8667 - val_acc: 0.7507\n",
            "Epoch 50/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.6733 - acc: 0.8063 - val_loss: 0.7309 - val_acc: 0.7902\n",
            "Epoch 51/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.6666 - acc: 0.8074 - val_loss: 0.6950 - val_acc: 0.7970\n",
            "Epoch 52/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.6565 - acc: 0.8102 - val_loss: 0.7220 - val_acc: 0.7975\n",
            "Epoch 53/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.6484 - acc: 0.8127 - val_loss: 0.7811 - val_acc: 0.7767\n",
            "Epoch 54/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.6489 - acc: 0.8142 - val_loss: 0.7259 - val_acc: 0.7905\n",
            "Epoch 55/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.6432 - acc: 0.8152 - val_loss: 0.6910 - val_acc: 0.8008\n",
            "Epoch 56/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.6439 - acc: 0.8179 - val_loss: 0.8133 - val_acc: 0.7714\n",
            "Epoch 57/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.6314 - acc: 0.8188 - val_loss: 0.8576 - val_acc: 0.7598\n",
            "Epoch 58/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.6283 - acc: 0.8209 - val_loss: 0.7145 - val_acc: 0.7949\n",
            "Epoch 59/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.6240 - acc: 0.8204 - val_loss: 0.6791 - val_acc: 0.8042\n",
            "Epoch 60/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.6183 - acc: 0.8243 - val_loss: 0.6884 - val_acc: 0.8097\n",
            "Epoch 61/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.6176 - acc: 0.8234 - val_loss: 0.6599 - val_acc: 0.8078\n",
            "Epoch 62/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.6188 - acc: 0.8242 - val_loss: 0.6725 - val_acc: 0.8127\n",
            "Epoch 63/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.6069 - acc: 0.8273 - val_loss: 0.6798 - val_acc: 0.8074\n",
            "Epoch 64/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.6131 - acc: 0.8255 - val_loss: 0.6512 - val_acc: 0.8136\n",
            "Epoch 65/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5983 - acc: 0.8318 - val_loss: 0.6930 - val_acc: 0.8102\n",
            "Epoch 66/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.6003 - acc: 0.8318 - val_loss: 0.6644 - val_acc: 0.8100\n",
            "Epoch 67/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.6011 - acc: 0.8304 - val_loss: 0.7363 - val_acc: 0.7935\n",
            "Epoch 68/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5907 - acc: 0.8332 - val_loss: 0.7118 - val_acc: 0.7982\n",
            "Epoch 69/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.5906 - acc: 0.8341 - val_loss: 0.6757 - val_acc: 0.8120\n",
            "Epoch 70/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5845 - acc: 0.8358 - val_loss: 0.6871 - val_acc: 0.8099\n",
            "Epoch 71/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5786 - acc: 0.8370 - val_loss: 0.7605 - val_acc: 0.7943\n",
            "Epoch 72/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5826 - acc: 0.8353 - val_loss: 0.7272 - val_acc: 0.7975\n",
            "Epoch 73/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.5760 - acc: 0.8380 - val_loss: 0.7051 - val_acc: 0.8031\n",
            "Epoch 74/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5752 - acc: 0.8372 - val_loss: 0.7044 - val_acc: 0.8040\n",
            "Epoch 75/100\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 0.5683 - acc: 0.8412 - val_loss: 0.7881 - val_acc: 0.7870\n",
            "Epoch 76/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5673 - acc: 0.8409 - val_loss: 0.7525 - val_acc: 0.7941\n",
            "Epoch 77/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5672 - acc: 0.8418 - val_loss: 0.7160 - val_acc: 0.8020\n",
            "Epoch 78/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5636 - acc: 0.8409 - val_loss: 0.7737 - val_acc: 0.7877\n",
            "Epoch 79/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5585 - acc: 0.8425 - val_loss: 0.7143 - val_acc: 0.8102\n",
            "Epoch 80/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5588 - acc: 0.8446 - val_loss: 0.7479 - val_acc: 0.7878\n",
            "Epoch 81/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5547 - acc: 0.8458 - val_loss: 0.7288 - val_acc: 0.7959\n",
            "Epoch 82/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5548 - acc: 0.8436 - val_loss: 0.7041 - val_acc: 0.8036\n",
            "Epoch 83/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5538 - acc: 0.8454 - val_loss: 0.7056 - val_acc: 0.8085\n",
            "Epoch 84/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.5521 - acc: 0.8468 - val_loss: 0.7373 - val_acc: 0.7933\n",
            "Epoch 85/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5471 - acc: 0.8476 - val_loss: 0.6733 - val_acc: 0.8124\n",
            "Epoch 86/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5409 - acc: 0.8491 - val_loss: 0.6710 - val_acc: 0.8099\n",
            "Epoch 87/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.5366 - acc: 0.8505 - val_loss: 0.6990 - val_acc: 0.8084\n",
            "Epoch 88/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5391 - acc: 0.8500 - val_loss: 0.7207 - val_acc: 0.8026\n",
            "Epoch 89/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5376 - acc: 0.8512 - val_loss: 0.6402 - val_acc: 0.8241\n",
            "Epoch 90/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5318 - acc: 0.8525 - val_loss: 0.7001 - val_acc: 0.8040\n",
            "Epoch 91/100\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 0.5367 - acc: 0.8512 - val_loss: 0.6823 - val_acc: 0.8022\n",
            "Epoch 92/100\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 0.5319 - acc: 0.8523 - val_loss: 0.6513 - val_acc: 0.8162\n",
            "Epoch 93/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5266 - acc: 0.8528 - val_loss: 0.7736 - val_acc: 0.7923\n",
            "Epoch 94/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5288 - acc: 0.8525 - val_loss: 0.6748 - val_acc: 0.8140\n",
            "Epoch 95/100\n",
            "50000/50000 [==============================] - 5s 95us/step - loss: 0.5291 - acc: 0.8536 - val_loss: 0.7852 - val_acc: 0.7900\n",
            "Epoch 96/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5255 - acc: 0.8546 - val_loss: 0.8672 - val_acc: 0.7670\n",
            "Epoch 97/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5222 - acc: 0.8552 - val_loss: 0.6471 - val_acc: 0.8230\n",
            "Epoch 98/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5170 - acc: 0.8551 - val_loss: 0.7627 - val_acc: 0.8032\n",
            "Epoch 99/100\n",
            "50000/50000 [==============================] - 5s 97us/step - loss: 0.5200 - acc: 0.8571 - val_loss: 0.6876 - val_acc: 0.8158\n",
            "Epoch 100/100\n",
            "50000/50000 [==============================] - 5s 96us/step - loss: 0.5224 - acc: 0.8574 - val_loss: 0.6803 - val_acc: 0.8139\n",
            "10000/10000 [==============================] - 1s 88us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6803314639568329, 0.8139]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Esr6pO7htDBY",
        "colab_type": "code",
        "outputId": "b0be1b94-1a5d-46b0-be7b-4cf36a9cf451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "plt.figure(figsize=[8,6])\n",
        "plt.plot(history.history['loss'],'r',linewidth = 3.0)\n",
        "plt.plot(history.history['val_loss'],'b',linewidth= 3.0)\n",
        "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
        "plt.xlabel('Epoch ',fontsize=16)\n",
        "plt.ylabel('Loss',fontsize=16)\n",
        "plt.title('Loss Curves',fontsize=16)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss Curves')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAGKCAYAAADkN4OIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6ImIiFLY/wOlWpSyZjRUaQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WuA-lLyuRIK",
        "colab_type": "code",
        "outputId": "7ded857e-effa-4df6-c246-3467f429e691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "plt.figure(figsize=[8,6])\n",
        "plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
        "plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
        "plt.legend(['Training Accuracy','Validation Accuracy'],fontsize=18)\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Accuracy',fontsize=16)\n",
        "plt.title('Accuracy Curves',fontsize=16)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Accuracy Curves')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAGKCAYAAADkN4OIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzZhv7gFvs0Q",
        "colab_type": "code",
        "outputId": "65fd8a18-c2b0-49c3-fa02-e607025c510f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "model2 = createModel()\n",
        "\n",
        "model2.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "batch_size = 256\n",
        "epoch = 50\n",
        "datagen  = ImageDataGenerator(width_shift_range=0.1,height_shift_range=0.1,horizontal_flip=True,vertical_flip=False)\n",
        "\n",
        "history2 = model2.fit_generator(datagen.flow(train_data, train_labels_one_hot,batch_size=batch_size),\n",
        "                               steps_per_epoch=int(np.ceil(train_data.shape[0]/float(batch_size))),\n",
        "                               epochs=epochs,\n",
        "                               validation_data=(test_data,test_labels_one_hot),\n",
        "                               workers=4)\n",
        "\n",
        "model2.evaluate(test_data, test_labels_one_hot)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, activation=\"relu\", kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "196/196 [==============================] - 24s 124ms/step - loss: 2.5686 - acc: 0.2104 - val_loss: 2.0556 - val_acc: 0.2467\n",
            "Epoch 2/100\n",
            "196/196 [==============================] - 22s 113ms/step - loss: 1.9603 - acc: 0.3006 - val_loss: 1.7854 - val_acc: 0.3721\n",
            "Epoch 3/100\n",
            "196/196 [==============================] - 23s 115ms/step - loss: 1.8241 - acc: 0.3553 - val_loss: 1.6600 - val_acc: 0.4283\n",
            "Epoch 4/100\n",
            "196/196 [==============================] - 22s 115ms/step - loss: 1.7294 - acc: 0.3892 - val_loss: 1.5327 - val_acc: 0.4505\n",
            "Epoch 5/100\n",
            "196/196 [==============================] - 22s 113ms/step - loss: 1.6589 - acc: 0.4145 - val_loss: 1.4939 - val_acc: 0.4770\n",
            "Epoch 6/100\n",
            "196/196 [==============================] - 22s 113ms/step - loss: 1.5943 - acc: 0.4412 - val_loss: 1.5299 - val_acc: 0.4697\n",
            "Epoch 7/100\n",
            "196/196 [==============================] - 22s 111ms/step - loss: 1.5410 - acc: 0.4694 - val_loss: 1.4882 - val_acc: 0.4898\n",
            "Epoch 8/100\n",
            "196/196 [==============================] - 21s 109ms/step - loss: 1.4945 - acc: 0.4868 - val_loss: 1.4006 - val_acc: 0.5253\n",
            "Epoch 9/100\n",
            "196/196 [==============================] - 22s 112ms/step - loss: 1.4548 - acc: 0.5007 - val_loss: 1.3188 - val_acc: 0.5517\n",
            "Epoch 10/100\n",
            "196/196 [==============================] - 23s 116ms/step - loss: 1.4199 - acc: 0.5209 - val_loss: 1.2623 - val_acc: 0.5645\n",
            "Epoch 11/100\n",
            "196/196 [==============================] - 22s 113ms/step - loss: 1.3676 - acc: 0.5398 - val_loss: 1.2977 - val_acc: 0.5721\n",
            "Epoch 12/100\n",
            "196/196 [==============================] - 22s 110ms/step - loss: 1.3501 - acc: 0.5462 - val_loss: 1.6082 - val_acc: 0.4964\n",
            "Epoch 13/100\n",
            "196/196 [==============================] - 22s 111ms/step - loss: 1.3037 - acc: 0.5670 - val_loss: 1.1686 - val_acc: 0.6125\n",
            "Epoch 14/100\n",
            "109/196 [===============>..............] - ETA: 9s - loss: 1.2915 - acc: 0.5712"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-bba376fad59c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                                \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_labels_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                workers=4)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0_qrttvD3iB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVlTbk4uwxF_",
        "colab_type": "code",
        "outputId": "e3bbcf63-b7d8-4190-9856-bcbccafb99e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "plt.figure(figsize=[8,6])\n",
        "plt.plot(history2.history['loss'],'r',linewidth = 3.0)\n",
        "plt.plot(history2.history['val_loss'],'b',linewidth= 3.0)\n",
        "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
        "plt.xlabel('Epoch ',fontsize=16)\n",
        "plt.ylabel('Loss',fontsize=16)\n",
        "plt.title('Loss Curves',fontsize=16)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss Curves')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAGKCAYAAADkN4OIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERFFMRb0REREUez/AVqiMQPs5wRdAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brodhArBwxhj",
        "colab_type": "code",
        "outputId": "be13d36a-dbf0-4a57-935a-bb1510ac6969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "plt.figure(figsize=[8,6])\n",
        "plt.plot(history2.history['acc'],'r',linewidth=3.0)\n",
        "plt.plot(history2.history['val_acc'],'b',linewidth=3.0)\n",
        "plt.legend(['Training Accuracy','Validation Accuracy'],fontsize=18)\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Accuracy',fontsize=16)\n",
        "plt.title('Accuracy Curves',fontsize=16)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Accuracy Curves')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAGKCAYAAADkN4OIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3XeYk8Xax/HvsCy9SFUUEAERFUUF\nFSvYEFGKvStW7B47do/6qth7wwIqFvRYUFGxYUMUUBClCCIICAqI9GXbvH/cCU+ym+xmd5Psbvh9\nrisXT5k8mewpd2bmnhnnvUdEREQyU43KroCIiIikjgK9iIhIBlOgFxERyWAK9CIiIhlMgV5ERCSD\nKdCLiIhkMAV6kQpyzg1zznnn3AOVXZfqwpmTnXOfOueWO+fynHMLnXOvOucOqOz6iWQSp3n0IuXn\nnKsLLAEaAX8DW3nv8yu3VlWbcy4LeBU4EhgBvAv8A7QBjgX6A0289ysrrZIiGaRmZVdApJobiAX5\nMUBfoA/wXqXWKAbnXG3v/YbKrkfItcAxwDHe+/8VuTfSOdcbyKvoh1Sx7yxSadR1L1IxpwMrgEHA\n+tB5Mc65rs65t0Ld1Oudc7Occ9cWKXOkc+4b59wa59wq59z3zrn+oXvtQsMDg4q8p1foeq+Ia+Oc\nc1875/o55350zm0ALgjdu8g5961z7h/n3L/OuQnOucNj1Le+c+4u59xvzrkNzrklzrn/Oec2d851\nC33mgBjvGx7qgs+K83eoBVwBvB8jyAPgvR/rvV8X8V3GxXjOPOfc8IjzQaE67e+ce9059y/wnXPu\nKudcrnOuWYxnTHfOvRNxXs85N9Q593voPb875653ztWIKNPAOfeIc+6P0N/lb+fcJ865zrG+i0hV\noBa9SDk557YEDgaGee+XOufeBo5yzjXx3q+IKLcHMA6YA1wGLAS2BXaOKHMx8DDwNvZjYQ2wG9Cu\nnNXrFHrebcBcrGuc0POeAeZh//vvB7znnDvMe/9hqC61gI+BrsBdwASgMXAo1qU+2Tk3ERgMRAbK\nzYDjgLu99wVx6tUd2AwYXc7vVZqRwCtYj0FNYFroOxwPPB5R127A9sCNofOawEfADtjfbBrQI3S/\nKfbjBOABbGjhOmA20AzYJ/SdRKom771eeulVjhdwNeCBvULnh4bOzytS7ktgAVAvznMaAauBN0v4\nrHahZw8qcr1X6HqviGvjgEJgl1LqXwMLhmOBdyKunxl6Zv8S3jsIKAC2jrh2CZAPtC7hfceHnn1o\ngn/jccC4GNfnAcOL1McDD8Qo+zHwbZFrD2I9MbVD56eG3r9/kXLXA7lAy9D5z8D9lf3fPb30KstL\nXfci5Xc6MNt7/23o/BPgTyK6751z9bAW30gf6o6OYW+gAfB0Eus2z3s/pejFULf7e865v7CgnAcc\nAmwXUaw3sMR7X1Kr+1XgX+CciGuDsS75hRWuffm9FePaC0AP51xH2Nh6PxEY5YMx/D7AfGC8c65m\n+IX9CMrGWvcAE4FBzrnrnHPd4w1RiFQlCvQi5eCc6451877pnNss1G3dEHgTCyqdQkWbYP87Kyn4\nhcePkxkgFxe94JxrA3yKdUVfjP3A2B34EKhTpD6LSnq49z4HeB44MxQU98P+Hk+WUq8FoX+3TuA7\nlEex7439Z7IWa7WD/ZBpif0ACGsZqlNekdf3ofvh/4wuBp7Cej0mAn875x4I/aATqZIU6EXKJ9xq\nvwbrAg6/LgpdPy307wqsG32rEp61LPRvSWVyQv/WKnK9WJJZSKx5s32wsfbjvPejvPcTvPeTgKJB\nalkpdQl7AtgCGIC15udh49wlmYT1BPRL4Plg37vodwb7sRJLse/tvV+LtfRPDl06BZjrvf8mothy\n4Hfsh0+s17uhZ63x3l/rve+IDafcgf1nfnOC30ck7RToRcoolKx2IvAdcECM1xTgVOecC3XXfw2c\nEppzH8t4LPnu3BI+9i9gA9ClyPViGfMlCAf0jVPXQj0P+xQpNxbYwjlXYjD23v8WKnsVlvw2zHtf\nWMp7coH7gCOcc0fHKuOcOySihTwf6BT6m4fv74/1npTFC0AH59yh2JTIl4rc/xCbx7/Gez8pxmtZ\n0Qd67+d77+/DEveK/uciUmUo616k7A7HWtJXeO/HFb3pnHsKa+32Aj4HrgS+AL51zt2HddG3x5Ll\nLvberw5NtXvEOfc/LHN8NbALkOO9f8R7751zrwFnOed+BWaF6tGrDPX+BBuXfyFUj1bAf4E/iP7R\n/xI29v6Kc+5O7AdNQyzZ8EHv/cyIso9jmfd5wLMJ1uNOLKP/tdAUufCCOa2Bo4GjsCEPsFyAc4Hn\nQmW3AS4HyrqYzqdY/sSzQF3gxSL3RwJnAJ+G/jZTsZ6EDliW/UDv/Trn3LfYjIFp2I+znqHvMqKM\n9RFJn8rOBtRLr+r2wqbArSJ+Fn1jYB3RWeG7YgHtX2y+/UzgmiLvOwYLqutDz/8OOCLi/mZYgFqG\nBcYnsWAfK+v+6zh1Oy702TnAL8AJwHAseS+yXAPgHqxFnYuNfb9BKPs8olxW6Lu+Xsa/ocO60D/D\nhjfysB9ArwD7FSk7GJvKth7r/ehG/Kz7jiV85j2hMuPj3K8D3BL6+2wI/Y0nhq7VDJUZCvyI/dBY\niwX8Syr7v5N66VXSS0vgiki5OecOwbrvD/bef1rZ9RGR4hToRaTMnHMdsOGHB4AN3vtulVwlEYlD\nyXgiUh43Ah9gXdynlVJWRCqRWvQiIiIZTC16ERGRDKZALyIiksEyYh598+bNfbt27Sq7GiIiImkx\nefLkZd77FomUzYhA365dOyZNmlTZ1RAREUkL59z8RMuq615ERCSDKdCLiIhkMAV6ERGRDKZALyIi\nksEU6EVERDKYAr2IiEgGU6AXERHJYAr0IiIiGUyBXkREJIMp0IuIiGQwBXoREZEMpkAvIiISx7PP\nQrNmcNZZUFBQ2bUpHwV6ERGRGPLz4cor4Z9/4Lnn4KGHKrtG5aNALyIiEsN338G//wbn118Ps2ZV\nXn3KS4FeREQkho8+ij7PyYEzzqh+XfgK9CIiIjF8+GHxa99+Cw8+mP66VIQCvYiISBFLl8KkSXac\nlQWXXRbcu+GG6tWFr0AvIiJSxMcfg/d23KMHDB0Ku+xi59WtC1+BXkREpIjI8fk+fSA7G4YPh5o1\n7Vp16sJXoBcREYlQWBgd6A891P7t2hVuvDG4Xl268BXoRUREIkydCn/9ZcfNm0O3bsG9a6+FXXe1\n4+rSha9ALyIiEiGyNd+7N9SIiJSxuvAfeCCt1SszBXoREZEIkdPq+vQpfn/nnYt34c+cmfp6lZcC\nvYiISMiqVfDNN8F5796xy0V24W/YAGeeWXW78BXoRUREQj77zNa4Bwvkm28eu1y4Cz87286//RZe\ney0tVSwzBXoREZGQotPqSrLzznDVVcH5Cy+kpk4VpUAvIiJJt2QJnHwynHeedYdXB95Hj8+Hp9WV\nZPDg4Pjjj+17VzUK9CIiklR5eXDUUfDyy/DUU9C3L6xZUzl1WboUFi1KrOyvv8K8eXbcsCHstVfp\n72nbFnr2tOPCQnj11XJVM6UU6EVEJKmuvdbGrMO++Qb694f169Nbj8mToX17e731VunlI7vtDzoI\natVK7HNOOSU4fumlstUxHRToRUQkaUaPhvvuK37988+tlb9hQ/rqct111pOQmwvnnGOt+5KUtds+\n7Jhjgh8FkyfDjBllr2sqKdCLiEhSzJsHp58enB9+ONxxR3D+4Ydw/PHWtZ9qEyfC2LHB+fLlcOml\n8cvn5MC4ccF5WQL9ZptBv37B+ciRib83HRToRUSkwnJzLYj/+6+dt2kDI0ZYN/5NNwXl3nkHTj01\n9XPO77yz+LVXXoF3341d/quvgqGF7baDbbYp2+dFdt+PHGnj9VWFAr2IiFTYNdfA99/bcc2aMGoU\nNGtm57fcEj0N7bXXbIGZVAXD6dOjx+R79QqOzz8fVq4s/p7SVsMrzWGHQZMmdjxvHowfX/ZnpIoC\nvYiIVMhbb0Vv2Tp0qO3hHuacXbv44uDaCy/ABRcEe74n0113BccDBsAbb0CLFna+aBEMGVL8PeUd\nnw+rXRuOOy44r0pJeQr0IiJSbnPn2g5uYQMGwGWXFS/nnP0YOPvs4NpTT1nZZAb7uXNtWl/Ytdda\nz8KjjwbXnnwSvvgiOF+wwHoBwAJ2eLpcWUV2348ald7Ew5Io0IuISLls2GCt2HBXeLt28PzzFtRj\nqVHDgmxkQHzoIevWT1awv+eeYPz/oINgzz3t+Nhj7UdI2NlnB2PykdPqevaEevXK99l7721/A4AV\nK2DMmPI9J9kU6EVEZCPvLTHtjDNsrfc+fWxce+hQa6V+/z38/beVu/JKm04Gtub7qFHBOHU8WVn2\nY+DYY4Nr990Hl19e8WC/eDE891xwft11wbFz8Pjj0Lixnc+ZY7kDUPFu+7AaNarmnPqalV0BERGp\nfEuW2Lj5s8/aCnGlqVcP1q0Lzu+9F3bfPbHPqlnTMtPz8uDtt+3agw/aZjIPPxy/R6A0999v2f9g\nLfkDDoi+v+WWVs9zzgnqfNRR8MknQZkyJ+ItWWJjBWvXQufOnLx7V26nEwDvvWct+9J+/KSa86nI\nhEiz7t27+0mTJlV2NUREqpX8fPjgAwvu771X/ilvRx8Nr79e9gCdlwcnngj/+19w7fzzbTy9Rhn7\nm5cvh623tngLNo2vf//i5by3Lv3PP7fzFi2ChXTatIH58xP8HnPn2jjB888XG4zfne+ZhP3qebr3\nG5xz/CrYfnvo3DlpUd85N9l73z2RsmrRi4hsYgoKbJ75449bd3dRDRvCSSdZ9/qaNTZd7Pffg39/\n/x1Wr7ayXbrYD4XytMKzs21u+6mnBlu8PvGE/QB58smyBftHHgmC/E47wRFHxC7nHAwbZmXWr49e\nLe/QQxP4Hj//bGn9r74a95fRKby0MdC/NLYF54w9NqjYTz8l/qWSRIFeRGQT89JLcOONxa/vtx+c\ndZYt6Vq/fvz3e29d0n/9ZQvL1KlT/rpkZ1t9srKCbPlhwyzYDxtm10uzerV1+Ydde23JPxI6dIDb\nbi3kyquiC/Vp9A18W8Oa9ltsYWMMYRMm2K+j0aOLP3D33WGffWzMY8YMTvj9Na7gPgqoyZf0ZD5t\n2Zo/rFVfGbz31f7VrVs3LyKyqXjsMe87dPD+3nvL9/799vPewrX3m2/u/dVXez9zZnLrWFb5+d6f\nempQL/D+tNPsemnuuSd4T4cO3uflxSi0YYP348d7P3So90cc4fM2a+5357uN78siz6+gcfCgrCzv\nW7f2fq+9vN9zz+iKhV8HHeT9J594X1gY/Vnr1vk++6zcWOyOnV72vmtX7++4Iyl/K++9Byb5BGOk\nxuhFRKqRP/6wVnR4VbnffrPd2RL122/QsaMdZ2XBwoXWeK0KCgosUe7554NrJ51kS+nWjNP/nJNj\nf4/wPvBP/3cx5+w3E/7808YlFi2CH3+E776zwhGm0YXdmcgG6tCfd3iHgYlVdOBA6zbYY4+4RUaO\nDDLwt98efvml/EmGsWiMXkQkQz31VPTSscOGxV7XPZ4XXgiO+/SpOkEe7IfHM89YUB82zK69/LJN\n6TvkEEuiO+AAaNoUmDULRo/m+edqsWSJ7VazJYs47eb2QG5Cn7dTi7/4fufr+KrWQRzfajwsH2Cr\n5yxYUHyru6wsOPlkW+t3hx1KffbAgTb8sXat7WY3ZYpNV6wMatGLiCRZfn78FmhF5OTY8PGyZcG1\nli0tLiWyd3phobX+58+389dft/H4qqawEC66yBLzinIUskvtGRy0YQwH8hkX8DjzsB1o7ucyLuPB\n4m8Ka9/eEhHCr223jd/Mzsmx3oAFC+Cff6B7d2jbtkzf49RTg7n0l18ee/ve8ipLi16BXkQkie64\nwxZi2XZbuPpq63rOzk7Os198EU47rfj1UaOiF6CJZ9y4YG55kybWs127dnLqFtOqVdak3XzzMs+X\n8xtyufmKNdzzdGNy8krPyGvGMubX3o76W21mE+ZbtQr+bd/ekuW23LK836Rcxo4NFuDZYgsbJkkk\nuTARCvQiIpXggQes5RapbVsL+GeeCXXrVuz5e+4Z7BDXpo01NsG6tCMXfYnnjDNg+HA7vuACeOyx\nitUnru++s+br//5nzfPsbKvw1lvbHyTy33r1bN7e3Ln2+v13+3fhQigsJIfajGdvPuNAPuUgJrI7\nBTFGnW+7fj033FYnuQPhFZSfb187nD8wdqwNQSSDAr2ISJq99JJ11cbTsqX9CDj/fGjUqOzP//77\nYN322rXh22+tNzk8Xv/rr9aLEM+aNdaqDM81/+67EnPJyq6gwKae3XcffPNNEh8cYcstWdn7WL5s\newqfLt+Fz76sybRpsNtu8NlnwfK2Vcnll9sPQLDemBEjkvPcsgT6Sp8al4yXpteJSGUaM8b7mjWD\nWVd77+397bd737x58RlZm23m/Q03eL90adk+I3Lq2emn27UjjgiuXXllye8fMSIou/32xWeEldua\nNd4/+qj3HTvGnoLWpEns66W9nPN+q61sLuBNN3k/aVLMSm/YkKTvkSKTJwdfqXlz73Nzk/NcNL1O\nRCQ9JkywrvPwuu9dusCXX9oY+Lp1lkV+zz3WEx2pSRNrhe6yS+mf8fff1gUcXsd94kRrzb/3HvTr\nZ9eaN7fPiDfmfuCBwbKvd91lyeNx5eXZsnljx9rYet269qpXLziuW9dWzRk+3JLVImVn29q2l18O\nXbtad8KCBZYFOH++zREMH69bZ1u+tW9v8+TC/269dcVW4qkivLdFiPbbz9bVT1avg1r0IiJp8Msv\n3jdtGrTYtt7a+4ULi5fbsMH7Z5/1ftttoxutnTt7v3Zt6Z/zf/8XvGfPPYPr+fnet2kT3Hv55djv\nnzcvKFOjRuw6bjRhgvc771y+Vvhmm3k/ZIj3ixaV/qWkQihDi17b1IpsIjKg865UH31ka5y//nrq\nP+uPPyyjOtyYbd7cGsBbbVW8bK1alow3Y4YtkR5eXnbmTEvUK0l+fvQ0s4suCo6zsoKd2MDm2Mfy\n4ovB8SGHxK4jq1fDJZfAXnuVfT329u1tDdoFC2xSf5qz26Vk6roX2QQ8/DDcdJMli1VkG9CqbP16\niy///ms9vosXw2abpeazli2zrtiZM+28QQPrFu+eWEcqzz4LZ58dnL//PvTtG7vsm2/a7nBgO60t\nWBDdPf/nn5bAHt5fZcZ0T2c3y/r3GzfG77U3nfZuzpw5dv/ll61XPcro0XDhhdHjC/XqwQ032OIw\n69dHv9ats3/z8+2HQf/+yZs3JglR172IbLRhg/d16wa9qy+8UNk1So1XXonuRX799dR8zurV0Uuf\nZ2d7//HHZXtGYaH3AwcGz9h8c+///jt22QMOCMpdf32MAgUFfuAB/24sc1ndJ6L+EF+z98bTRnU3\n+HXT5gRJbYsWeX/00cW74Pv08X7u3LJ9KUkr1HUvImGTJlnjK+zyy23v7kwTubQrwJgxyX1+To6t\nX77//jY1Daxn5KWX4OCDy/as8Fap4eVn//oLzj23+PDKL78ECXRZWTB4MFbo55/h3nstE69ZMwZ/\nfvzG94xYfyw5BE3+4QzaeHz8+uHU3amjffCAAbYIe+Rm8C1aWJN/zBhLiJPMkOgvgmS9gD7ALGAO\nMCROmeOA6cAvwMulPVMtepH47rqreIPtzDMru1bJtXixJZlFfscttvC+oKDiz54zx/urroo9Ve7x\nxyv27A8+iH7eM89E3z///ODe0T3/9v7yy71v375YRQpwvh1zN156sd653h9+uF+3R0/fiKC1/zV7\nF/8Skf+lWL68Yl9I0oYytOjTHeSzgN+A9kAtYCqwQ5Ey2wI/Ak1C5y1Le64CvUh8ffvG/v/1L76o\n7Jolz333xf6OkyeX73l5ed6//bb3hx4a+7k1a9rWqMlw0UXBc+vX9372bLv+759rff06eRvvfU7P\n+EF68839HV1GbjzdZx/rmn/55aBIx2bLfeFhfS0zPvK9227r/WefJefLSNqUJdCnu+t+D2CO936u\n9z4XeBUYUKTMOcBj3vsVAN77v9NcR5GMUVAQvUhZeGU1sG7gDRvSX6dUiOy2b9IkOH7//bI9p7AQ\n7r/feq0HDrQs/kht28Ltt1tC3JVXlr++kYYOhc6d7XjtWjil37/kH308I7a+kbU5ttTrjvxMT74I\n3tSggS1uP2yY7eK2eDFnfHzSxo10vvnG8csvwXK3AKf/pyluzPs2bjNtmqXoP/20ZdiHF8CXzJTo\nL4JkvIBjgGcizk8FHi1S5m3gbuAbYALQJ86zzgUmAZPatm2b5N9KIpnhxx+DhlurVt4vWOB9gwbB\ntdtuq+waVtyUKcH3qVPH+yefDM579Cjbs0aNKt5Yds77ww7zfvRom7eeCpM//NvXrJG/8TNv4hbf\niZkbz59gsP0HOHiw9ffn5MR8zjHHBPU++uhgOMM57+fPT03dpXJQhVv0iaiJdd/3Ak4Ehjnnik2S\n8d4/7b3v7r3v3qJFizRXUaR6+Oqr4Hj//aF1a/i//wuu3X47G6ddVVeRrfkjj7SGbnijtO++i97S\ntTTPPhscN29uq8f99pvlpvXrl+QZZAUF1uUwcCC7Hd6K2wqv33jrVm7mV7YDoFHtHE75/Gyb+vbk\nk7aJfJzl7wYPDo7D+8mANdjLuMOqZJB0B/pFQJuI89aha5EWAqO993ne+9+BX7HALyJl9OWXwfF+\n+9m/F14I3brZ8YYNtsmKT2A5jSVLbOOUqiQ/3zLhw047DZo2tandYN+raPd7PH/+CR9/HJxPnGhL\nxSY9+Xz6dLj5ZnvwEUfAO+9AQQFXcQ/78WWx4mecV4cGvbontM3rgQdChw7Frw8alIR6S7WV7kA/\nEdjWObeNc64WcAIwukiZt7HWPM655kAnYG46KymSCbwv3qIHa5U+/XQQNz75xGZUxbN4sbUUt9oK\nttsO/vvf1NW5rD7+2Kamgc0YC09zi1x8JtFpdiNHBi3gXr1s+fWk8B6mToUbb7TpbDvuCLfeGuwx\nG5LVcz9eeHAFDRtG/+q64ILEP6pGDZumF6lBA1tjXTZdxTf1TSHvfb5z7iLgIywD/znv/S/OuVux\n8YbRoXu9nXPTgQLgKu99Bs76FUmt2bODINikicWXsN12s9VOH3zQzi+/HA47zFrDYatW2WYs998f\nbNgCcMsttuLcpZem/CuUKrLb/uST2ZiM1rcvXB/qCf/wQ+slL6nb3fvo7UNP32UqvLcg6OoID32H\nj2vUgIYNbb/Zxo3t30aNgk1YvIcffoA33rBXvPGR5s2tuX322bDddrQDHmtqPRMAhx8OnTqV4Q+C\n7Tl/ww22Lw3AMccES+7KJirRwfyq/NL0OpHihg0LErP69St+f9Uq71u3Dsqcc45d37DB+0ce8b5F\ni+KJaZGvESPS+32K+vdfS74L12fq1OBeYaHlroXvjR9f8rN++CEoW5e1fhUNSv7y8V61atmE+5Yt\n45epV8/7Y4+1zL8YSXWFhd6/+KJtO1vWrWzDTj898e8u1RNlSMZLa4teRNInsts+PD4fqWFDePRR\nm0YGNlOrbVubkvXbb9Flu3aF226zqWDh6Xpnnmkt+/79U1L9Ur3xhq1WF67fzjsH95yzVn04uW7M\nmGDcvpgVKxhxzs+A/ZGO4k0asqZ8lcrNjZ3916CBZfMdc4wl09WrF/cRzsEpp9irvB56CDp2tF6c\nuN9bNhna1EYkQ22zDcybZ8cTJkTPoY80cKDlg8XStq1l6Z90kvVWr1gBPXvaNGyw5O+PPrJr6daz\nZ5BseP/9cNll0fcjN4PZbTeYPLnIA3Jy4JFHyPu/u9lq5S8spSUAYzmEQ3qssXGM8O4/zgUvsMH8\n1ath5Uob41i50l75+cHzGzWyZWaPOQZ6986IvdWl6ijLpjYK9CIZaMGCYDpVvXq2o1t2dvyy229v\ni7WENWliY9wXXlg8Pi1eDPvuC3NDKbKNGsG4cbDrrkn/GnH9/rvtjAo29r5wYbBufNiqVdCsWRB7\n//wTWrXCBuxfesmS4xYs4D0Opx/vAbBl9t/88cF0sg7qVfZKeW8/Hlatsj9m69a2P61ICpQl0FfF\nefQiUkGR3fZ77RU/yAO0aWNd+FlZ1kK/6irrur/iitiN0FatLNu9VSs7X7XK9mWvyNS7mTMtyW/X\nXW2Y4eefSy4fub/6oYcWD/JgP0Aihyw+vOFry0DcaSdLgAtlvY/g9I1lTvlPi/IFebDWft26sPnm\n9itEQV6qCI3Ri2SgWNPqSjJokPUuN2xor9K0b29d9vvvb70FS5fCIYfY+H3r1onVce5ceO01e02d\nGn2vRw/LqI81Lcz76Gz7cIY6YK31yZMt4/2HH+g7e0c+x6YHjHluMWfwSNSzVjTryOiVR0Go1X/a\n6S6xyotUI+q6F8lAO+5o67IAfPZZ6pYy//Zbm7senn63/faWsFe3rvUG1K4d/W9uLrz7Lrz6Knz/\nfenPv+EGm7cfuVbM+PGwzz523KiRLeRTd/5Mmx/3wgvWRx8yne3ZEftDNGIly2hONvk23+yKK3iq\n6bWc9x/rtujWzbb0FakOytJ1rxa9SIZZtiwI8tnZ8ZPwkmGvvSzprV8/m7c9Y0b5svBr17ZF4vr0\ngTvvDMb/b78dpkyxIfXGje1aZGv+uJ1mUPeAM4IN4ovYnhlszTzm045VNGb8Gc/Q86hm9kuhSRNG\n7B2UPf30mI8QqfY0Ri+SYb7+Ojju3r3EmVxJceihNmbuytjrnZ1tC8K8+CL8/bdNlzv7bFt6tnfv\noNx779mPlZkzIWd1Hq+NzNt477Rvzi0e5Fu2hBNOgLvvxn38MX3PCAbwx7Q43X5RNGnC7NnWIwG2\n0M4JJ5T1m4tUD2rRi2SYso7PJ8Pxx1vX/MiRlnCek2Pr6OfkRB/n5Vku3Akn2AY0kSvxhTWts44x\nd8/luvq1uPstWxZu1izYY4fVnMJL/OvPB2Ab5rIPoUn92dnWrTBokHULRGQf9s2BJ5634zFjbGgB\nohP6+vYF7Y0lmUqBXiTDxNrIJh0GDLBXuaxbZyv1PPoozJhBFjAU2JXjOZPnWE89VvuGPMH5G99y\nKi9So9tuFtxPPNHm0sVwwAFMq7NfAAAgAElEQVQ2NLBhg2Xz//GHJQxGDgGo214ymQK9SAZZvdoS\nzsG60sNJa1XW8uXw+OPw8MMxV5Q7gdfozEwG8jbzaRd179T3T4S+N5f6EfXr2yY14V3sPvgAOneG\n+fPtvEkTG0IQyVQK9CIZ5Ntvgx3Ydt7ZlqitkubPt+XsnnkmesccsAn97drZGq4dOrBLhw5MajmT\n4x5pzuffNwCsp6Jj38R3e+nbNwj0Y8ZEZ/yfcELc7d1FMoICvUgGiey2T9f4fJlMnWpb4r36qs15\nj9S2ra3Sc+aZtjZ8hObAR8fbb4MpU8q+VW7fvsFue598Er2TnbrtJdMp0ItkkNI2sqkUq1bZqjjD\nh9sk+KK6doWrr4Zjjy1xCb/sbLjmmvJVoWNH2HZb27o3sgOhUyfYY4/yPVOkulCgF6mgDRts3nej\nRpZFXrdu5dUjcqZZpQb6wkL4/HN4/nmbaL9+ffEyBxxgkbt377LPzSuHvn1tV7dIp52Wlo8WqVQK\n9CIVsHix7Yy2ZElwrXZtS/CKfLVqBRdfHL2VarJNnGjBHqylGmv995SbO9da7iNGWHp7UTVr2ry6\nq66C3XdPa9ViBfpTT01rFUQqhQK9SAU880x0kAcLtkuWFL/+0Ue2WUxJG8xURFqn1Xlvgfynn2zc\nPfyaPTt2+Z13hjPOsP1uW7ZMceVi239/Wzwo3HV/wAHBDn8imUyBXqQCRo0Kjps0scVicnNjl12w\nAEaPDvZIT7aULpSzYgV8+KGl9U+dagH+339Lfk/TpnDyyTbPfdddK72PvE4d23jnnXfsXEl4sqlQ\noBcpp+nTg+1U69SxBm79+jYcvWJF8HrhBWv5g00ZT0Wgz8+3nePCktKi//VX24Hm3XdtXd2iWfKx\nZGXZynSDBtlKdVVs3tpdd8HKlbDddvYbRGRToEAvUk6vvx4cH354MCOsXj17bbWVnW+zjeWkFRTY\nTnIzZtgub8k0daotlgO26lu7duV4SH6+ZcWPHm3BvbQN5jfbzLrku3YNXjvuWHnZiAno3NlyBEU2\nJQr0IuUU2W1/3HHxy7VubUvDvvmmnT/5ZPGksIoqOq2uTL3k3lvy3NVX28by8eyxBxx2mO3n2rUr\ntGlT6d3xIlI6BXqRcvjll2Ar2Hr1Sl9C9YILgkA/fDjccYd18ydLuRfKWbIEzj3XWvBF1atng9r9\n+tkXrJQ0fhGpKAV6kXKIbM0fcUTpQfvAA21ceNYsWz/m5ZfhnHOSUxfvy7lQzqhRcP758M8/wbUt\ntoCBAy24H3BAle6GF5HEaD96kTLyPvFu+zDnLKaGPf64PScZPvkk2A+mWbMExv+XL7cF3o8/PjrI\nX3wxzJkDTzxhk84V5EUyggK9SBn9/DPMnGnH9evbsHUiTj89iJ1TpsCECRWvy6pVcPbZwfkRR0CN\nkv5X/e670KWLLUkb1rYtfPqp7SCXzPEEEakSFOhFyiiyNd+vnw1lJ2KzzaKndD3+eMXrcsUVwQJ0\nTZva9LFivIfff7fNYvr3j17J56yzYNo0G1sQkYykQC9SBt5HN4YT6baPFNl9P2pUyUnupRkzJpif\nD/bDYYstQpWcOxeee84Wc996a2jf3ub4hW2xBbz3nj2gUaPyV0JEqjwl44mUQeQqrw0a2NowZbHb\nbtCjh3Xb5+ZaLC7Pjmz//BPdZX9s/xyOXzMSTh0H48bBwoXx33ziifDIIzagLyIZT4FepAwiu+37\n9y9fvtoFFwTj808+CVdeGb0/eiIuucQ21AFo2WAdj3+6A4yeH/8NDRrAvvvC4MGWVS8imwx13UtG\n+/ZbuPfe6OTy8ipPtn0sxx4bNKbnzbMl5Mvirbdg5Mjg/Ok1J9J8bZEg37ChZc7ffbftXbtiBXzw\ngYK8yCZILXrJWIsW2Xova9daF/mECRUbjv7xR9t9DiyOHnpo+Z5Tp47lwN19t50//njpC+6ELV0K\ngwd7wFakO40RDGC03dx+e0u469nTNpGpqf95i4ha9JLB3nvPgjzY+vKnnAKFheV/XmRrfsAAC9jl\nNXhwsHrsBx9Y7lxpvIfzj/6bpUvtjVuxkIe4FGrVgltvtTl7V15p+7wryItIiAK9ZKyiXeLvvgs3\n31y+ZyWr2z6sfftg/r338NRTpbxh+XJe7fkE//sq2Mv9Wc5is/12tgzBG2+0gC8iUoTzyVqeqxJ1\n797dT5o0qbKrIVVIbi40bx7s6BZp1CgbJy+LSZOsoQzQuDH89VfFd2B9/31b4AagWf31LDznVuqs\nWRa9x23o9efK+nRhGitoCsC5tYbz1GP51lVf4go5IpKJnHOTvffdEymr/j3JSOPHB0G+XTtbZ/6j\nj+x80CDo1Mk2YEtUZGt+4MDkbLPepw+02zKXeX/WYvnaurz+4EJO5SUAPPAvm/EXm/MXXbmD6zYG\n+Xb1/+beKYdBx80rXgkRyXhqCkhGiuy2P+wweOUV6NjRztetszH28PrwpUl2t31Y1puvc97S2zae\nX83ddGMSbfiDOuTQlBVsz0x68QVjCTL/nn+vJQ0V5EUkQQr0kpEiA32fPtCkCYwebdnyAPPnW/d9\nXl7pz5o40cqDLWN78MEVrFxBAVx7LRx3HGfmPUktNgCwhFb8QDcW0oZcYncZXHIJ9OpVwc8XkU2K\nAr1knD//tPw0gOxs220VbPbZyJFBtvu4cXDZZaU/L7I1f+SRFcx5++cfm0sXWpS+BcsY3Pi1mEUb\nNoQOHWDvve1zb70V7rmnAp8tIpskjdFLxhk7Njjed9+gFQ+2Cc3tt8P119v5Y4/ZWH28veGT2m0/\nbZoN8EfOpTvsMO4b3o/Df4QNG2Dzze3VsmXim+WIiJREgV4yTtHx+aKuvdamnL/+up1feKFt91qn\njnXl5+baKy/PFqhZsMDKNWkCBx1Uzkq98YZlAYYn9gNcdx3ceivZWVnlXnxHRKQ0CvSSUQoKolv0\nsTadcc42cps92wJ+Xp6tM1Oao46yoYCEeA/Tp9v4wCefwNtvB/fq14cRI+DooxN8mIhI+SnQS0b5\n/nubeg6w5ZbQpUvscvXrW+zt3j2x7HvnoneLK6awMAjs48bBF1/EfnDHjvbBO+5Y+oeKiCSBAr1k\nlKLZ9uHEu1i23ho+/9zWms/NtSS7yFd2dnC81162vWwxCxfCLbfAO++U/ovhiCPghRdsDEBEJE0U\n6CWjFA30penSxQJ9ma1bZ9viDR1qx7E0a2YbzPTqZa8uXUr+5SEikgIK9JIxli2zOe9gq8JWeL57\nLN5bFt9VV8Eff0Tfa97cAno4uO+wg5anFZFKp0AvGePjjy0Og3WzJ72H/Mcf4dJL4auvoq937Qr3\n328T9tViF5EqRs0NyRilTasrt7//ton23bpFB/nmzeHpp2HyZDjwQAV5EamS1KKXjFBYGGxaA4mN\nz5eqoACeeMJW11m1Krhes6atRXvjjbYmrohIFaZALxlhyhTbOhasob3bbkl44LnnBoP+YYcfDvfd\nZ9vhiYhUA+q6l4wQ2W1/6KEVyIFbu9ZWz+nePTrId+oEY8bAe+8pyItItaIWvWSEsk6ri+n99+GC\nC6Kz6WvVsq77a65Jzib0IiJppkAv1d7KlTB+fHDeu3cZH/Dnn5ZN/8Yb0dcPOMDG6NWCF5FqTF33\nUu19+qnlzYElxrdsmeAb8/Lg4Ydt/9rIIN+sGQwfbg9WkBeRai7tgd4518c5N8s5N8c5NyTG/UHO\nuaXOuSmhV0krjIuUb1rdxx/DLrtYSz4yo37QIJg5E04/XdPlRCQjpLXr3jmXBTwGHAIsBCY650Z7\n76cXKfqa9/6idNZNqp5162DYMGjbFvr3h6ys4mW8L+P4/G+/weWXw+jR0dc7dYInn7TuehGRDJLu\nFv0ewBzv/VzvfS7wKjAgzXWQasB7GDgQ/vMf2x52l13g3XeDle/Cpk8P9otv3Bj23DPOA1evhiFD\nbFnayCDfoIGtV//TTwryIpKR0h3otwIWRJwvDF0r6mjn3E/OuTecc21iPcg5d65zbpJzbtLSpUtT\nUVepRK++ar3rYT//bK36ffeFL78Mrke25g85xNayiVJYaHu/d+pkAT03N7h3xhm2Kf3VVyujXkQy\nVlVMxnsXaOe93xn4GBgRq5D3/mnvfXfvffcWLVqktYKSWitXWu96LOPH254xffvamjZxu+29t6Xy\n9tzTxt2XLAnu9ehhG9c/9xxssUUqvoKISJWR7kC/CIhsobcOXdvIe7/ce78hdPoM0C1NdZMq4qab\ngri85ZbW6L7kEtsfPuyDD2DXXeGzz4Jrhx4aOvjqK/s10KcPTJoUFNhyS3jxRfjmG9h995R/DxGR\nqiDdgX4isK1zbhvnXC3gBCAqK8o51yritD8wI431k0r2ww/w6KPB+QMPQMeO8NBD8Ouvlgwfuepd\nYaH926ULtF480aL9/vtHbz5Tpw5cdx3MmgWnnKKtY0Vkk5LW/8fz3ucDFwEfYQF8lPf+F+fcrc65\n/qFilzjnfnHOTQUuAQals45SeQoL4fzzg+B9yCFw7LHB/XbtbHr7Tz9Zol6kwze8CXvsAWPHBhez\ns+2Bv/0G//d/lngnIrKJcb5oGnM11L17dz8psotWqqWnn4bBg+24Vi2YNs1y6OKZ8Mka7jtrOgV/\nLGQY59CMf+xGjRpw2mk2BrDNNqmvuIhImjnnJnvvuydSNqF59M455zPhF4FUWUuX2uy3sGuuKTnI\n8+ef9LjiMF7/46fo68cdB//9L3TunJJ6iohUN4l23c93zt3onNsypbWRTdY118CKFXa8zTZw7bUl\nFJ41C/be2/rww444An78EV57TUFeRCRCooH+M2AIMM8596ZzrqzbhsgmZsMGG2Nv08Y2f1u2LH7Z\nr7+G558Pzh99FOrWjVP4u+9gn31g/nw7z8qyN7/7rq2qIyIiURIK9N77QcCWwJVAJ+BD59xvzrlr\nnHOaxC7FvPUWfPIJLFwId9wBW29t27wvXhxdLi/P8uXCjjzS5sjH9P77cOCBsHy5nderZ6vcDRqU\niq8gIpIREs66996v9N4/7L3vAvQExgO3AAucc68653qlpopSHX37bfT5unVw333WLX/RRcGW7w8/\nbKvegcXtBx+M88Dnn4cBA+xBYDvMffZZCb8KREQEyj+97hvgLWAKUAvoB3zqnPveObd9sion1deE\nCcHxlhGZHRs2wGOPQYcO1hC/+ebg3s032wY2UbyHO++EM88M9qJt184WvYm7sL2IiISVKdA759o4\n524F/gBGAf9im9I0BPoAdYmzZK1sOnJyLC8ubOpU62GPXIwuP9+WoF+71s532ME2sIlSUGDbyF53\nXXCta1dbB1f7xIuIJCShQO+c6+ecew+YC1wAvAJ08t4f5r1/13tf6L3/GLgcUEbUJu7HH23sHWDb\nbaF5c+jXz/Loxo61heuKevxxmzu/0a+/wkEHwSOPBNd69YIvvoBWrYq+XURE4ki0Rf8O0AI4G9jK\ne3+V935ujHK/ASOTVTmpniK77Xv0CI6ds0z8L76w16GHQpMmtmhdz56hQnl51lW/885WKOzYY20H\nm8aN0/IdREQyRUIL5gDdvfc/lFYoFPzPqFiVpLqLF+gj7b9/jJb9pElw1lnR8+OzsmwlnVtv1Rr1\nIiLlkOj/cy5wzsVcp8w518k51zyJdZJq7rvvguN4gT7K2rU2927PPaODfLduFvxvv11BXkSknBJt\n0T8O/AMMjnHvMqAZcFyyKiXV1+LFwVo2devCTjuV8oaPP7YF7n//PbhWty7cdpsl4tVM9L+iIiIS\nS6LNpH2xHediGQvsk5zqSHUX2Zrv1i16D/koublw3nnQu3d0kD/4YJtYf8UVCvIiIkmQ6P+TNgFW\nxrm3CmvRiyQ0Ps+yZXD00fDll8G1Jk1s8/nTTrOsPRERSYpEW/QLgXirk+wJLI5zTzYxpQb66dNt\nLD4yyB9zDMyYAaefriAvIpJkiQb6N4BrnXOHR14MnQ/BFs+RTVx+PkycGJwXC/Qffgh77QVzQzMz\nnYOhQ2HUKNh887TVU0RkU5Jo1/2twP7AaOfcEmARsBWwBTAB+G9qqifVyS+/BEvRt24NW20VuuG9\nLXxz2WVQWGjX6teHl16CgQMrpa4iIpuKhAK9936dc64ncCpwCDYmPwdLxHvJe5+fuipKdRGz2z4v\nDy6+GJ56KrjZpo2tiattZUVEUi7htGbvfR7wXOglUkxkoN9zT+Cff2xFu88+i77x9tuwxRZpr5+I\nyKZI85ckaaJa9Fv+AT0Ohtmzg4snnQTPPgt16qS/ciIim6iEA71zrjdwPrAdUPT/qb33vkMyKybV\ny4oVMHOmHdfMKmS38/eEVUuCArffbrvQKateRCStEt29ri/wAVAP6AzMxLaqbQMUAl/Gf7dURb/8\nAs88Y73ryfD998Fx14IfqBcO8nXrwuuvw/XXK8iLiFSCRKfX3Qg8BvQNnd/gve8F7AhkYT8CpJpY\nuhT22w/OOcfy4X4odbui0n03PsjH7EGoD791a/jmG5snLyIilSLRQN8ZeBdrvXtCXf7e+1+BW7Af\nAlJNjBplXe0ACxbAPvvAK69U4IHLlzPh0UkbT3swwZLuvv8edt21YpUVEZEKSTTQFwL53nsPLAXa\nRtz7E9D4fDVSNKjn5Fie3JAhUFBQxodNn47ffQ8m/BNsbrhnv81h3Dho1arCdRURkYpJNNDPAtqF\njicB/3HOtXLOtQCuAOYlv2qSCvPnW2862FbvnSI2Hx46FPr1g3//TfBhY8ZAjx7M/j2LFTQFoGnd\n9XR8+15l1ouIVBGJBvqRwPah45uxsfmFwBLgQOCm5FdNUuHVV4Pj3r2td/3wiIWNP/jAet3DGfRx\njRxpvwpWr2YCwVq3PQ6oi6uhpDsRkaoioUDvvX/Me3916HgysBO2N/1lwC7e+zdSV0VJppdfDo5P\nPBEaN4Z33oFrrw2u//qrBfv334/zkE8+gUGDNi5nO6HBIRtvxd2xTkREKkWpgd45V8s5d6lzrkv4\nmvd+off+Ge/9w9776amtoiTL9Onw0092XKdOsMx8VhbccYe19uvWtWurVlmD/f77izxkyhQ46ijb\nwQagSxcmbHPCxtsK9CIiVUupgd57nwvcBaFBWKm2IpPw+vWDhg2j7x9/vI3ftw2lWnoPV1xh3fmA\nDfD37QurV9t569ase/NDfpqeDdg0+T32SO13EBGRskl0jH4G0D6VFZHErVkDd91luXCJ8j662/6k\nk2KX23VX22p2n32Ca2ecAX/PWgGHHQaLF9vFxo3hgw+YvGSrjZn6nTvbZRERqToSDfQ3ATc653ZK\nZWUkMYMG2Zj64YfDp58m9p6JE4Nt4Bs3tpgdT8uW8NZbwb4zf/0F5+z9C37GDLtQq5ZtTNOlS+wd\n60REpMpINNBfAzQAfnTOzXHOfeWc+zLi9UUK6ygRpk2D//0vOL/88sTmvke25o86CmrXLrl8ixbw\n/PPB+eh/9mUY59jJiBHQqxcQZ2taERGpMhIN9AXAdOArYAGQH7oWfhWmpHZSzJ13Rp//9JPF3ZIU\nFMBrrwXn8brti+rTBy7eJdjG4DIe4NdrnoUTLPnOe/j226C8Ar2ISNXjbLG76q179+5+0qRJpRes\n5ubMge222zirbaNWrWxKXIMGsd/36adw8MF2vPnmsGiRZdqX6v77WX/F9ezORH7BJl107+4ZP96R\nnW3L54YT9+rXh5UrE3yuiIhUiHNusve+eyJlE23RSxVw991BkO/ZM1hhdvFiuPfe+O+LzLY//vgE\ng/HIkXDFFdQlh5GcTK0aeQBMmuT473+tyHffBcX32ENBXkSkKkp0m9r9S3uluqKbukWLYPjw4PzW\nW22L97B77oE//yz+vg0b4I2I5YxOPDGBD3v4YTj11I2nXfdtxP9FfNadd8LXX0ePz++5ZwLPFRGR\ntKuZYLlx2K51JVF7LoXuuw/yrFHNPvvYNrP77AMPPWTj9OvWwQ03wHPPRb/vww+tSx1gm21KCciF\nhZbOf/fdwbUddoB33uHyzbL54BP47DMrdsop0KRJUEzj8yIiVVOiXfcHYGvaR76OBUZgG9ockYrK\niVm2DJ56Kji/7jpbnCYry34AhA0fbgvXRSq65K2Ltwx9bi6cfnp0kO/RA778Epo2pUYNS/oLB/f5\n86M/Sy16EZGqKdG17r+I8XrTe38mMBrol9pqbtoeesha7ABdu0bPgT/44GBTmvBKduH8yjVr4N13\ng7Jxs+1Xr7al8l56KbjWv79l8TVrtvFS69bRPzjC2rUL5tyLiEjVkoxkvPeB45LwHIlh1Sp45JHg\nPNyaj3TPPUEi3GefBZvRvPMOrF9vxzvtBDvuGOMDliyxOfFjxwbXzj3XJuvXq1es+LHHWsM/krrt\nRUSqrmQE+u3QPPqUeeKJYIy9Uyc4+ujiZbbf3mJz2FVX2Xh+0W77YmbPhr33hh9+CK7997/w5JNQ\nM376xsMP23h/2F57JfZdREQk/RJKxnPOnRbjci2gC3AW8GYyKyVm/fro3eOGDIk/he2WW6znffVq\n20v+rruiG+knnFDkDeGN6Jcts/MaNaxf/uyzS61Xo0bw5pu2FG/z5nDWWWX5ViIikk4JLZjjnIvX\nYt8AvAZc6r1fmcyKlUWmLpjz6KNw8cV23KaNLZhTq1b88nfdFb2vfNhee8H48REX5s2zwf5Vq+y8\nbl1bOq+fUi1ERKqDsiyYk+j0um1iXMvx3v+VeLWkLHJzoxPgr7qq5CAP8J//WFf/H39EX49Kwiss\ntO3owkG+WTN47z0NtIuIZKhEs+7nx3gpyKfQyy/bErNgG8wk0j1ep07xtfBr1LAEuo0efRTGjQtu\nKsiLiGS0RFfGO8I5d1Gcexc65/omt1qbtoIC64YPu+yymAnwMZ1wgi1HG3bwwba+PQCzZsE11wQ3\nhwxRkBcRyXCJZt3fCNSPc69u6L4kyVtvWUwG2zv+ggsSf2+NGvDYY7awTXY2XH996EZ+vmXP5eTY\n+c47w003JbPaIiJSBSU6Rt8Z+CHOvSnADcmpjuTm2jr2YRddZMG+LLp3t5lzELHezb33BovTZ2fD\nCy+Uvim9iIhUe4kG+hpAnE1QaQhkJ6c6cvPNMG2aHdetC5deWr7nRCxoZ4vhR7beb7nFsu5FRCTj\nJdp1PxU4Oc69k4GfklOdTdvnn8PQocH57bdbIl6F5ObCaacFO+LssQdcfXUFHyoiItVFoi36+4D/\nOedeB4YBC4GtgHOBI7ENbqQCli+3nWHDyxr07m3T5Srstttg6lQ7rlPHdqYpYdU7ERHJLIlOr3sL\nuBQ4FPgAmAZ8FDq/xHuf8Mp4zrk+zrlZzrk5zrkhJZQ72jnnnXMJLQhQnXkP55xje86DrTY3fLgl\n1lXI999Hz7e7807o3LmCDxURkeok4aad9/4R59xwYG+gGbAMGO+9X5PoM5xzWcBjwCFYr8BE59xo\n7/30IuUaYj8svkv02dXZsGGWaR/2/PPQqlUFH7p+ve0+U1Bg5z17wiWXVPChIiJS3ZSpD9d7vxpr\nyZfXHsAc7/1cAOfcq8AAYHqRcrcBQ4GrKvBZ1cLMmdFd9BdeCEcckYQHX3+9PRygQQP79VDhLgIR\nEaluEl0w5xrn3CNx7j3snEs0IG8FLIg4D4/1Rz5vN6CN9/79BJ9ZbW3YYLvKhbeS3XFH23K2wr76\nCh58MDi///7o7eZERGSTkWgT7wziZ9ZPCd2vMOdcDeB+4IoEyp7rnJvknJu0dOnSZHx82l13HUyZ\nYse1a8Mrr9iUugrJybEd6MJZfX36JLQjnYiIZKZEA31bYHace3OBrRN8ziKgTcR569C1sIbY1rfj\nnHPzgB7A6FgJed77p7333b333VtUeA5a+o0dG70F7T33wE47JeHBt94Kv/5qxw0bWgKAc0l4sIiI\nVEeJBvp1FOlij9Aa2642EROBbZ1z2zjnagEnAKPDN733K733zb337bz37YAJQH/vfUbtQfv33za1\nPaxvX1sBr8KmTIne8u7uu6F16yQ8WEREqqtEA/1XwFXOuag1U0PnV4Tul8p7nw9chCX0zQBGee9/\ncc7d6pzrn3i1qy/vbSe6v0J7/22+ueXJVbjRnZ9vDw5n2e+/P5x7bgUfKiIi1V2iWfe3AOOBX51z\nL2Hd7VsBp2BT7QYl+oHe+zHAmCLXYu6u4r3vlehzq4vx421n2LDhw6FlyyQ8+P774YfQdgS1a1uX\nvbLsRUQ2eQkFeu/9VOfcAcC9wDVYT0Ah8DVwtPd+auqqmFm+/jo4Pukky5WrsNmzbZH8sFtugU6d\nkvBgERGp7hJu8nnvv/fe748lzLUGGoZa3PWdc8+lqH4ZZ+LE4LhXryQ80Hvrog9vP7vLLnBFqZMW\nRERkE1Hmvl3v/XqgHnCtc+534HPguGRXLFNNikgr7J6MxX2feQbGjbPjrCx49lnbhlZERIQyBHrn\nXOPQ3PVvgFnA9cAK4HxgyxTVL6MsXQrz59tx7drQpUsFH7hoEVx5ZXB+5ZWw224VfKiIiGSSEgO9\nc66Gc66vc+41YDHwJDZn/rFQkf9475/y3q9KcT0zQmRrfpddKtjw9t7Wy10V+tNvu230OL2IiAgl\nJOM55+4DTgJaAjnAW8AI4BOgETZNTsogMtDvvnsFH/bGG/DOO8H5sGFJWFZPREQyTUlZ95cBHpsK\nN8h7vzx8wznnU12xTBSZiFeh8fl//oleYWfwYNudTkREpIiSuu6fBVYDhwOznHOPOuf2SE+1MlNS\nWvTew8UX2/J6AFtuCUOHVrhuIiKSmeIGeu/9OcAWwMnAJGAw8K1zbgY2l16t+jJYtAgWL7bj+vVh\nu+3K+aAXX4SXXw7On3gCGjeucP1ERCQzlZiM573P8d6/4r3vg21scy1QAAwBHHCXc+4U51yd1Fe1\neotszXfrZjPhymz2bEvACzvzTOi/SawcLCIi5VSWBXMWe+/v9t53AfbAMu+3BV7AMvKlBBWeP5+b\na5vXr1lj5506wcMPJ4PtzFcAACAASURBVKVuIiKSucq1GLr3fpL3/mJs/vzRwLhkVioTVTgR74Yb\nYPJkO87Ots3r69dPSt1ERCRzJbqpTUze+zxs2t1byalOZvK+gol4Y8fahvVhQ4dqYRwREUmItjdL\ng3nzYHlocuJmm0GHDmV4c9HN6/v0gUsvTWb1REQkgynQp0HR8fmE9573Hs44I3rz+uHDtf2siIgk\nTBEjDco9Pv/wwzBmTHA+YoQFexERkQQp0KdBucbnf/wRrr46OL/iCjj00KTWS0REMp8CfYoVFgbJ\n8pBgi37tWptKl5tr57vtBnfckZL6iYhIZlOgT7HZs4MN5lq2hDZtEnjTZZfBrFl2XL++TaWrVStl\ndRQRkcylQJ9iZU7E++Yb24ku7JFHbHEcERGRclCgT7EyJeIVFETvSnfkkTBoUCqqJSIimwgF+hQr\nUyLesGEwZYod160LDzxQhrl4IiIixSnQp1B+PvzwQ3BeYot++XK4/vrgfMgQ2HrrlNVNREQ2DQr0\nKTRjBqxfb8etW8MWW5RQ+MYb4Z9/7HibbeCqq1JePxERyXwK9CmU8I51U6bAU08F5w88YF33IiIi\nFaRAn0KRiXhxx+e9h4svtgn3YIviaI95ERFJEgX6FEqoRf/yy/D113acnQ0PPaQEPBERSRoF+hTJ\nzYWpU4Pzbt1iFFq9Onos/j//ge22S3ndRERk06FAnyLTpgUr2LZvD82axSh0222weLEdt2plCXki\nIiJJpECfIqV228+aBQ8+GJzfcw80bJjyeomIyKZFgT5FSkzE8x4uvRTy8ux8333hpJPSVjcREdl0\nKNCnSIkt+tGj4aOP7LhGDVvPXgl4IiKSAgr0KbBuHfz8sx07Z7vMbpSTY7vThQ0eDLvsktb6iYjI\npkOBPgWmTrX9acCS6Bs1irg5ciT8/rsdN21qCXkiIiIpokCfAnHH572HRx8NzocMiZOOLyIikhwK\n9CkQd3z+22+jd6c766y01ktERDY9CvQpEDfQP/ZYcHzSSdZ1LyIikkIK9Em2ejXMnGnHWVkReXZ/\n/QWvvx4UvPDCtNdNREQ2PQr0SfbDDzYUD7DjjlCvXujGsGHBvPm99oJdd62U+omIyKalZmVXIFMs\nX24J9U88EVzbmIiXnx+9Da1a8yIikiYK9BVQWAiffgrPPgtvvRWsbR+2zz6hg9GjYeFCO27ZEo45\nJq31FBGRTZcCfTn88QcMHw7PPQfz5xe/X68eDBoEJ54YuhA5pe6cc6B27TTUUkRERIG+zJ54Ai66\nyFrzRe2+O5x9NpxwQsQiOdOnw+ef23GNGrYSnoiISJoo0JfRXXdFB/mmTeHUU21K/E47xXjD448H\nxwMGQJs2Ka+jiIhImAJ9Gaxebd32ADVrWvLdgAEl9MSvWgUjRgTnF12U8jqKiIhEUqAvg/D8eIBO\nneC440p5w4svwpo1drz99nDAASmrm4iISCyaR18G06cHx9tvX0ph76NXwrvgAm1FKyIiaadAXwaR\ngX6HHUopPG4czJhhxw0awGmnpapaIiIicSnQl0E4bkMCgT6yNX/qqUX2qhUREUkPBfoySLjrfuFC\nePvt4Fwr4YmISCVRoE/Q+vUwd64d16hhyXhxPfUUFBTYca9etui9iIhIJVCgT9Cvvwab1bRvb9vJ\nx7RhAzz9dHCu1ryIiFQiBfoEJdxt/+ab8PffdrzlljbRXkREpJKkPdA75/o452Y55+Y454bEuH+e\nc26ac26Kc+5r51xpaW9pkXDG/csvB8eDB0N2dsrqJCIiUpq0BnrnXBbwGHAYsANwYoxA/rL3fifv\n/S7A3cD96axjPAll3K9eDWPHBucnn5zSOomIiJQm3S36PYA53vu53vtc4FUgqm/be78q4rQ+4NNY\nv7gS6rofMybYq7ZrV+jQIeX1EhERKUm6l8DdClgQcb4Q2LNoIefchcDlQC3gwPRULb68PJg9Ozjv\n3DlOwTffDI6POiqldRIREUlElUzG894/5r3vAFwD3BCrjHPuXOfcJOfcpKVLl6a0PnPmQH6+Hbdt\nCw0bxiiUkwPvvx+cK9CLiEgVkO5AvwiI3Ke1dehaPK8CA2Pd8N4/7b3v7r3v3qJFiyRWsbiEuu0/\n/hjWrrXjjh01d15ERKqEdAf6icC2zrltnHO1gBOA0ZEFnHPbRpweDsymkiWUcf/WW8HxUUdpAxsR\nEakS0jpG773Pd85dBHwEZAHPee9/cc7dCkzy3o8GLnLOHQzkASuA09NZx1hKzbjPz4d33gnO1W0v\nIiJVRNr3o/fejwHGFLl2U8TxpemuU2lKbdF/+SX8848db7UV7L57WuolIiJSmiqZjFeVFBTAzJnB\necwx+shs+yOPtMXwRUREqgBFpFLMm2fL1wNssQU0aVKkQGFh8fF5ERGRKkKBvhSldtt//z38+acd\nN2sG++2XlnqJiIgkQoG+FKVOrYvsth8wAGqmPe1BREQkLgX6UpTYovdeq+GJiEiVpkBfihKn1k2b\nBr/9ZscNGsBBB6WtXiIiIolQP3MJvC+l6z4yCe/ww6FOnbTUSySelStXsmzZMnLDmyuJSLVRq1Yt\nmjdvTuPGjZP6XAX6EixYEKxq27QptGxZpIC67aUKycnJ4a+//qJ169bUrVsXp9UZRaoN7z3r169n\n4cKF1K5dmzpJbDiq674ERbvto/5/c84c+OknO65dGw47LK11Eylq6dKltGjRgnr16inIi1Qzzjnq\n1atH8+bNSfZGbQr0JUi427537zhb2omkT05ODg0aNKjsaohIBTRs2JCcnJykPlOBvgQlZtyr216q\nmPz8fGpqeqdItVazZk3yw/uiJ4kCfQniZtwvWgQTJthxVhb065fWeonEoy57keotFf8bVqCPo8SM\n+7ffDo579bIV8URERKogBfo4/voLVqyw4wYNoHXriJvqthcRkWpCgT6OuBn3y5fDF18ENwcMSGu9\nRKRyDBkyBOccS5YsKdf7c3JycM5x3nnnJblmIiVToI8jbiLeu+/a3rUAPXrY/vMikhbOuYRf8+bN\nq+zqVnk//vjjxr/XxIkTK7s6kiJK0Y0j7vi8uu1FKs2LL74Ydf7VV1/x9NNPc+6557JfkZ0jW7Ro\nkdTPvv3227nlllvKvZBJnTp1WL9+fZWaGfH/7d15XJVV/sDxzwHZRTBF3BUXInPfxlzQ0cTUsczU\ncikXSlsnG3Ny2u2nafUbp2YccwNcQslQy9FyS9PMsVKzDJefiBVjOiqkaCwqfH9/3KV7uRcFRS/g\n9/163Rf3Oc+5z/O9By7nnuU5T1xcHFWt996Oj4+nQ4cOHo5IXQ9l5y+ujHE7414EPv/8tx3aba/U\nDTVy5Ein7UuXLjFv3jzuuOMOl31FERGys7MJCgoq0bkrVap0zZV0aa52dq1yc3NJTExk+PDhiAhL\nly5l5syZBAQEeDq0Kzp37hzBunZJsWnXfRHcdt3/8AOcOWN5XrUqNG16o8NSSpXAunXrMMawbNky\n3nnnHaKiovDz8+Mf//gHADt27OChhx6iadOmBAYGUqVKFaKjo1mzZo3LsdyN0dvSjh49yqRJk6hT\npw7+/v60bduWjRs3Or3e3Ri9Y9q2bdvo2rUrgYGBhIWF8eijj5Kdne0Sx6ZNm/jd736Hv78/tWrV\nYuLEifYu+BkzZhS7bFauXMmZM2cYNWoUo0eP5uzZs6xYsaLI/ElJSURHRxMSEkJgYCBRUVFMmDCB\nfNtQJlBQUMDs2bPp0KEDlStXJjg4mFatWjF16tTLlqNNzZo1ueuuu9yWz7p16+jcuTNBQUEMGTIE\ngPT0dJ555hlatWpFaGgoAQEBNG/enJkzZ1JQUOBy/NzcXF5//XVatmxJQEAAoaGhdOzYkblz5wIw\nffp0jDF87tigs/r111+pUqUK/fr1K0bpli3aoncjM9My6x4s96lp0MC645tvfsvUpk2hNXGVUmXV\nG2+8wdmzZxk7diw1atSgUaNGAHzwwQekpaXxwAMPUL9+fU6dOsXChQsZMGAAK1asYFAxh+eGDRtG\nQEAAf/7zn8nJyeFvf/sbd999N6mpqdQpxjyer776ig8++ICHH36YkSNH8umnnzJ37lx8fX35+9//\nbs/36aef0rdvX2rUqMHzzz9PcHAwSUlJbHWcIFxMcXFxREVF0bFjRwBuu+024uPj3faMTJw4kZkz\nZ9KiRQsmTpxIeHg4qampJCcnM2PGDLy9vRER7r//fpKTk+nSpQsvvvgiISEh7N+/n+TkZF588cUS\nx2jzxRdfsHTpUsaNG8eYMWPw9vYGYPfu3fzrX//innvuoXHjxuTl5bF27VomTpzIjz/+yDvvvGM/\nRm5uLr169WLHjh307duX0aNH4+Pjw3fffceHH37I+PHjGTNmDC+//DLx8fEuQ0EffPAB586d4+GH\nH77q9+ExIlLuH+3atZPStH27iKWfXqR1a4cdL7zw245nny3Vcyp1rfbv3+9+h+1vtiw+rlFCQoIA\nkpCQ4Hb/J598IoCEhYVJRkaGy/7z58+7pJ07d04iIiKkTZs2TunPPfecAHL8+HGXtEGDBklBQYE9\nfdu2bQLIq6++ak/LyckRQMaPH++S5u3tLXv27HE6X8+ePcXPz09yc3PtaS1btpTAwED56aef7Gl5\neXnSrl07AWT69Oluy6GwtLQ0McY45Z8xY4YYY+TIkSNOebdu3SqA9OnTR/Ly8pz2Ob7nRYsWCSCx\nsbFO6SIi+fn59ufuytEmPDxc+vTpY9+2lQ8g27Ztc8n/66+/upxLRGTw4MHi4+Mjp0+ftqdNmTJF\nAJkyZYpLfsf47r33XgkKCpKsrCynPF27dpUaNWrIhQsXXF5f2or8LDsAdkkx60jtunejyBn3e/b8\n9rxt2xsWj1Lq2owdO5ZbbrnFJd1xnD47O5uMjAxyc3Pp3r07e/fuJS8vr1jHnzBhgtOKZl27dsXX\n15fDhw8X6/Xdu3enTZs2Tmk9e/YkLy+P9PR0AH788Ue+++47Bg8eTL169ez5fH19+eMf/1is89gk\nJCRgjOHBBx+0pz344IN4eXmRkJDglDcxMRGw9Ir4+vo67XN8z4mJiXh7e/Pmm2+6rO7m5XVtVc3v\nfvc7lxY24HQDp7y8PDIzMzl9+jR9+vTh4sWL7HH4n52YmEiNGjX4y1/+4nIcx/jGjRvHr7/+SlJS\nkj3t0KFDbN++nYceeggfH59rei+eoBW9G0XOuC/cda+UKhciIyPdph8/fpyxY8cSFhZGUFAQ1atX\nJywsjIULFyIinD17tljHtw0F2BhjqFq1KhkZGVf1eoBq1hU3bcc4evQoALfeeqtLXndpRSkoKGDh\nwoW0b9+enJwcUlNTSU1NJTs7m44dO7Jw4UKn8e3Dhw/j4+ND8+bNL3vcw4cPU79+fbdfqK5VUb+/\nCxcu8Oqrr9KkSRMCAgKoVq0aYWFhPPLIIwD8Yl31TEQ4cuQIt99++xUr6piYGBo2bEhcXJw9LT4+\nHqB8dtujY/RuuZ1xf/w42CaPBAXpRDxVfoh4OgKPCwwMdEnLz8+nV69eHD16lKeffpp27doREhKC\nl5cXc+fOJTk52e2ELndsY8aFSTHLvqjXl+QYxbVhwwbS09NJT0+naRH/xzZs2OA0Ka40XW4t96Ju\n5uLu9wfw5JNPMn/+fEaMGMHLL79MWFgYPj4+7Ny5k5deeqnYvz9HXl5exMbG8tJLL5GSksKtt97K\n4sWL6dq1a4m+UJUlWtG74bbr3rE136qV5WY2Sqlya9euXRw4cIDXX3/dpTt31qxZHoqqaA0bNgQs\n3ciFuUsrSnx8PEFBQSxcuNDt/rFjxxIXF2ev6CMjI9myZQspKSm0bNmyyONGRkayadMmMjMzL9uq\nt+3LzMykZs2a9vSsrKxi94DYvPfee8TExPDee+85pX///fdO28YYmjRpQkpKChcvXrxiq37s2LG8\n+uqrxMXF0b17d06cOMH06dNLFFtZol33hZw7B9YhMSpVgsaNrTt0fF6pCsXWii7cYt6zZw9r1671\nREiX1bBhQ5o3b05ycrJ93B4s3deOM/MvJyMjg48++oh+/foxePBgt4/+/fuzevVqe6U7fPhwwHJZ\n3MWLF52O51h2I0aMID8/n8mTJ7uUqeO2rRt+06ZNTnn++te/Fus9OB6zUqVKLufKyspymm3vGN/J\nkyd588033R7LUe3atenfvz9Llizh3XffpUqVKgwdOrRE8ZUl2qIv5ODB355HRoL9i59W9EpVKC1b\ntiQyMpKpU6dy5swZmjZtyoEDB5g/fz4tW7Z0mshVVsycOZO+ffvSqVMnHn30UYKDg1m2bJm9O/xK\ntzhdsmQJFy5c4L777isyz3333UdSUhJLlixhwoQJREdH8/TTT/POO+/Qvn17hgwZQnh4OGlpaSxf\nvpyUlBT8/f0ZOXIkK1euZP78+Rw4cIABAwZQpUoVDh06xNatW+3l2a9fPyIiInjuuec4ceIE9erV\nY+vWrezdu5eQkJBil4UxhkGDBrFo0SJGjBhBjx49OHHiBAsWLKBGjRouSyBPmjSJtWvX8uKLL/Lv\nf/+bXr164evry759+/jpp5/4+OOPnfKPGzeO1atXs379esaPH1/k8EF5oBV9IUXOuNeJeEpVKL6+\nvnz88cdMmjSJ+Ph4cnJyaNGiBcuWLWP79u1lsqLv3bs3H3/8MS+88ALTpk2jatWqDB8+nIEDBxId\nHX3FVe3i4+Px8/Ojf//+Rebp27cvAQEBxMfHM2HCBADefvtt2rVrx+zZs5kxYwYiQv369Rk4cKC9\nG9wYQ3JyMrNmzSIhIYFXXnkFHx8fGjVq5NQa9vHxYc2aNTz99NO8/fbb+Pn50a9fPz777DNat25d\novKYNWsWoaGhrFy5khUrVtCgQQOeeuopmjVr5vIe/f392bJlC2+++SZJSUls3LiRwMBAIiMj3U6y\n69u3L/Xq1SM9PZ3Y2NgSxVXWmNKe6OEJ7du3l127dpXKsZ57Dmw9Oy+9BK+9hmUFHds95319Lf37\nhS4zUcrTDhw4wG1Ol4mom0ViYiIjR45k1apVDBw40NPhVAgiQtOmTQkKCuLbb7+9oecuzmfZGLNb\nRNoX53g6Rl+I2xn3e/f+lti8uVbySimPKCgo4MKFC05peXl59pZxdHS0hyKreD755BOOHDnCuHHj\nPB3KNdOu+0Lcdt3r+LxSqgzIysritttuY8SIEURGRnLq1CmWLVtGSkoKr7zyynW5hv1ms2nTJo4c\nOcK0adOoXbs2Y8aM8XRI10wregc5OZCWZnnu5WWZjAdoRa+UKhMCAgKIiYlh5cqV9pvCREVFMW/e\nPPsiMeravPjii+zevZvmzZsze/bscj0Jz0YregeHDv22tkijRpYb2gA6EU8pVSb4+fmxaNEiT4dR\noe3cudPTIZQ6HaN3YAwMGGC5dt6+2uP585ZvAGBp5l9mwQillFKqrNEWvYNWrWD1astz+8qJ3333\nWzM/KgoqQDeOUkqpm4e26Itgv5mRjs8rpZQqx7SivxKt6JVSSpVjWtFfiU7EU0opVY5pRX85eXng\neBekEi7PqJRSSnmaVvSXk5ICtvsjN2oEoaGejUcppZQqIa3oL0fH55VSSpVzWtFfjlb0St10unbt\nSpMmTZzSRo4cSaVKxbsaOTU1FWMMU6dOLfXYLl26hDHG7d3WlCqKVvSXoxPxlCpThgwZgjGGvY43\nmipERIiIiCA0NJScnJwbGF3pyMzM5NVXX2Xbtm2eDqVYJk6ciDGGqKgoT4eiiqAVfVHy88Hx1oRa\n0Svlcbb7gickJBSZZ8uWLfzwww888MADV7w/e3ElJCTw66+/lsqxriQzM5MpU6a4regrVapETk4O\nc+bMuSGxXMnFixdZsmQJjRs35tChQ3zxxReeDkm5oRV9UQ4dstzlBqB2bQgP92w8SiliYmKoV68e\niYmJLrdrtbF9CbB9KSgNPj4++Pn5ldrxroW/v3+xhxGut9WrV3Pq1Cni4uKoVq0a8fHxng6pWPLz\n88nOzvZ0GDeMVvRF0fF5pcocLy8vRo8eTUZGBqtt61U7yMrKYsWKFTRv3pwOHTrY05cuXcqAAQOo\nX78+fn5+hIWFMWjQIL53vHz2Mooao9+2bRudO3cmICCAmjVr8sc//tFty//SpUtMnTqVbt26ER4e\njq+vLw0aNOCJJ54gMzPTnm/Tpk00bdoUgJdeegljDMYY+5yBy43Rz507lzZt2hAQEEBoaCh9+vRh\nx44dLnHYXr99+3a6detGYGAg1atXZ9y4cSXutYiLiyMyMpLu3bszfPhwli9fzvnz593mPXv2LM8/\n/zxRUVH4+/tTrVo1unXrxvLly53yHT9+nCeffJKIiAj8/PwIDw8nJiaGzZs32/PUrVuXO++80+Uc\nmzZtwhjDe++9Z09bsGABxhi2bNnClClTaNSoEX5+fqxcuRKAdevWMXToUCIiIvD396dq1ar06dOH\nzz//3O37OHz4MKNGjaJu3br4+vpSu3ZtBg4cyDfWod7bb7+diIgIxLZ0uoNly5ZhjGHp0qVXKNnS\nVTa+FpZFWtErVSaNGTOGqVOnkpCQwODBg532JSUlkZOT49KanzVrFuHh4YwfP57w8HBSU1OZN28e\nnTt35ptvvqFx48YljmPHjh307t2b0NBQJk+eTJUqVVi2bBnbt293yZubm8tf//pX7rvvPgYOHEhQ\nUBBfffUV8+bN44svvuDrr7/Gx8eH5s2b87//+788++yzDB48mHvuuQeA4ODgy8YyceJEZs6cSadO\nnZg+fTpnz55l7ty59OjRgzVr1hATE+OUf/fu3axatYrY2FhGjhzJ5s2bmT9/PpUqVWL27NnFev/H\njh1j/fr1vPbaawCMHj2af/zjHyxfvpyxY8c65c3MzKRLly4cPHiQoUOH8vjjj5Ofn8/u3btZu3Yt\nQ4cOBSAtLY0uXbpw6tQpRo8eTdu2bTl//jw7d+5k06ZN9OzZs1ixufPMM8+Qn5/PuHHjqFKliv0L\nVXx8PGfOnGH06NHUqVOH//znPyxYsICePXuydetWOnfubD/Gl19+Se/evcnPzyc2Npbbb7+djIwM\nPvvsM3bu3EmbNm145JFHeOaZZ9i8eTO9evVyiiEuLo6qVasyaNCgq34fV0VEyv2jXbt2Uup69BCx\n3M5GZNWq0j++UqVs//79btNtf8Zl8XG1evbsKd7e3vLzzz87pXfq1El8fX3l1KlTTunnz593Oca+\nffvEx8dHnnrqKaf0Ll26SOPGjZ3SRowYId7e3k5pHTp0EF9fXzl8+LA9LTc3V9q2bSuA/M///I89\nPT8/X7Kzs11imDNnjgCyYsUKe9rhw4ddXm9z8eJFASQ2NtaelpKSIoBER0fLhQsX7Onp6ekSHBws\njRo1kvz8fKfXe3l5yddff+107JiYGPH19XUbpztTp04VY4z8+OOP9rQWLVpI586dXfI+8sgjAkhc\nXJzLPltsIiK9e/cWY4xs2rTpsvnq1KkjvXr1csmzceNGAWTJkiX2tPnz5wsgt912m9v35u5v4+ef\nf5aqVavKgAEDnM4fFRUl/v7+8v333xcZX0ZGhvj7+8uwYcOc9qelpYkxxuXvzZ2iPsuOgF1SzDpS\nu+7dEXGeca8teqXKlNjYWPLz81m8eLE97eDBg+zcuZO7776b6tWrO+UPCgoCLA2brKwsTp8+Tc2a\nNWnSpAlffvllic//888/8/XXXzNo0CCnS/H8/PyYMGGCS34vLy/7xMD8/HzOnDnD6dOn7S3Uq4nB\n5sMPPwTgueeew8fHx55et25dRo0aRVpaGt99953Ta7p27Ur79u2d0nr27MmFCxf48ccfr3hOESE+\nPp7f//731K9f354+atQoduzYwSHbrb2xvN/333+fFi1auLT0wVI2AKdOnWLjxo3079/fpSXsmO9q\nPf74424nZ9r+NgDOnz9PRkYGPj4+dOzY0en3snv3bg4ePMjDDz/M7bffXmR8t9xyC/fddx+rVq3i\nl19+se9PSEhAREp17khxaUXvztGjcPas5fktt0C9ep6NRynlZNCgQYSGhjrNvrdNBHNXmezevZt+\n/foRHBxMSEgIYWFhhIWFceDAAad/xsWVlpYG4PaSsmbNmrl9TVJSEh06dCAgIICqVasSFhZGZGQk\nwFXFYHP06FEAt5WPLc0Wr02jRo1c8larVg2AjIyMK57zs88+Iy0tjV69epGammp/dOrUCWMMcXFx\n9rz//e9/ycrKovUVlhA/fPgwAG2u0xVOtrIuLDU1lfvvv5/Q0FCCg4OpXr06YWFhrF+/3un3UpL4\nxo0bR25uLomJiQAUFBSwcOFC2rdvT6tWrUrh3ZSMVvTuFB6fN8ZzsSh1jTzfQV/042r5+/szfPhw\nDh06xI4dO8jPz2fJkiXUrVuXPn36OOX94YcfiI6OZt++fbz88susWrWKDRs2sHHjRqKioigoKLjG\nEr6y5cuXM2zYMCpVqsTf//53/vWvf7Fx40bWrl0LcENicOTt7V3kPinGL8ZWkb/wwgs0bdrU/uja\ntSsiwpIlS7hkWz78OjBF/E++3DkDAwNd0rKysujWrRsbNmzgmWeeITk5mfXr17Nx40a6d+9+1b+X\n6OhooqKi7OW0YcMG0tPTPbbQkU7Gc0cn4ilV5sXGxjJ79mwSEhLIzMzkxIkTvPDCCy5dvCtWrCA7\nO5t169bRrVs3e7qIcPr0aUJCQkp8bluL+ODBgy779u/f75K2ZMkSAgMD2bJlC/7+/vZ0d7P+i6rE\nrhRLSkoKDRo0cBuLuxb81Tp79iwrV67krrvuctsNvXfvXqZNm8batWu55557CA8Pp0qVKpdd5Aiw\nT467Uj6wdI87Xq1gU7jn4ko2btzIiRMnWLx4MQ8++KDTvsmTJztt23oEihMfwCOPPMLEiRPZs2cP\ncXFxBAYGMmzYsBLFV1q0Re+OroinVJnXtm1bWrduzfvvv88///lPjDFuu+1trdfCLdU5c+Zw+vTp\nqzp37dq1ad++JuMKhQAADZ1JREFUPatWreLIkSP29Ly8PN5++223MXh5eTm1EEXE7TK5lStXBnBb\nkbljm5n/1ltvObVojx07xqJFi2jUqBEtW7Ys3hsrhqVLl5KTk8Njjz3G4MGDXR6TJ0/G39/fPpTi\n7e3NAw88wL59+1i0aJHL8Wy/l7CwMGJiYlizZg1btmwpMh9YKt39+/dz/Phxe1pubm6xrxiwKepv\n45NPPmH37t1OaW3btiUqKooFCxZw4MCBy8YH8NBDD+Hn58cbb7zB6tWrGTJkCFWqVClRfKXlhrfo\njTF3Ae8A3sACEZlRaP+fgIeBS8ApYKyIXHl2SGkR0Ra9UuVEbGwsTz31FOvWraNHjx5uW679+/fn\n+eefZ8SIETzxxBOEhISwfft21q9fT0RExFWfe+bMmfTq1YvOnTvz+OOPExISwtKlS912fQ8ePJiP\nPvqInj178uCDD5KXl8eqVavIzc11yRseHk7Dhg1JTEykYcOG1KhRg+DgYPr37+82jmbNmvGnP/2J\nmTNn0r17d4YOHUpWVhZz5swhJyeH2bNnX/NENkdxcXFUrlzZ5ZI9m8qVK9OnTx/Wrl3LiRMnqFmz\nJq+//jqfffYZY8aMYd26dXTu3JmCggL7tecLFy4EYPbs2XTu3JmYmBj75XXZ2dns3LmTyMhIpk2b\nBsCTTz5JcnIyvXr1Yvz48eTl5bF48WL7l6Tiio6OJiwsjAkTJnDkyBHq1KnDnj17SExMpHnz5k4V\nupeXFwkJCdx555106NCBhx9+mGbNmvHLL7+wdetWBgwYwGOPPWbPX716de69916SkpIAPHt/guJO\nzy+NB5bK/QjQCPAFvgWaFcrzeyDQ+vwx4P0rHbdUL687duy3IcTKlUUcLulQqiwrziU5FU1mZqb4\n+/sLIIsXLy4y35YtW6Rz585SuXJlCQ0Nlf79+0tKSorbS+mKe3md7bidOnUSPz8/qVGjhjz55JOy\nd+9et5fHvfvuuxIVFSV+fn5Sq1YtGT9+vJw8edLlcjkRkX//+99yxx13SGBgoAD2eNxdXmczZ84c\nadWqlfj5+UlwcLD07t1btm/f7pTncq+3XYb2+eefF1mO3377rQAydOjQIvOIiCxevFgAeeONN+xp\nmZmZMnHiRGnUqJH4+vpKtWrVpFu3bpKcnOz02vT0dBk3bpzUrVtXfHx8pEaNGtKnTx/ZvHmzU764\nuDhp2rSp+Pj4SEREhLz11luyfv36Ii+vK+p97d27V2JiYiQkJEQqV64sPXr0kO3btxf5O9+/f78M\nGzZMwsPDxcfHR2rVqiX33nuvfPPNNy55N2/eLIDceuutly0vd+e4EkpweZ2Ra5kRU0LGmDuAV0Wk\nj3X7LwAiMr2I/G2AWSLS5XLHbd++vezatat0glyzBgYMsDzv2hWKWB1JqbLmwIED3HbbbZ4OQyll\ntWPHDrp06cKbb77JpEmTiv264nyWjTG7RaT9ZTNZ3egx+jpAusP2f6xpRYkFPnG3wxgzzhizyxiz\n69SpU6UXoXbbK6WUKgWzZs3C19eX0aNHezSOMjvr3hgzEmgPdHe3X0TmAfPA0qIvtRPrRDyllFJX\n6fz586xZs4Z9+/aRlJTE448/TlhYmEdjutEV/THAcfWZutY0J8aYO4EXgO4ikneDYrPQFr1SSqmr\ndOLECYYNG0blypUZOnQoM2bMuPKLrrMbXdF/DTQ1xkRgqeAfAIY7ZrCOy88F7hKRkzc0uowM+Okn\ny3M/P9DxTqWUUiXQpEmTYi06dCPd0DF6EbkEPAmsBw4Ay0UkxRjzmjHmbmu2t4DKwAfGmL3GGNd7\nUV4vjt32LVqAw7rRSimlVHl0w8foReRj4ONCaS87PHe9yfCN0ro1LF9u6b6vVctjYSillFKlpcxO\nxvOI6tVhyBDLQ6lySERKvISqUqrsuB7d/roErlIVRKVKla7rjUSUUtffpUuXqFSpdNvgWtErVUH4\n+/tz/vx5T4ehlLoG586dc7rxUWnQil6pCiIsLIxTp06RnZ1d5mb9KqUuT0TIzs7m9OnTpX7dvY7R\nK1VB+Pv7Ex4ezokTJ8jLu7HLTyilrp2fnx/h4eGl3qLXil6pCiQkJOSq7q+ulKq4tOteKaWUqsC0\noldKKaUqMK3olVJKqQpMK3qllFKqAtOKXimllKrAtKJXSimlKjCt6JVSSqkKzFSEFbSMMaeAH0vx\nkNWB06V4vJuZlmXp0bIsPVqWpUfLsnSUtBwbiEixltCrEBV9aTPG7BKR9p6OoyLQsiw9WpalR8uy\n9GhZlo7rWY7ada+UUkpVYFrRK6WUUhWYVvTuzfN0ABWIlmXp0bIsPVqWpUfLsnRct3LUMXqllFKq\nAtMWvVJKKVWBaUVfiDHmLmPMIWNMqjFmsqfjKU+MMfHGmJPGmO8d0m4xxmw0xhy2/qzqyRjLA2NM\nPWPMFmPMfmNMijHmaWu6lmUJGWP8jTFfGWO+tZblFGt6hDHmS+vn/H1jjK+nYy0vjDHexphvjDFr\nrNtallfBGPODMWafMWavMWaXNe26fMa1ondgjPEG/gn0BZoBw4wxzTwbVbmyELirUNpk4FMRaQp8\nat1Wl3cJmCgizYBOwBPWv0Mty5LLA3qKSCugNXCXMaYT8AbwNxFpAvwCxHowxvLmaeCAw7aW5dX7\nvYi0dris7rp8xrWid9YRSBWRNBG5ACQB93g4pnJDRLYBmYWS7wEWWZ8vAgbe0KDKIRE5LiJ7rM/P\nYfmnWgctyxITi/PWTR/rQ4CeQLI1XcuymIwxdYH+wALrtkHLsjRdl8+4VvTO6gDpDtv/saapqxcu\nIsetz08A4Z4MprwxxjQE2gBfomV5VaxdzXuBk8BG4AhwRkQuWbPo57z43gb+DBRYt6uhZXm1BNhg\njNltjBlnTbsun/FKpXEQpYpDRMQYo5d5FJMxpjKwApggIlmWxpOFlmXxiUg+0NoYEwqsAqI8HFK5\nZIz5A3BSRHYbY3p4Op4KoKuIHDPG1AA2GmMOOu4szc+4tuidHQPqOWzXtaapq/dfY0wtAOvPkx6O\np1wwxvhgqeQTRWSlNVnL8hqIyBlgC3AHEGqMsTV09HNePF2Au40xP2AZ1uwJvIOW5VURkWPWnyex\nfAHtyHX6jGtF7+xroKl1Fqkv8ACw2sMxlXergVHW56OAjzwYS7lgHfeMAw6IyEyHXVqWJWSMCbO2\n5DHGBAC9scx52AIMtmbTsiwGEfmLiNQVkYZY/jduFpERaFmWmDEmyBgTbHsOxADfc50+47pgTiHG\nmH5YxqG8gXgRmebhkMoNY8wyoAeWuzD9F3gF+BBYDtTHcofBoSJSeMKecmCM6Qp8Duzjt7HQ57GM\n02tZloAxpiWWSU3eWBo2y0XkNWNMIyyt0luAb4CRIpLnuUjLF2vX/bMi8gcty5Kzltkq62YlYKmI\nTDPGVOM6fMa1oldKKaUqMO26V0oppSowreiVUkqpCkwreqWUUqoC04peKaWUqsC0oldKKaUqMK3o\nlaoAjDGjjTFSxOOMh2NbaIz5jydjUOpmpkvgKlWxDMGy3rijS+4yKqVuDlrRK1Wx7BWRVE8HoZQq\nO7TrXqmbiEMXf7Qx5kNjzHljTIYx5p/WJWId89Yyxiw2xpw2xuQZY74zxox0c8wIY8wSY8wJa740\nY8w7bvK1McZ8bozJNsYcNsY8Wmh/TWPMImPMz9bjHDfGrLHe9EMpdZW0Ra9UxeLtcIMRmwIRKSiU\n9h6WpTZnY7mZxstAEDAa7OtvbwWqYll+Nx0YCSwxxgSKyDxrvgjgKyDbeozDWJbvjCl0virAUizL\nS78GjAHeNcYcEpEt1jxLgAbAJOv5woFeQODVFIRSykIreqUqloNu0tYCfyiU9rGIPGt9vsF6O8zX\njDGvi8j/YamImwK/F5HPrPk+McaEA1ONMXHW279OAQKAViLys8PxFxU6XzDwuK1SN8ZsA/oAw7Dc\nFAUsd5V7XkQSHV73QbHetVKqSFrRK1Wx3IvrZDx3s+6XF9pOAqZiad3/HxANHHOo5G3eAxKAZlhu\nuhMDrClUybuT7dByR0TyjDH/h6X1b/M1MMl6977NwPeiN+NQ6pppRa9UxfJ9MSfj/beI7TrWn7cA\nx9287oTDfoBquH6xcOcXN2l5gL/D9v1Y7nj4Zyxd/MeNMXOAqW6GHpRSxaST8ZS6OYUXsX3M+jMT\nqOnmdTUd9gOc5rcvB9dERE6KyBMiUgeIAhZiGRoYXxrHV+pmpRW9UjenoYW2HwAKsNzzHiwT8eoa\nY7oUyjccOAnst25vAP5gjKlVmsGJyCEReR5LT0Dz0jy2Ujcb7bpXqmJpbYyp7iZ9l4g4LpzTzxjz\nFpaKuiOWLvPFInLYun8h8DSw0hjzApbu+RFAb2C8dSIe1tf1A3YYY14HUrG08O8SEZdL8YpijAkB\nNgGJWCYUXgTuwTLrf0Nxj6OUcqUVvVIVS1Gz1MOwdLPbjAQmAo8BF4D5gG0WPiLyqzGmO/AmMAPL\nrPlDwIMi8p5Dvh+MMZ2wTOSbDlTG0v3/UQnjzgX2AI9gucSuwHq+ESJS0mMppRwYndSq1M3DGDMa\ny6z5prqCnlI3Bx2jV0oppSowreiVUkqpCky77pVSSqkKTFv0SimlVAWmFb1SSilVgWlFr5RSSlVg\nWtErpZRSFZhW9EoppVQFphW9UkopVYH9PwmGgjzg8vRMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIoJXtmmwx-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}